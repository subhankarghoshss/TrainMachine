

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>ML Glossary documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_overrides.css" type="text/css" />
  <link rel="stylesheet" href="_static/_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html#document-index" class="icon icon-home"> ML Glossary
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
            
              <!-- Local TOC -->
              <div class="local-toc"><p class="caption"><span class="caption-text">Basics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-linear_regression">Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-gradient_descent">Gradient Descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-logistic_regression">Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-glossary">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Math</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-calculus">Calculus</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-linear_algebra">Linear Algebra</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-probability">Probability (TODO)</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-statistics">Statistics (TODO)</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-math_notation">Notation</a></li>
</ul>
<p class="caption"><span class="caption-text">Neural Networks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-nn_concepts">Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-forwardpropagation">Forwardpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-backpropagation">Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-activation_functions">Activation Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-layers">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-loss_functions">Loss Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-optimizers">Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-regularization">Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-architectures">Architectures</a></li>
</ul>
<p class="caption"><span class="caption-text">Algorithms (TODO)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-classification_algos">Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-clustering_algos">Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-regression_algos">Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-reinforcement_learning">Reinforcement Learning</a></li>
</ul>
<p class="caption"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-datasets">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-libraries">Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-papers">Papers</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-other_content">Other</a></li>
</ul>
<p class="caption"><span class="caption-text">Contributing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-contribute">How to contribute</a></li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html#document-index">ML Glossary</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html#document-index">Docs</a> &raquo;</li>
        
      <li>ML Glossary  documentation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/bfortuner/ml-cheatsheet/blob/master/docs/index.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="machine-learning-glossary">
<h1>Machine Learning Glossary<a class="headerlink" href="#machine-learning-glossary" title="Permalink to this headline">¶</a></h1>
<p>Brief visual explanations of machine learning concepts with diagrams, code examples and links to resources for learning more.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">If you find errors, please raise an <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet/issues">issue</a> or <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet/blob/master/README.md">contribute</a> a better definition!</p>
</div>
<div class="toctree-wrapper compound">
<span id="document-linear_regression"></span><div class="section" id="linear-regression-1">
<span id="linear-regression"></span><h2>Linear Regression<a class="headerlink" href="#linear-regression-1" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#introduction" id="toc-entry-1">Introduction</a></li>
<li><a class="reference internal" href="#simple-regression" id="toc-entry-2">Simple regression</a><ul>
<li><a class="reference internal" href="#making-predictions" id="toc-entry-3">Making predictions</a></li>
<li><a class="reference internal" href="#cost-function" id="toc-entry-4">Cost function</a></li>
<li><a class="reference internal" href="#gradient-descent" id="toc-entry-5">Gradient descent</a></li>
<li><a class="reference internal" href="#training" id="toc-entry-6">Training</a></li>
<li><a class="reference internal" href="#model-evaluation" id="toc-entry-7">Model evaluation</a></li>
<li><a class="reference internal" href="#summary" id="toc-entry-8">Summary</a></li>
</ul>
</li>
<li><a class="reference internal" href="#multivariable-regression" id="toc-entry-9">Multivariable regression</a><ul>
<li><a class="reference internal" href="#growing-complexity" id="toc-entry-10">Growing complexity</a></li>
<li><a class="reference internal" href="#normalization" id="toc-entry-11">Normalization</a></li>
<li><a class="reference internal" href="#making-predictions-1" id="toc-entry-12">Making predictions</a></li>
<li><a class="reference internal" href="#initialize-weights" id="toc-entry-13">Initialize weights</a></li>
<li><a class="reference internal" href="#cost-function-1" id="toc-entry-14">Cost function</a></li>
<li><a class="reference internal" href="#gradient-descent-1" id="toc-entry-15">Gradient descent</a></li>
<li><a class="reference internal" href="#simplifying-with-matrices" id="toc-entry-16">Simplifying with matrices</a></li>
<li><a class="reference internal" href="#bias-term" id="toc-entry-17">Bias term</a></li>
<li><a class="reference internal" href="#model-evaluation-1" id="toc-entry-18">Model evaluation</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="introduction">
<h3><a class="toc-backref" href="#toc-entry-1">Introduction</a><a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h3>
<p>Linear Regression is a supervised machine learning algorithm where the predicted output is continuous and has a constant slope. It’s used to predict values within a continuous range, (e.g. sales, price) rather than trying to classify them into categories (e.g. cat, dog). There are two main types:</p>
<p class="rubric">Simple regression</p>
<p>Simple linear regression uses traditional slope-intercept form, where <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are the variables our algorithm will try to “learn” to produce the most accurate predictions. <span class="math notranslate nohighlight">\(x\)</span> represents our input data and <span class="math notranslate nohighlight">\(y\)</span> represents our prediction.</p>
<div class="math notranslate nohighlight">
\[y = mx + b\]</div>
<p class="rubric">Multivariable regression</p>
<p>A more complex, multi-variable linear equation might look like this, where <span class="math notranslate nohighlight">\(w\)</span> represents the coefficients, or weights, our model will try to learn.</p>
<div class="math notranslate nohighlight">
\[f(x,y,z) = w_1 x + w_2 y + w_3 z\]</div>
<p>The variables <span class="math notranslate nohighlight">\(x, y, z\)</span> represent the attributes, or distinct pieces of information, we have about each observation. For sales predictions, these attributes might include a company’s advertising spend on radio, TV, and newspapers.</p>
<div class="math notranslate nohighlight">
\[Sales = w_1 Radio + w_2 TV + w_3 News\]</div>
</div>
<div class="section" id="simple-regression">
<h3><a class="toc-backref" href="#toc-entry-2">Simple regression</a><a class="headerlink" href="#simple-regression" title="Permalink to this headline">¶</a></h3>
<p>Let’s say we are given a <a class="reference external" href="http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv">dataset</a> with the following columns (features): how much a company spends on Radio advertising each year and its annual Sales in terms of units sold. We are trying to develop an equation that will let us to predict units sold based on how much a company spends on radio advertising. The rows (observations) represent companies.</p>
<table border="1" class="docutils">
<colgroup>
<col width="35%" />
<col width="38%" />
<col width="28%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Company</strong></td>
<td><strong>Radio ($)</strong></td>
<td><strong>Sales</strong></td>
</tr>
<tr class="row-even"><td>Amazon</td>
<td>37.8</td>
<td>22.1</td>
</tr>
<tr class="row-odd"><td>Google</td>
<td>39.3</td>
<td>10.4</td>
</tr>
<tr class="row-even"><td>Facebook</td>
<td>45.9</td>
<td>18.3</td>
</tr>
<tr class="row-odd"><td>Apple</td>
<td>41.3</td>
<td>18.5</td>
</tr>
</tbody>
</table>
<div class="section" id="making-predictions">
<h4><a class="toc-backref" href="#toc-entry-3">Making predictions</a><a class="headerlink" href="#making-predictions" title="Permalink to this headline">¶</a></h4>
<p>Our prediction function outputs an estimate of sales given a company’s radio advertising spend and our current values for <em>Weight</em> and <em>Bias</em>.</p>
<div class="math notranslate nohighlight">
\[Sales = Weight \cdot Radio + Bias\]</div>
<dl class="docutils">
<dt>Weight</dt>
<dd>the coefficient for the Radio independent variable. In machine learning we call coefficients <em>weights</em>.</dd>
<dt>Radio</dt>
<dd>the independent variable. In machine learning we call these variables <em>features</em>.</dd>
<dt>Bias</dt>
<dd>the intercept where our line intercepts the y-axis. In machine learning we can call intercepts <em>bias</em>. Bias offsets all predictions that we make.</dd>
</dl>
<p>Our algorithm will try to <em>learn</em> the correct values for Weight and Bias. By the end of our training, our equation will approximate the <em>line of best fit</em>.</p>
<img alt="_images/linear_regression_line_intro.png" class="align-center" src="_images/linear_regression_line_intro.png" />
<p class="rubric">Code</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict_sales</span><span class="p">(</span><span class="n">radio</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">weight</span><span class="o">*</span><span class="n">radio</span> <span class="o">+</span> <span class="n">bias</span>
</pre></div>
</div>
</div>
<div class="section" id="cost-function">
<h4><a class="toc-backref" href="#toc-entry-4">Cost function</a><a class="headerlink" href="#cost-function" title="Permalink to this headline">¶</a></h4>
<p>The prediction function is nice, but for our purposes we don’t really need it. What we need is a <a class="reference internal" href="index.html#document-loss_functions"><span class="doc">cost function</span></a> so we can start optimizing our weights.</p>
<p>Let’s use <a class="reference internal" href="index.html#mse"><span class="std std-ref">MSE (L2)</span></a> as our cost function. MSE measures the average squared difference between an observation’s actual and predicted values. The output is a single number representing the cost, or score, associated with our current set of weights. Our goal is to minimize MSE to improve the accuracy of our model.</p>
<p class="rubric">Math</p>
<p>Given our simple linear equation <span class="math notranslate nohighlight">\(y = mx + b\)</span>, we can calculate MSE as:</p>
<div class="math notranslate nohighlight">
\[MSE =  \frac{1}{N} \sum_{i=1}^{n} (y_i - (m x_i + b))^2\]</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last simple">
<li><span class="math notranslate nohighlight">\(N\)</span> is the total number of observations (data points)</li>
<li><span class="math notranslate nohighlight">\(\frac{1}{N} \sum_{i=1}^{n}\)</span> is the mean</li>
<li><span class="math notranslate nohighlight">\(y_i\)</span> is the actual value of an observation and <span class="math notranslate nohighlight">\(m x_i + b\)</span> is our prediction</li>
</ul>
</div>
<p class="rubric">Code</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cost_function</span><span class="p">(</span><span class="n">radio</span><span class="p">,</span> <span class="n">sales</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="n">companies</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">radio</span><span class="p">)</span>
    <span class="n">total_error</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">companies</span><span class="p">):</span>
        <span class="n">total_error</span> <span class="o">+=</span> <span class="p">(</span><span class="n">sales</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">weight</span><span class="o">*</span><span class="n">radio</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">bias</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">total_error</span> <span class="o">/</span> <span class="n">companies</span>
</pre></div>
</div>
</div>
<div class="section" id="gradient-descent">
<h4><a class="toc-backref" href="#toc-entry-5">Gradient descent</a><a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h4>
<p>To minimize MSE we use <a class="reference internal" href="index.html#document-gradient_descent"><span class="doc">Gradient Descent</span></a> to calculate the gradient of our cost function. Gradient descent consists of looking at the error that our weight currently gives us, using the derivative of the cost function to find the gradient (The slope of the cost function using our current weight), and then changing our weight to move in the direction opposite of the gradient. We need to move in the opposite direction of the gradient since the gradient points up the slope instead of down it, so we move in the opposite direction to try to decrease our error.</p>
<p class="rubric">Math</p>
<p>There are two <a class="reference internal" href="index.html#glossary-parameters"><span class="std std-ref">parameters</span></a> (coefficients) in our cost function we can control: weight <span class="math notranslate nohighlight">\(m\)</span> and bias <span class="math notranslate nohighlight">\(b\)</span>. Since we need to consider the impact each one has on the final prediction, we use partial derivatives. To find the partial derivatives, we use the <a class="reference internal" href="index.html#chain-rule"><span class="std std-ref">Chain rule</span></a>. We need the chain rule because <span class="math notranslate nohighlight">\((y - (mx + b))^2\)</span> is really 2 nested functions: the inner function <span class="math notranslate nohighlight">\(y - (mx + b)\)</span> and the outer function <span class="math notranslate nohighlight">\(x^2\)</span>.</p>
<p>Returning to our cost function:</p>
<div class="math notranslate nohighlight">
\[f(m,b) =  \frac{1}{N} \sum_{i=1}^{n} (y_i - (mx_i + b))^2\]</div>
<p>Using the following:</p>
<div class="math notranslate nohighlight">
\[(y_i - (mx_i + b))^2 = A(B(m,b))\]</div>
<p>We can split the derivative into</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}A(x) = x^2\\\frac{df}{dx} = A'(x) = 2x\end{aligned}\end{align} \]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}B(m,b) = y_i - (mx_i + b) = y_i - mx_i - b\\\frac{dx}{dm} = B'(m) = 0 - x_i - 0 = -x_i\\\frac{dx}{db} = B'(b) = 0 - 0 - 1 = -1\end{aligned}\end{align} \]</div>
<p>And then using the <a class="reference internal" href="index.html#chain-rule"><span class="std std-ref">Chain rule</span></a> which states:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{df}{dm} = \frac{df}{dx} \frac{dx}{dm}\\\frac{df}{db} = \frac{df}{dx} \frac{dx}{db}\end{aligned}\end{align} \]</div>
<p>We then plug in each of the parts to get the following derivatives</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{df}{dm} = A'(B(m,f)) B'(m) = 2(y_i - (mx_i + b)) \cdot -x_i\\\frac{df}{db} = A'(B(m,f)) B'(b) = 2(y_i - (mx_i + b)) \cdot -1\end{aligned}\end{align} \]</div>
<p>We can calculate the gradient of this cost function as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
f'(m,b) =
  \begin{bmatrix}
    \frac{df}{dm}\\
    \frac{df}{db}\\
  \end{bmatrix}
&amp;=
  \begin{bmatrix}
    \frac{1}{N} \sum -x_i \cdot 2(y_i - (mx_i + b)) \\
    \frac{1}{N} \sum -1 \cdot 2(y_i - (mx_i + b)) \\
  \end{bmatrix}\\
&amp;=
  \begin{bmatrix}
     \frac{1}{N} \sum -2x_i(y_i - (mx_i + b)) \\
     \frac{1}{N} \sum -2(y_i - (mx_i + b)) \\
  \end{bmatrix}
\end{align}\end{split}\]</div>
<p class="rubric">Code</p>
<p>To solve for the gradient, we iterate through our data points using our new weight and bias values and take the average of the partial derivatives. The resulting gradient tells us the slope of our cost function at our current position (i.e. weight and bias) and the direction we should update to reduce our cost function (we move in the direction opposite the gradient). The size of our update is controlled by the <a class="reference internal" href="index.html#glossary-learning-rate"><span class="std std-ref">learning rate</span></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">radio</span><span class="p">,</span> <span class="n">sales</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">weight_deriv</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">bias_deriv</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">companies</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">radio</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">companies</span><span class="p">):</span>
        <span class="c1"># Calculate partial derivatives</span>
        <span class="c1"># -2x(y - (mx + b))</span>
        <span class="n">weight_deriv</span> <span class="o">+=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">radio</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">sales</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">weight</span><span class="o">*</span><span class="n">radio</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">bias</span><span class="p">))</span>

        <span class="c1"># -2(y - (mx + b))</span>
        <span class="n">bias_deriv</span> <span class="o">+=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">sales</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">weight</span><span class="o">*</span><span class="n">radio</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">bias</span><span class="p">))</span>

    <span class="c1"># We subtract because the derivatives point in direction of steepest ascent</span>
    <span class="n">weight</span> <span class="o">-=</span> <span class="p">(</span><span class="n">weight_deriv</span> <span class="o">/</span> <span class="n">companies</span><span class="p">)</span> <span class="o">*</span> <span class="n">learning_rate</span>
    <span class="n">bias</span> <span class="o">-=</span> <span class="p">(</span><span class="n">bias_deriv</span> <span class="o">/</span> <span class="n">companies</span><span class="p">)</span> <span class="o">*</span> <span class="n">learning_rate</span>

    <span class="k">return</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span>
</pre></div>
</div>
</div>
<div class="section" id="training">
<span id="simple-linear-regression-training"></span><h4><a class="toc-backref" href="#toc-entry-6">Training</a><a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h4>
<p>Training a model is the process of iteratively improving your prediction equation by looping through the dataset multiple times, each time updating the weight and bias values in the direction indicated by the slope of the cost function (gradient). Training is complete when we reach an acceptable error threshold, or when subsequent training iterations fail to reduce our cost.</p>
<p>Before training we need to initialize our weights (set default values), set our <a class="reference internal" href="index.html#glossary-hyperparameters"><span class="std std-ref">hyperparameters</span></a> (learning rate and number of iterations), and prepare to log our progress over each iteration.</p>
<p class="rubric">Code</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">radio</span><span class="p">,</span> <span class="n">sales</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">iters</span><span class="p">):</span>
    <span class="n">cost_history</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
        <span class="n">weight</span><span class="p">,</span><span class="n">bias</span> <span class="o">=</span> <span class="n">update_weights</span><span class="p">(</span><span class="n">radio</span><span class="p">,</span> <span class="n">sales</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>

        <span class="c1">#Calculate cost for auditing purposes</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">radio</span><span class="p">,</span> <span class="n">sales</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
        <span class="n">cost_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

        <span class="c1"># Log Progress</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span> <span class="s2">&quot;iter=</span><span class="si">{:d}</span><span class="s2">    weight=</span><span class="si">{:.2f}</span><span class="s2">    bias=</span><span class="si">{:.4f}</span><span class="s2">    cost=</span><span class="si">{:.2}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">cost_history</span>
</pre></div>
</div>
</div>
<div class="section" id="model-evaluation">
<h4><a class="toc-backref" href="#toc-entry-7">Model evaluation</a><a class="headerlink" href="#model-evaluation" title="Permalink to this headline">¶</a></h4>
<p>If our model is working, we should see our cost decrease after every iteration.</p>
<p class="rubric">Logging</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">iter</span><span class="o">=</span><span class="mi">1</span>     <span class="n">weight</span><span class="o">=.</span><span class="mi">03</span>    <span class="n">bias</span><span class="o">=.</span><span class="mi">0014</span>    <span class="n">cost</span><span class="o">=</span><span class="mf">197.25</span>
<span class="nb">iter</span><span class="o">=</span><span class="mi">10</span>    <span class="n">weight</span><span class="o">=.</span><span class="mi">28</span>    <span class="n">bias</span><span class="o">=.</span><span class="mi">0116</span>    <span class="n">cost</span><span class="o">=</span><span class="mf">74.65</span>
<span class="nb">iter</span><span class="o">=</span><span class="mi">20</span>    <span class="n">weight</span><span class="o">=.</span><span class="mi">39</span>    <span class="n">bias</span><span class="o">=.</span><span class="mi">0177</span>    <span class="n">cost</span><span class="o">=</span><span class="mf">49.48</span>
<span class="nb">iter</span><span class="o">=</span><span class="mi">30</span>    <span class="n">weight</span><span class="o">=.</span><span class="mi">44</span>    <span class="n">bias</span><span class="o">=.</span><span class="mi">0219</span>    <span class="n">cost</span><span class="o">=</span><span class="mf">44.31</span>
<span class="nb">iter</span><span class="o">=</span><span class="mi">30</span>    <span class="n">weight</span><span class="o">=.</span><span class="mi">46</span>    <span class="n">bias</span><span class="o">=.</span><span class="mi">0249</span>    <span class="n">cost</span><span class="o">=</span><span class="mf">43.28</span>
</pre></div>
</div>
<p class="rubric">Visualizing</p>
<img alt="_images/linear_regression_line_1.png" class="align-center" src="_images/linear_regression_line_1.png" />
<img alt="_images/linear_regression_line_2.png" class="align-center" src="_images/linear_regression_line_2.png" />
<img alt="_images/linear_regression_line_3.png" class="align-center" src="_images/linear_regression_line_3.png" />
<img alt="_images/linear_regression_line_4.png" class="align-center" src="_images/linear_regression_line_4.png" />
<p class="rubric">Cost history</p>
<img alt="_images/linear_regression_training_cost.png" class="align-center" src="_images/linear_regression_training_cost.png" />
</div>
<div class="section" id="summary">
<h4><a class="toc-backref" href="#toc-entry-8">Summary</a><a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h4>
<p>By learning the best values for weight (.46) and bias (.25), we now have an equation that predicts future sales based on radio advertising investment.</p>
<div class="math notranslate nohighlight">
\[Sales = .46 Radio + .025\]</div>
<p>How would our model perform in the real world? I’ll let you think about it :)</p>
</div>
</div>
<div class="section" id="multivariable-regression">
<h3><a class="toc-backref" href="#toc-entry-9">Multivariable regression</a><a class="headerlink" href="#multivariable-regression" title="Permalink to this headline">¶</a></h3>
<p>Let’s say we are given <a class="reference external" href="http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv">data</a> on TV, radio, and newspaper advertising spend for a list of companies, and our goal is to predict sales in terms of units sold.</p>
<table border="1" class="docutils">
<colgroup>
<col width="27%" />
<col width="19%" />
<col width="19%" />
<col width="16%" />
<col width="19%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Company</td>
<td>TV</td>
<td>Radio</td>
<td>News</td>
<td>Units</td>
</tr>
<tr class="row-even"><td>Amazon</td>
<td>230.1</td>
<td>37.8</td>
<td>69.1</td>
<td>22.1</td>
</tr>
<tr class="row-odd"><td>Google</td>
<td>44.5</td>
<td>39.3</td>
<td>23.1</td>
<td>10.4</td>
</tr>
<tr class="row-even"><td>Facebook</td>
<td>17.2</td>
<td>45.9</td>
<td>34.7</td>
<td>18.3</td>
</tr>
<tr class="row-odd"><td>Apple</td>
<td>151.5</td>
<td>41.3</td>
<td>13.2</td>
<td>18.5</td>
</tr>
</tbody>
</table>
<div class="section" id="growing-complexity">
<h4><a class="toc-backref" href="#toc-entry-10">Growing complexity</a><a class="headerlink" href="#growing-complexity" title="Permalink to this headline">¶</a></h4>
<p>As the number of features grows, the complexity of our model increases and it becomes increasingly difficult to visualize, or even comprehend, our data.</p>
<img alt="_images/linear_regression_3d_plane_mlr.png" class="align-center" src="_images/linear_regression_3d_plane_mlr.png" />
<p>One solution is to break the data apart and compare 1-2 features at a time. In this example we explore how Radio and TV investment impacts Sales.</p>
</div>
<div class="section" id="normalization">
<h4><a class="toc-backref" href="#toc-entry-11">Normalization</a><a class="headerlink" href="#normalization" title="Permalink to this headline">¶</a></h4>
<p>As the number of features grows, calculating gradient takes longer to compute. We can speed this up by “normalizing” our input data to ensure all values are within the same range. This is especially important for datasets with high standard deviations or differences in the ranges of the attributes. Our goal now will be to normalize our features so they are all in the range -1 to 1.</p>
<p class="rubric">Code</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">For</span> <span class="n">each</span> <span class="n">feature</span> <span class="n">column</span> <span class="p">{</span>
    <span class="c1">#1 Subtract the mean of the column (mean normalization)</span>
    <span class="c1">#2 Divide by the range of the column (feature scaling)</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Our input is a 200 x 3 matrix containing TV, Radio, and Newspaper data. Our output is a normalized matrix of the same shape with all values between -1 and 1.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
    <span class="o">**</span>
    <span class="n">features</span>     <span class="o">-</span>   <span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">features</span><span class="o">.</span><span class="n">T</span>   <span class="o">-</span>   <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

    <span class="n">We</span> <span class="n">transpose</span> <span class="n">the</span> <span class="nb">input</span> <span class="n">matrix</span><span class="p">,</span> <span class="n">swapping</span>
    <span class="n">cols</span> <span class="ow">and</span> <span class="n">rows</span> <span class="n">to</span> <span class="n">make</span> <span class="n">vector</span> <span class="n">math</span> <span class="n">easier</span>
    <span class="o">**</span>

    <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="o">.</span><span class="n">T</span><span class="p">:</span>
        <span class="n">fmean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
        <span class="n">frange</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>

        <span class="c1">#Vector Subtraction</span>
        <span class="n">feature</span> <span class="o">-=</span> <span class="n">fmean</span>

        <span class="c1">#Vector Division</span>
        <span class="n">feature</span> <span class="o">/=</span> <span class="n">frange</span>

    <span class="k">return</span> <span class="n">features</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Matrix math</strong>. Before we continue, it’s important to understand basic <a class="reference internal" href="index.html#document-linear_algebra"><span class="doc">Linear Algebra</span></a> concepts as well as numpy functions like <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html">numpy.dot()</a>.</p>
</div>
</div>
<div class="section" id="making-predictions-1">
<span id="multiple-linear-regression-predict"></span><h4><a class="toc-backref" href="#toc-entry-12">Making predictions</a><a class="headerlink" href="#making-predictions-1" title="Permalink to this headline">¶</a></h4>
<p>Our predict function outputs an estimate of sales given our current weights (coefficients) and a company’s TV, radio, and newspaper spend. Our model will try to identify weight values that most reduce our cost function.</p>
<div class="math notranslate nohighlight">
\[Sales = W_1 TV + W_2 Radio + W_3 Newspaper\]</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
  <span class="o">**</span>
  <span class="n">features</span> <span class="o">-</span> <span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
  <span class="n">weights</span> <span class="o">-</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">predictions</span> <span class="o">-</span> <span class="p">(</span><span class="mi">200</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
  <span class="o">**</span>
  <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">predictions</span>
</pre></div>
</div>
</div>
<div class="section" id="initialize-weights">
<h4><a class="toc-backref" href="#toc-entry-13">Initialize weights</a><a class="headerlink" href="#initialize-weights" title="Permalink to this headline">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">W1</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">W2</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">W3</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="n">W1</span><span class="p">],</span>
    <span class="p">[</span><span class="n">W2</span><span class="p">],</span>
    <span class="p">[</span><span class="n">W3</span><span class="p">]</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="cost-function-1">
<h4><a class="toc-backref" href="#toc-entry-14">Cost function</a><a class="headerlink" href="#cost-function-1" title="Permalink to this headline">¶</a></h4>
<p>Now we need a cost function to audit how our model is performing. The math is the same, except we swap the <span class="math notranslate nohighlight">\(mx + b\)</span> expression for <span class="math notranslate nohighlight">\(W_1 x_1 + W_2 x_2 + W_3 x_3\)</span>. We also divide the expression by 2 to make derivative calculations simpler.</p>
<div class="math notranslate nohighlight">
\[MSE =  \frac{1}{2N} \sum_{i=1}^{n} (y_i - (W_1 x_1 + W_2 x_2 + W_3 x_3))^2\]</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cost_function</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="o">**</span>
    <span class="n">features</span><span class="p">:(</span><span class="mi">200</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">targets</span><span class="p">:</span> <span class="p">(</span><span class="mi">200</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">weights</span><span class="p">:(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">returns</span> <span class="n">average</span> <span class="n">squared</span> <span class="n">error</span> <span class="n">among</span> <span class="n">predictions</span>
    <span class="o">**</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>

    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

    <span class="c1"># Matrix math lets use do this without looping</span>
    <span class="n">sq_error</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

    <span class="c1"># Return average squared error among predictions</span>
    <span class="k">return</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="n">sq_error</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="gradient-descent-1">
<h4><a class="toc-backref" href="#toc-entry-15">Gradient descent</a><a class="headerlink" href="#gradient-descent-1" title="Permalink to this headline">¶</a></h4>
<p>Again using the <a class="reference internal" href="index.html#chain-rule"><span class="std std-ref">Chain rule</span></a> we can compute the gradient–a vector of partial derivatives describing the slope of the cost function for each weight.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
f'(W_1) = -x_1(y - (W_1 x_1 + W_2 x_2 + W_3 x_3)) \\
f'(W_2) = -x_2(y - (W_1 x_1 + W_2 x_2 + W_3 x_3)) \\
f'(W_3) = -x_3(y - (W_1 x_1 + W_2 x_2 + W_3 x_3))
\end{align}\end{split}\]</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Features:(200, 3)</span>
<span class="sd">    Targets: (200, 1)</span>
<span class="sd">    Weights:(3, 1)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

    <span class="c1">#Extract our features</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x3</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>

    <span class="c1"># Use dot product to calculate the derivative for each weight</span>
    <span class="n">d_w1</span> <span class="o">=</span> <span class="o">-</span><span class="n">x1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">targets</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="n">d_w2</span> <span class="o">=</span> <span class="o">-</span><span class="n">x2</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">targets</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="n">d_w2</span> <span class="o">=</span> <span class="o">-</span><span class="n">x2</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">targets</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">)</span>

    <span class="c1"># Multiply the mean derivative by the learning rate</span>
    <span class="c1"># and subtract from our weights (remember gradient points in direction of steepest ASCENT)</span>
    <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d_w1</span><span class="p">))</span>
    <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d_w2</span><span class="p">))</span>
    <span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d_w3</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">weights</span>
</pre></div>
</div>
<p>And that’s it! Multivariate linear regression.</p>
</div>
<div class="section" id="simplifying-with-matrices">
<h4><a class="toc-backref" href="#toc-entry-16">Simplifying with matrices</a><a class="headerlink" href="#simplifying-with-matrices" title="Permalink to this headline">¶</a></h4>
<p>The gradient descent code above has a lot of duplication. Can we improve it somehow? One way to refactor would be to loop through our features and weights–allowing our function to handle any number of features. However there is another even better technique: <em>vectorized gradient descent</em>.</p>
<p class="rubric">Math</p>
<p>We use the same formula as above, but instead of operating on a single feature at a time, we use matrix multiplication to operative on all features and weights simultaneously. We replace the <span class="math notranslate nohighlight">\(x_i\)</span> terms with a single feature matrix <span class="math notranslate nohighlight">\(X\)</span>.</p>
<div class="math notranslate nohighlight">
\[gradient = -X(targets - predictions)\]</div>
<p class="rubric">Code</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">]</span>
    <span class="p">[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">]</span>
    <span class="o">.</span>
    <span class="o">.</span>
    <span class="o">.</span>
    <span class="p">[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">]</span>
<span class="p">]</span>

<span class="n">targets</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">2</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="p">]</span>

<span class="k">def</span> <span class="nf">update_weights_vectorized</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="o">**</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span>
    <span class="n">X</span><span class="p">:</span> <span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">Targets</span><span class="p">:</span> <span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">Weights</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="o">**</span>
    <span class="n">companies</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1">#1 - Get Predictions</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

    <span class="c1">#2 - Calculate error/loss</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">targets</span> <span class="o">-</span> <span class="n">predictions</span>

    <span class="c1">#3 Transpose features from (200, 3) to (3, 200)</span>
    <span class="c1"># So we can multiply w the (200,1)  error matrix.</span>
    <span class="c1"># Returns a (3,1) matrix holding 3 partial derivatives --</span>
    <span class="c1"># one for each feature -- representing the aggregate</span>
    <span class="c1"># slope of the cost function across all observations</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>  <span class="n">error</span><span class="p">)</span>

    <span class="c1">#4 Take the average error derivative for each feature</span>
    <span class="n">gradient</span> <span class="o">/=</span> <span class="n">companies</span>

    <span class="c1">#5 - Multiply the gradient by our learning rate</span>
    <span class="n">gradient</span> <span class="o">*=</span> <span class="n">lr</span>

    <span class="c1">#6 - Subtract from our weights to minimize cost</span>
    <span class="n">weights</span> <span class="o">-=</span> <span class="n">gradient</span>

    <span class="k">return</span> <span class="n">weights</span>
</pre></div>
</div>
</div>
<div class="section" id="bias-term">
<h4><a class="toc-backref" href="#toc-entry-17">Bias term</a><a class="headerlink" href="#bias-term" title="Permalink to this headline">¶</a></h4>
<p>Our train function is the same as for simple linear regression, however we’re going to make one final tweak before running: add a <a class="reference internal" href="index.html#glossary-bias-term"><span class="std std-ref">bias term</span></a> to our feature matrix.</p>
<p>In our example, it’s very unlikely that sales would be zero if companies stopped advertising. Possible reasons for this might include past advertising, existing customer relationships, retail locations, and salespeople. A bias term will help us capture this base case.</p>
<p class="rubric">Code</p>
<p>Below we add a constant 1 to our features matrix. By setting this value to 1, it turns our bias term into a constant.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="model-evaluation-1">
<h4><a class="toc-backref" href="#toc-entry-18">Model evaluation</a><a class="headerlink" href="#model-evaluation-1" title="Permalink to this headline">¶</a></h4>
<p>After training our model through 1000 iterations with a learning rate of .0005, we finally arrive at a set of weights we can use to make predictions:</p>
<div class="math notranslate nohighlight">
\[Sales = 4.7TV + 3.5Radio + .81Newspaper + 13.9\]</div>
<p>Our MSE cost dropped from 110.86 to 6.25.</p>
<img alt="_images/multiple_regression_error_history.png" class="align-center" src="_images/multiple_regression_error_history.png" />
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="https://en.wikipedia.org/wiki/Linear_regression">https://en.wikipedia.org/wiki/Linear_regression</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="http://www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables.html">http://www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables.html</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><a class="reference external" href="http://machinelearningmastery.com/simple-linear-regression-tutorial-for-machine-learning">http://machinelearningmastery.com/simple-linear-regression-tutorial-for-machine-learning</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td><a class="reference external" href="http://people.duke.edu/~rnau/regintro.htm">http://people.duke.edu/~rnau/regintro.htm</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[5]</td><td><a class="reference external" href="https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression">https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[6]</td><td><a class="reference external" href="https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms">https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms</a></td></tr>
</tbody>
</table>
</div>
</div>
</div>
<span id="document-gradient_descent"></span><div class="section" id="gradient-descent-1">
<span id="gradient-descent"></span><h2>Gradient Descent<a class="headerlink" href="#gradient-descent-1" title="Permalink to this headline">¶</a></h2>
<p>Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the <a class="reference internal" href="index.html#glossary-parameters"><span class="std std-ref">parameters</span></a> of our model. Parameters refer to coefficients in <a class="reference internal" href="index.html#document-linear_regression"><span class="doc">Linear Regression</span></a> and <a class="reference internal" href="index.html#nn-weights"><span class="std std-ref">weights</span></a> in neural networks.</p>
<div class="section" id="introduction">
<h3>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h3>
<p>Consider the 3-dimensional graph below in the context of a cost function. Our goal is to move from the mountain in the top right corner (high cost) to the dark blue sea in the bottom left (low cost). The arrows represent the direction of steepest descent (negative gradient) from any given point–the direction that decreases the cost function as quickly as possible. <a class="reference external" href="http://www.adalta.it/Pages/-GoldenSoftware-Surfer-010.asp">Source</a></p>
<img alt="_images/gradient_descent.png" class="align-center" src="_images/gradient_descent.png" />
<p>Starting at the top of the mountain, we take our first step downhill in the direction specified by the negative gradient. Next we recalculate the negative gradient (passing in the coordinates of our new point) and take another step in the direction it specifies. We continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill–a local minimum. <a class="reference external" href="https://youtu.be/5u0jaA3qAGk">image source</a>.</p>
<img alt="_images/gradient_descent_demystified.png" class="align-center" src="_images/gradient_descent_demystified.png" />
</div>
<div class="section" id="learning-rate">
<h3>Learning rate<a class="headerlink" href="#learning-rate" title="Permalink to this headline">¶</a></h3>
<p>The size of these steps is called the <em>learning rate</em>. With a high learning rate we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing. With a very low learning rate, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom.</p>
</div>
<div class="section" id="cost-function">
<h3>Cost function<a class="headerlink" href="#cost-function" title="Permalink to this headline">¶</a></h3>
<p>A <a class="reference internal" href="index.html#cost-function"><span class="std std-ref">Loss Functions</span></a> tells us “how good” our model is at making predictions for a given set of parameters. The cost function has its own curve and its own gradients. The slope of this curve tells us how to update our parameters to make the model more accurate.</p>
</div>
<div class="section" id="step-by-step">
<h3>Step-by-step<a class="headerlink" href="#step-by-step" title="Permalink to this headline">¶</a></h3>
<p>Now let’s run gradient descent using our new cost function. There are two parameters in our cost function we can control: <span class="math notranslate nohighlight">\(m\)</span> (weight) and <span class="math notranslate nohighlight">\(b\)</span> (bias). Since we need to consider the impact each one has on the final prediction, we need to use partial derivatives. We calculate the partial derivatives of the cost function with respect to each parameter and store the results in a gradient.</p>
<p class="rubric">Math</p>
<p>Given the cost function:</p>
<div class="math notranslate nohighlight">
\[f(m,b) =  \frac{1}{N} \sum_{i=1}^{N} (y_i - (mx_i + b))^2\]</div>
<p>The gradient can be calculated as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f'(m,b) =
   \begin{bmatrix}
     \frac{df}{dm}\\
     \frac{df}{db}\\
    \end{bmatrix}
=
   \begin{bmatrix}
     \frac{1}{N} \sum -2x_i(y_i - (mx_i + b)) \\
     \frac{1}{N} \sum -2(y_i - (mx_i + b)) \\
    \end{bmatrix}\end{split}\]</div>
<p>To solve for the gradient, we iterate through our data points using our new <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span> values and compute the partial derivatives. This new gradient tells us the slope of our cost function at our current position (current parameter values) and the direction we should move to update our parameters. The size of our update is controlled by the learning rate.</p>
<p class="rubric">Code</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">m_deriv</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">b_deriv</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="c1"># Calculate partial derivatives</span>
        <span class="c1"># -2x(y - (mx + b))</span>
        <span class="n">m_deriv</span> <span class="o">+=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span>

        <span class="c1"># -2(y - (mx + b))</span>
        <span class="n">b_deriv</span> <span class="o">+=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span>

    <span class="c1"># We subtract because the derivatives point in direction of steepest ascent</span>
    <span class="n">m</span> <span class="o">-=</span> <span class="p">(</span><span class="n">m_deriv</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">))</span> <span class="o">*</span> <span class="n">learning_rate</span>
    <span class="n">b</span> <span class="o">-=</span> <span class="p">(</span><span class="n">b_deriv</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">))</span> <span class="o">*</span> <span class="n">learning_rate</span>

    <span class="k">return</span> <span class="n">m</span><span class="p">,</span> <span class="n">b</span>
</pre></div>
</div>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="http://ruder.io/optimizing-gradient-descent">http://ruder.io/optimizing-gradient-descent</a></td></tr>
</tbody>
</table>
</div>
</div>
<span id="document-logistic_regression"></span><div class="section" id="logistic-regression-1">
<span id="logistic-regression"></span><h2>Logistic Regression<a class="headerlink" href="#logistic-regression-1" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#introduction" id="toc-entry-1">Introduction</a><ul>
<li><a class="reference internal" href="#comparison-to-linear-regression" id="toc-entry-2">Comparison to linear regression</a></li>
<li><a class="reference internal" href="#types-of-logistic-regression" id="toc-entry-3">Types of logistic regression</a></li>
</ul>
</li>
<li><a class="reference internal" href="#binary-logistic-regression" id="toc-entry-4">Binary logistic regression</a><ul>
<li><a class="reference internal" href="#sigmoid-activation" id="toc-entry-5">Sigmoid activation</a></li>
<li><a class="reference internal" href="#decision-boundary" id="toc-entry-6">Decision boundary</a></li>
<li><a class="reference internal" href="#making-predictions" id="toc-entry-7">Making predictions</a></li>
<li><a class="reference internal" href="#cost-function" id="toc-entry-8">Cost function</a></li>
<li><a class="reference internal" href="#gradient-descent" id="toc-entry-9">Gradient descent</a></li>
<li><a class="reference internal" href="#mapping-probabilities-to-classes" id="toc-entry-10">Mapping probabilities to classes</a></li>
<li><a class="reference internal" href="#training" id="toc-entry-11">Training</a></li>
<li><a class="reference internal" href="#model-evaluation" id="toc-entry-12">Model evaluation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#multiclass-logistic-regression" id="toc-entry-13">Multiclass logistic regression</a><ul>
<li><a class="reference internal" href="#procedure" id="toc-entry-14">Procedure</a></li>
<li><a class="reference internal" href="#softmax-activation" id="toc-entry-15">Softmax activation</a></li>
<li><a class="reference internal" href="#scikit-learn-example" id="toc-entry-16">Scikit-Learn example</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="introduction">
<h3><a class="toc-backref" href="#toc-entry-1">Introduction</a><a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h3>
<p>Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.</p>
<div class="section" id="comparison-to-linear-regression">
<h4><a class="toc-backref" href="#toc-entry-2">Comparison to linear regression</a><a class="headerlink" href="#comparison-to-linear-regression" title="Permalink to this headline">¶</a></h4>
<p>Given data on time spent studying and exam scores. <a class="reference internal" href="index.html#document-linear_regression"><span class="doc">Linear Regression</span></a> and logistic regression can predict different things:</p>
<blockquote>
<div><ul class="simple">
<li><strong>Linear Regression</strong> could help us predict the student’s test score on a scale of 0 - 100. Linear regression predictions are continuous (numbers in a range).</li>
<li><strong>Logistic Regression</strong> could help use predict whether the student passed or failed. Logistic regression predictions are discrete (only specific values or categories are allowed). We can also view probability scores underlying the model’s classifications.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="types-of-logistic-regression">
<h4><a class="toc-backref" href="#toc-entry-3">Types of logistic regression</a><a class="headerlink" href="#types-of-logistic-regression" title="Permalink to this headline">¶</a></h4>
<blockquote>
<div><ul class="simple">
<li>Binary (Pass/Fail)</li>
<li>Multi (Cats, Dogs, Sheep)</li>
<li>Ordinal (Low, Medium, High)</li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="binary-logistic-regression">
<h3><a class="toc-backref" href="#toc-entry-4">Binary logistic regression</a><a class="headerlink" href="#binary-logistic-regression" title="Permalink to this headline">¶</a></h3>
<p>Say we’re given <a class="reference external" href="http://scilab.io/wp-content/uploads/2016/07/data_classification.csv">data</a> on student exam results and our goal is to predict whether a student will pass or fail based on number of hours slept and hours spent studying. We have two features (hours slept, hours studied) and two classes: passed (1) and failed (0).</p>
<table border="1" class="docutils">
<colgroup>
<col width="35%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Studied</strong></td>
<td><strong>Slept</strong></td>
<td><strong>Passed</strong></td>
</tr>
<tr class="row-even"><td>4.85</td>
<td>9.63</td>
<td>1</td>
</tr>
<tr class="row-odd"><td>8.62</td>
<td>3.23</td>
<td>0</td>
</tr>
<tr class="row-even"><td>5.43</td>
<td>8.23</td>
<td>1</td>
</tr>
<tr class="row-odd"><td>9.21</td>
<td>6.34</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Graphically we could represent our data with a scatter plot.</p>
<img alt="_images/logistic_regression_exam_scores_scatter.png" class="align-center" src="_images/logistic_regression_exam_scores_scatter.png" />
<div class="section" id="sigmoid-activation">
<h4><a class="toc-backref" href="#toc-entry-5">Sigmoid activation</a><a class="headerlink" href="#sigmoid-activation" title="Permalink to this headline">¶</a></h4>
<p>In order to map predicted values to probabilities, we use the <a class="reference internal" href="index.html#activation-sigmoid"><span class="std std-ref">sigmoid</span></a> function. The function maps any real value into another value between 0 and 1. In machine learning, we use sigmoid to map predictions to probabilities.</p>
<p class="rubric">Math</p>
<div class="math notranslate nohighlight">
\[S(z) = \frac{1} {1 + e^{-z}}\]</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last simple">
<li><span class="math notranslate nohighlight">\(s(z)\)</span> = output between 0 and 1 (probability estimate)</li>
<li><span class="math notranslate nohighlight">\(z\)</span> = input to the function (your algorithm’s prediction e.g. mx + b)</li>
<li><span class="math notranslate nohighlight">\(e\)</span> = base of natural log</li>
</ul>
</div>
<p class="rubric">Graph</p>
<img alt="_images/sigmoid.png" class="align-center" src="_images/sigmoid.png" />
<p class="rubric">Code</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="decision-boundary">
<h4><a class="toc-backref" href="#toc-entry-6">Decision boundary</a><a class="headerlink" href="#decision-boundary" title="Permalink to this headline">¶</a></h4>
<p>Our current prediction function returns a probability score between 0 and 1. In order to map this to a discrete class (true/false, cat/dog), we select a threshold value or tipping point above which we will classify values into class 1 and below which we classify values into class 2.</p>
<div class="math notranslate nohighlight">
\[\begin{split}p \geq 0.5, class=1 \\
p &lt; 0.5, class=0\end{split}\]</div>
<p>For example, if our threshold was .5 and our prediction function returned .7, we would classify this observation as positive. If our prediction was .2 we would classify the observation as negative. For logistic regression with multiple classes we could select the class with the highest predicted probability.</p>
<img alt="_images/logistic_regression_sigmoid_w_threshold.png" class="align-center" src="_images/logistic_regression_sigmoid_w_threshold.png" />
</div>
<div class="section" id="making-predictions">
<h4><a class="toc-backref" href="#toc-entry-7">Making predictions</a><a class="headerlink" href="#making-predictions" title="Permalink to this headline">¶</a></h4>
<p>Using our knowledge of sigmoid functions and decision boundaries, we can now write a prediction function. A prediction function in logistic regression returns the probability of our observation being positive, True, or “Yes”. We call this class 1 and its notation is <span class="math notranslate nohighlight">\(P(class=1)\)</span>. As the probability gets closer to 1, our model is more confident that the observation is in class 1.</p>
<p class="rubric">Math</p>
<p>Let’s use the same <a class="reference internal" href="index.html#multiple-linear-regression-predict"><span class="std std-ref">multiple linear regression</span></a> equation from our linear regression tutorial.</p>
<div class="math notranslate nohighlight">
\[z = W_0 + W_1 Studied + W_2 Slept\]</div>
<p>This time however we will transform the output using the sigmoid function to return a probability value between 0 and 1.</p>
<div class="math notranslate nohighlight">
\[P(class=1) = \frac{1} {1 + e^{-z}}\]</div>
<p>If the model returns .4 it believes there is only a 40% chance of passing. If our decision boundary was .5, we would categorize this observation as “Fail.””</p>
<p class="rubric">Code</p>
<p>We wrap the sigmoid function over the same prediction function we used in <a class="reference internal" href="index.html#multiple-linear-regression-predict"><span class="std std-ref">multiple linear regression</span></a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
  <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">  Returns 1D array of probabilities</span>
<span class="sd">  that the class label == 1</span>
<span class="sd">  &#39;&#39;&#39;</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="cost-function">
<h4><a class="toc-backref" href="#toc-entry-8">Cost function</a><a class="headerlink" href="#cost-function" title="Permalink to this headline">¶</a></h4>
<p>Unfortunately we can’t (or at least shouldn’t) use the same cost function <a class="reference internal" href="index.html#mse"><span class="std std-ref">MSE (L2)</span></a> as we did for linear regression. Why? There is a great math explanation in chapter 3 of Michael Neilson’s deep learning book <a class="footnote-reference" href="#footnote-5" id="footnote-reference-1">[5]</a>, but for now I’ll simply say it’s because our prediction function is non-linear (due to sigmoid transform). Squaring this prediction as we do in MSE results in a non-convex function with many local minimums. If our cost function has many local minimums, gradient descent may not find the optimal global minimum.</p>
<p class="rubric">Math</p>
<p>Instead of Mean Squared Error, we use a cost function called <a class="reference internal" href="index.html#loss-cross-entropy"><span class="std std-ref">Cross-Entropy</span></a>, also known as Log Loss. Cross-entropy loss can be divided into two separate cost functions: one for <span class="math notranslate nohighlight">\(y=1\)</span> and one for <span class="math notranslate nohighlight">\(y=0\)</span>.</p>
<img alt="_images/ng_cost_function_logistic.png" class="align-center" src="_images/ng_cost_function_logistic.png" />
<p>The benefits of taking the logarithm reveal themselves when you look at the cost function graphs for y=1 and y=0. These smooth monotonic functions <a class="footnote-reference" href="#footnote-7" id="footnote-reference-2">[7]</a> (always increasing or always decreasing) make it easy to calculate the gradient and minimize cost. Image from Andrew Ng’s slides on logistic regression <a class="footnote-reference" href="#footnote-1" id="footnote-reference-3">[1]</a>.</p>
<img alt="_images/y1andy2_logistic_function.png" class="align-center" src="_images/y1andy2_logistic_function.png" />
<p>The key thing to note is the cost function penalizes confident and wrong predictions more than it rewards confident and right predictions! The corollary is increasing prediction accuracy (closer to 0 or 1) has diminishing returns on reducing cost due to the logistic nature of our cost function.</p>
<p class="rubric">Above functions compressed into one</p>
<img alt="_images/logistic_cost_function_joined.png" class="align-center" src="_images/logistic_cost_function_joined.png" />
<p>Multiplying by <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\((1-y)\)</span> in the above equation is a sneaky trick that let’s us use the same equation to solve for both y=1 and y=0 cases. If y=0, the first side cancels out. If y=1, the second side cancels out. In both cases we only perform the operation we need to perform.</p>
<p class="rubric">Vectorized cost function</p>
<img alt="_images/logistic_cost_function_vectorized.png" class="align-center" src="_images/logistic_cost_function_vectorized.png" />
<p class="rubric">Code</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cost_function</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Using Mean Absolute Error</span>

<span class="sd">    Features:(100,3)</span>
<span class="sd">    Labels: (100,1)</span>
<span class="sd">    Weights:(3,1)</span>
<span class="sd">    Returns 1D matrix of predictions</span>
<span class="sd">    Cost = (labels*log(predictions) + (1-labels)*log(1-predictions) ) / len(labels)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">observations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

    <span class="c1">#Take the error when label=1</span>
    <span class="n">class1_cost</span> <span class="o">=</span> <span class="o">-</span><span class="n">labels</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>

    <span class="c1">#Take the error when label=0</span>
    <span class="n">class2_cost</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">labels</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">predictions</span><span class="p">)</span>

    <span class="c1">#Take the sum of both costs</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">class1_cost</span> <span class="o">-</span> <span class="n">class2_cost</span>

    <span class="c1">#Take the average cost</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">cost</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">observations</span>

    <span class="k">return</span> <span class="n">cost</span>
</pre></div>
</div>
</div>
<div class="section" id="gradient-descent">
<h4><a class="toc-backref" href="#toc-entry-9">Gradient descent</a><a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h4>
<p>To minimize our cost, we use <a class="reference internal" href="index.html#document-gradient_descent"><span class="doc">Gradient Descent</span></a> just like before in <a class="reference internal" href="index.html#document-linear_regression"><span class="doc">Linear Regression</span></a>. There are other more sophisticated optimization algorithms out there such as conjugate gradient like <a class="reference internal" href="index.html#optimizers-lbfgs"><span class="std std-ref">BFGS</span></a>, but you don’t have to worry about these. Machine learning libraries like Scikit-learn hide their implementations so you can focus on more interesting things!</p>
<p class="rubric">Math</p>
<p>One of the neat properties of the sigmoid function is its derivative is easy to calculate. If you’re curious, there is a good walk-through derivation on stack overflow <a class="footnote-reference" href="#footnote-6" id="footnote-reference-4">[6]</a>. Michael Neilson also covers the topic in chapter 3 of his book.</p>
<div class="math notranslate nohighlight">
\[\begin{align}
s'(z) &amp; = s(z)(1 - s(z))
\end{align}\]</div>
<p>Which leads to an equally beautiful and convenient cost function derivative:</p>
<div class="math notranslate nohighlight">
\[C' = x(s(z) - y)\]</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last simple">
<li><span class="math notranslate nohighlight">\(C'\)</span> is the derivative of cost with respect to weights</li>
<li><span class="math notranslate nohighlight">\(y\)</span> is the actual class label (0 or 1)</li>
<li><span class="math notranslate nohighlight">\(s(z)\)</span> is your model’s prediction</li>
<li><span class="math notranslate nohighlight">\(x\)</span> is your feature or feature vector.</li>
</ul>
</div>
<p>Notice how this gradient is the same as the <a class="reference internal" href="index.html#mse"><span class="std std-ref">MSE (L2)</span></a> gradient, the only difference is the hypothesis function.</p>
<p class="rubric">Pseudocode</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Repeat</span> <span class="p">{</span>

  <span class="mf">1.</span> <span class="n">Calculate</span> <span class="n">gradient</span> <span class="n">average</span>
  <span class="mf">2.</span> <span class="n">Multiply</span> <span class="n">by</span> <span class="n">learning</span> <span class="n">rate</span>
  <span class="mf">3.</span> <span class="n">Subtract</span> <span class="kn">from</span> <span class="nn">weights</span>

<span class="p">}</span>
</pre></div>
</div>
<p class="rubric">Code</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Vectorized Gradient Descent</span>

<span class="sd">    Features:(200, 3)</span>
<span class="sd">    Labels: (200, 1)</span>
<span class="sd">    Weights:(3, 1)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

    <span class="c1">#1 - Get Predictions</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

    <span class="c1">#2 Transpose features from (200, 3) to (3, 200)</span>
    <span class="c1"># So we can multiply w the (200,1)  cost matrix.</span>
    <span class="c1"># Returns a (3,1) matrix holding 3 partial derivatives --</span>
    <span class="c1"># one for each feature -- representing the aggregate</span>
    <span class="c1"># slope of the cost function across all observations</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>  <span class="n">predictions</span> <span class="o">-</span> <span class="n">labels</span><span class="p">)</span>

    <span class="c1">#3 Take the average cost derivative for each feature</span>
    <span class="n">gradient</span> <span class="o">/=</span> <span class="n">N</span>

    <span class="c1">#4 - Multiply the gradient by our learning rate</span>
    <span class="n">gradient</span> <span class="o">*=</span> <span class="n">lr</span>

    <span class="c1">#5 - Subtract from our weights to minimize cost</span>
    <span class="n">weights</span> <span class="o">-=</span> <span class="n">gradient</span>

    <span class="k">return</span> <span class="n">weights</span>
</pre></div>
</div>
</div>
<div class="section" id="mapping-probabilities-to-classes">
<h4><a class="toc-backref" href="#toc-entry-10">Mapping probabilities to classes</a><a class="headerlink" href="#mapping-probabilities-to-classes" title="Permalink to this headline">¶</a></h4>
<p>The final step is assign class labels (0 or 1) to our predicted probabilities.</p>
<p class="rubric">Decision boundary</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">decision_boundary</span><span class="p">(</span><span class="n">prob</span><span class="p">):</span>
  <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">prob</span> <span class="o">&gt;=</span> <span class="o">.</span><span class="mi">5</span> <span class="k">else</span> <span class="mi">0</span>
</pre></div>
</div>
<p class="rubric">Convert probabilities to classes</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="n">predictions</span><span class="p">):</span>
  <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">  input  - N element array of predictions between 0 and 1</span>
<span class="sd">  output - N element array of 0s (False) and 1s (True)</span>
<span class="sd">  &#39;&#39;&#39;</span>
  <span class="n">decision_boundary</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="n">decision_boundary</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">decision_boundary</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Example output</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Probabilities</span> <span class="o">=</span> <span class="p">[</span> <span class="mf">0.967</span><span class="p">,</span> <span class="mf">0.448</span><span class="p">,</span> <span class="mf">0.015</span><span class="p">,</span> <span class="mf">0.780</span><span class="p">,</span> <span class="mf">0.978</span><span class="p">,</span> <span class="mf">0.004</span><span class="p">]</span>
<span class="n">Classifications</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="training">
<h4><a class="toc-backref" href="#toc-entry-11">Training</a><a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h4>
<p>Our training code is the same as we used for <a class="reference internal" href="index.html#simple-linear-regression-training"><span class="std std-ref">linear regression</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">iters</span><span class="p">):</span>
    <span class="n">cost_history</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">update_weights</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>

        <span class="c1">#Calculate error for auditing purposes</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
        <span class="n">cost_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

        <span class="c1"># Log Progress</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span> <span class="s2">&quot;iter: &quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; cost: &quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">cost_history</span>
</pre></div>
</div>
</div>
<div class="section" id="model-evaluation">
<h4><a class="toc-backref" href="#toc-entry-12">Model evaluation</a><a class="headerlink" href="#model-evaluation" title="Permalink to this headline">¶</a></h4>
<p>If our model is working, we should see our cost decrease after every iteration.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">iter</span><span class="p">:</span> <span class="mi">0</span> <span class="n">cost</span><span class="p">:</span> <span class="mf">0.635</span>
<span class="nb">iter</span><span class="p">:</span> <span class="mi">1000</span> <span class="n">cost</span><span class="p">:</span> <span class="mf">0.302</span>
<span class="nb">iter</span><span class="p">:</span> <span class="mi">2000</span> <span class="n">cost</span><span class="p">:</span> <span class="mf">0.264</span>
</pre></div>
</div>
<p><strong>Final cost:</strong>  0.2487.  <strong>Final weights:</strong> [-8.197, .921, .738]</p>
<p class="rubric">Cost history</p>
<img alt="_images/logistic_regression_loss_history.png" class="align-center" src="_images/logistic_regression_loss_history.png" />
<p class="rubric">Accuracy</p>
<p><a class="reference internal" href="index.html#glossary-accuracy"><span class="std std-ref">Accuracy</span></a> measures how correct our predictions were. In this case we simply compare predicted labels to true labels and divide by the total.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">predicted_labels</span><span class="p">,</span> <span class="n">actual_labels</span><span class="p">):</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">predicted_labels</span> <span class="o">-</span> <span class="n">actual_labels</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">diff</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">diff</span><span class="p">))</span>
</pre></div>
</div>
<p class="rubric">Decision boundary</p>
<p>Another helpful technique is to plot the decision boundary on top of our predictions to see how our labels compare to the actual labels. This involves plotting our predicted probabilities and coloring them with their true labels.</p>
<img alt="_images/logistic_regression_final_decision_boundary.png" class="align-center" src="_images/logistic_regression_final_decision_boundary.png" />
<p class="rubric">Code to plot the decision boundary</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">trues</span><span class="p">,</span> <span class="n">falses</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

    <span class="n">no_of_preds</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trues</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">falses</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trues</span><span class="p">))],</span> <span class="n">trues</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Trues&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">falses</span><span class="p">))],</span> <span class="n">falses</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Falses&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">);</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Decision Boundary&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;N/2&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Predicted Probability&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="multiclass-logistic-regression">
<h3><a class="toc-backref" href="#toc-entry-13">Multiclass logistic regression</a><a class="headerlink" href="#multiclass-logistic-regression" title="Permalink to this headline">¶</a></h3>
<p>Instead of <span class="math notranslate nohighlight">\(y = {0,1}\)</span> we will expand our definition so that <span class="math notranslate nohighlight">\(y = {0,1...n}\)</span>. Basically we re-run binary classification multiple times, once for each class.</p>
<div class="section" id="procedure">
<h4><a class="toc-backref" href="#toc-entry-14">Procedure</a><a class="headerlink" href="#procedure" title="Permalink to this headline">¶</a></h4>
<blockquote>
<div><ol class="arabic simple">
<li>Divide the problem into n+1 binary classification problems (+1 because the index starts at 0?).</li>
<li>For each class…</li>
<li>Predict the probability the observations are in that single class.</li>
<li>prediction = &lt;math&gt;max(probability of the classes)</li>
</ol>
</div></blockquote>
<p>For each sub-problem, we select one class (YES) and lump all the others into a second class (NO). Then we take the class with the highest predicted value.</p>
</div>
<div class="section" id="softmax-activation">
<h4><a class="toc-backref" href="#toc-entry-15">Softmax activation</a><a class="headerlink" href="#softmax-activation" title="Permalink to this headline">¶</a></h4>
<p>The softmax function (softargmax or normalized exponential function) is a function that takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval [ 0 , 1 ] , and the components will add up to 1, so that they can be interpreted as probabilities.
The standard (unit) softmax function is defined by the formula</p>
<div class="math notranslate nohighlight">
\[\begin{align}
 σ(z_i) = \frac{e^{z_{(i)}}}{\sum_{j=1}^K e^{z_{(j)}}}\ \ \ for\ i=1,.,.,.,K\ and\ z=z_1,.,.,.,z_K
\end{align}\]</div>
<p>In words: we apply the standard exponential function to each element <span class="math notranslate nohighlight">\(z_i\)</span> of the input vector <span class="math notranslate nohighlight">\(z\)</span> and normalize these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector <span class="math notranslate nohighlight">\(σ(z)\)</span> is 1. <a class="footnote-reference" href="#footnote-9" id="footnote-reference-5">[9]</a></p>
</div>
<div class="section" id="scikit-learn-example">
<h4><a class="toc-backref" href="#toc-entry-16">Scikit-Learn example</a><a class="headerlink" href="#scikit-learn-example" title="Permalink to this headline">¶</a></h4>
<p>Let’s compare our performance to the <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> model provided by scikit-learn <a class="footnote-reference" href="#footnote-8" id="footnote-reference-6">[8]</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearn</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Normalize grades to values between 0 and 1 for more efficient computation</span>
<span class="n">normalized_range</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Extract Features + Labels</span>
<span class="n">labels</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span>  <span class="p">(</span><span class="mi">100</span><span class="p">,)</span> <span class="c1">#scikit expects this</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">normalized_range</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="c1"># Create Test/Train</span>
<span class="n">features_train</span><span class="p">,</span><span class="n">features_test</span><span class="p">,</span><span class="n">labels_train</span><span class="p">,</span><span class="n">labels_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>

<span class="c1"># Scikit Logistic Regression</span>
<span class="n">scikit_log_reg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">scikit_log_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features_train</span><span class="p">,</span><span class="n">labels_train</span><span class="p">)</span>

<span class="c1">#Score is Mean Accuracy</span>
<span class="n">scikit_score</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">features_test</span><span class="p">,</span><span class="n">labels_test</span><span class="p">)</span>
<span class="nb">print</span> <span class="s1">&#39;Scikit score: &#39;</span><span class="p">,</span> <span class="n">scikit_score</span>

<span class="c1">#Our Mean Accuracy</span>
<span class="n">observations</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">run</span><span class="p">()</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">classifications</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>
<span class="n">our_acc</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">classifications</span><span class="p">,</span><span class="n">labels</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
<span class="nb">print</span> <span class="s1">&#39;Our score: &#39;</span><span class="p">,</span><span class="n">our_acc</span>
</pre></div>
</div>
<p><strong>Scikit score:</strong>  0.88. <strong>Our score:</strong> 0.89</p>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-3">[1]</a></td><td><a class="reference external" href="http://www.holehouse.org/mlclass/06_Logistic_Regression.html">http://www.holehouse.org/mlclass/06_Logistic_Regression.html</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="http://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning">http://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><a class="reference external" href="https://scilab.io/machine-learning-logistic-regression-tutorial/">https://scilab.io/machine-learning-logistic-regression-tutorial/</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td><a class="reference external" href="https://github.com/perborgen/LogisticRegression/blob/master/logistic.py">https://github.com/perborgen/LogisticRegression/blob/master/logistic.py</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-1">[5]</a></td><td><a class="reference external" href="http://neuralnetworksanddeeplearning.com/chap3.html">http://neuralnetworksanddeeplearning.com/chap3.html</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-4">[6]</a></td><td><a class="reference external" href="http://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x">http://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-2">[7]</a></td><td><a class="reference external" href="https://en.wikipedia.org/wiki/Monotoniconotonic_function">https://en.wikipedia.org/wiki/Monotoniconotonic_function</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-6">[8]</a></td><td><a class="reference external" href="http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression">http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression</a>&gt;</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-5">[9]</a></td><td><a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">https://en.wikipedia.org/wiki/Softmax_function</a></td></tr>
</tbody>
</table>
</div>
</div>
</div>
<span id="document-glossary"></span><div class="section" id="glossary-1">
<span id="glossary"></span><h2>Glossary<a class="headerlink" href="#glossary-1" title="Permalink to this headline">¶</a></h2>
<p>Definitions of common machine learning terms.</p>
<dl class="docutils" id="glossary-accuracy">
<dt>Accuracy</dt>
<dd>Percentage of correct predictions made by the model.</dd>
</dl>
<dl class="docutils" id="glossary-algorithm">
<dt>Algorithm</dt>
<dd>A method, function, or series of instructions used to generate a machine learning <a class="reference internal" href="#glossary-model"><span class="std std-ref">model</span></a>. Examples include linear regression, decision trees, support vector machines, and neural networks.</dd>
</dl>
<dl class="docutils" id="glossary-attribute">
<dt>Attribute</dt>
<dd>A quality describing an observation (e.g. color, size, weight). In Excel terms, these are column headers.</dd>
</dl>
<dl class="docutils" id="glossary-bias-metric">
<dt>Bias metric</dt>
<dd><p class="first">What is the average difference between your predictions and the correct value for that observation?</p>
<ul class="last simple">
<li><strong>Low bias</strong> could mean every prediction is correct. It could also mean half of your predictions are above their actual values and half are below, in equal proportion, resulting in low average difference.</li>
<li><strong>High bias</strong> (with low variance) suggests your model may be underfitting and you’re using the wrong architecture for the job.</li>
</ul>
</dd>
</dl>
<dl class="docutils" id="glossary-bias-term">
<dt>Bias term</dt>
<dd>Allow models to represent patterns that do not pass through the origin. For example, if all my features were 0, would my output also be zero? Is it possible there is some base value upon which my features have an effect? Bias terms typically accompany weights and are attached to neurons or filters.</dd>
</dl>
<dl class="docutils" id="glossary-categorical-variables">
<dt>Categorical Variables</dt>
<dd>Variables with a discrete set of possible values. Can be ordinal (order matters) or nominal (order doesn’t matter).</dd>
</dl>
<dl class="docutils" id="glossary-classification">
<dt>Classification</dt>
<dd><p class="first">Predicting a categorical output.</p>
<ul class="last simple">
<li><strong>Binary classification</strong> predicts one of two possible outcomes (e.g. is the email spam or not spam?)</li>
<li><strong>Multi-class classification</strong> predicts one of multiple possible outcomes (e.g. is this a photo of a cat, dog, horse or human?)</li>
</ul>
</dd>
</dl>
<dl class="docutils" id="glossary-classification-threshold">
<dt>Classification Threshold</dt>
<dd>The lowest probability value at which we’re comfortable asserting a positive classification. For example, if the predicted probability of being diabetic is &gt; 50%, return True, otherwise return False.</dd>
</dl>
<dl class="docutils" id="glossary-clustering">
<dt>Clustering</dt>
<dd>Unsupervised grouping of data into buckets.</dd>
</dl>
<dl class="docutils" id="glossary-confusion-matrix">
<dt>Confusion Matrix</dt>
<dd><p class="first">Table that describes the performance of a classification model by grouping predictions into 4 categories.</p>
<ul class="last simple">
<li><strong>True Positives</strong>: we <em>correctly</em> predicted they do have diabetes</li>
<li><strong>True Negatives</strong>: we <em>correctly</em> predicted they don’t have diabetes</li>
<li><strong>False Positives</strong>: we <em>incorrectly</em> predicted they do have diabetes (Type I error)</li>
<li><strong>False Negatives</strong>: we <em>incorrectly</em> predicted they don’t have diabetes (Type II error)</li>
</ul>
</dd>
</dl>
<dl class="docutils" id="glossary-continuous-variables">
<dt>Continuous Variables</dt>
<dd>Variables with a range of possible values defined by a number scale (e.g. sales, lifespan).</dd>
</dl>
<dl class="docutils" id="glossary-convergence">
<dt>Convergence</dt>
<dd>A state reached during the training of a model when the <a class="reference internal" href="#glossary-loss"><span class="std std-ref">loss</span></a> changes very little between each iteration.</dd>
</dl>
<dl class="docutils" id="glossary-deduction">
<dt>Deduction</dt>
<dd>A top-down approach to answering questions or solving problems. A logic technique that starts with a theory and tests that theory with observations to derive a conclusion. E.g. We suspect X, but we need to test our hypothesis before coming to any conclusions.</dd>
</dl>
<dl class="docutils" id="glossary-deep-learning">
<dt>Deep Learning</dt>
<dd>Deep Learning is derived from a machine learning algorithm called perceptron or multi layer perceptron that is gaining more and more attention nowadays because of its success in different fields like, computer vision to signal processing and medical diagnosis to self-driving cars. Like other AI algorithms, deep learning is based on decades of research. Nowadays, we have more and more data and cheap computing power that makes this algorithm really powerful in achieving state of the art accuracy. In modern world this algorithm is known as artificial neural network. Deep learning is much more accurate and robust compared to traditional artificial neural networks. But it is highly influenced by machine learning’s neural network and perceptron networks.</dd>
</dl>
<dl class="docutils" id="glossary-dimension">
<dt>Dimension</dt>
<dd>Dimension for machine learning and data scientist is different from physics. Here, dimension of data means how many features you have in your data ocean(data-set). e.g in case of object detection application, flatten image size and color channel(e.g 28*28*3) is a feature of the input set. In case of house price prediction (maybe) house size is the data-set so we call it 1 dimentional data.</dd>
</dl>
<dl class="docutils" id="glossary-epoch">
<dt>Epoch</dt>
<dd>An epoch describes the number of times the algorithm sees the entire data set.</dd>
</dl>
<dl class="docutils" id="glossary-extrapolation">
<dt>Extrapolation</dt>
<dd>Making predictions outside the range of a dataset. E.g. My dog barks, so all dogs must bark. In machine learning we often run into trouble when we extrapolate outside the range of our training data.</dd>
</dl>
<dl class="docutils" id="glossary-false-positive-rate">
<dt>False Positive Rate</dt>
<dd><p class="first">Defined as</p>
<div class="math notranslate nohighlight">
\[FPR = 1 - Specificity = \frac{False Positives}{False Positives + True Negatives}\]</div>
<p class="last">The False Positive Rate forms the x-axis of the <a class="reference internal" href="#glossary-roc-curve"><span class="std std-ref">ROC curve</span></a>.</p>
</dd>
</dl>
<dl class="docutils" id="glossary-feature">
<dt>Feature</dt>
<dd>With respect to a dataset, a feature represents an <a class="reference internal" href="#glossary-attribute"><span class="std std-ref">attribute</span></a> and value combination. Color is an attribute. “Color is blue” is a feature. In Excel terms, features are similar to cells. The term feature has other definitions in different contexts.</dd>
</dl>
<dl class="docutils" id="glossary-feature-selection">
<dt>Feature Selection</dt>
<dd>Feature selection is the process of selecting relevant features from a data-set for creating a Machine Learning model.</dd>
</dl>
<dl class="docutils" id="glossary-feature-vector">
<dt>Feature Vector</dt>
<dd>A list of features describing an observation with multiple attributes. In Excel we call this a row.</dd>
</dl>
<dl class="docutils" id="glossary-gradient-accumulation">
<dt>Gradient Accumulation</dt>
<dd>A mechanism to split the batch of samples—used for training a neural network—into several mini-batches of samples that will be run sequentially. This is used to enable using large batch sizes that require more GPU memory than available.</dd>
</dl>
<dl class="docutils" id="glossary-hyperparameters">
<dt>Hyperparameters</dt>
<dd>Hyperparameters are higher-level properties of a model such as how fast it can learn (learning rate) or complexity of a model. The depth of trees in a Decision Tree or number of hidden layers in a Neural Networks are examples of hyper parameters.</dd>
</dl>
<dl class="docutils" id="glossary-induction">
<dt>Induction</dt>
<dd>A bottoms-up approach to answering questions or solving problems. A logic technique that goes from observations to theory. E.g. We keep observing X, so we infer that Y must be True.</dd>
</dl>
<dl class="docutils" id="glossary-instance">
<dt>Instance</dt>
<dd>A data point, row, or sample in a dataset. Another term for <a class="reference internal" href="#glossary-observation"><span class="std std-ref">observation</span></a>.</dd>
</dl>
<dl class="docutils" id="glossary-label">
<dt>Label</dt>
<dd>The “answer” portion of an <a class="reference internal" href="#glossary-observation"><span class="std std-ref">observation</span></a> in <a class="reference internal" href="#glossary-supervised-learning"><span class="std std-ref">supervised learning</span></a>. For example, in a dataset used to classify flowers into different species, the features might include the petal length and petal width, while the label would be the flower’s species.</dd>
</dl>
<dl class="docutils" id="glossary-learning-rate">
<dt>Learning Rate</dt>
<dd>The size of the update steps to take during optimization loops like <a class="reference internal" href="index.html#document-gradient_descent"><span class="doc">Gradient Descent</span></a>. With a high learning rate we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing. With a very low learning rate, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom.</dd>
</dl>
<dl class="docutils" id="glossary-loss">
<dt>Loss</dt>
<dd>Loss = true_value(from data-set)- predicted value(from ML-model)  The lower the loss, the better a model (unless the model has over-fitted to the training data). The loss is calculated on training and validation and its interpretation is how well the model is doing for these two sets. Unlike accuracy, loss is not a percentage. It is a summation of the errors made for each example in training or validation sets.</dd>
</dl>
<dl class="docutils" id="glossary-machine-learning">
<dt>Machine Learning</dt>
<dd>Mitchell (1997) provides a succinct definition: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.” In simple language machine learning is a field in which human made algorithms have an ability learn by itself or predict future for unseen data.</dd>
</dl>
<dl class="docutils" id="glossary-model">
<dt>Model</dt>
<dd>A data structure that stores a representation of a dataset (weights and biases). Models are created/learned when you train an algorithm on a dataset.</dd>
</dl>
<dl class="docutils" id="glossary-neural-networks">
<dt>Neural Networks</dt>
<dd>Neural Networks are mathematical algorithms modeled after the brain’s architecture, designed to recognize patterns and relationships in data.</dd>
</dl>
<dl class="docutils" id="glossary-normalization">
<dt>Normalization</dt>
<dd>Restriction of the values of weights in regression to avoid overfitting and improving computation speed.</dd>
</dl>
<dl class="docutils" id="glossary-noise">
<dt>Noise</dt>
<dd>Any irrelevant information or randomness in a dataset which obscures the underlying pattern.</dd>
</dl>
<dl class="docutils" id="glossary-null-accuracy">
<dt>Null Accuracy</dt>
<dd>Baseline accuracy that can be achieved by always predicting the most frequent class (“B has the highest frequency, so lets guess B every time”).</dd>
</dl>
<dl class="docutils" id="glossary-observation">
<dt>Observation</dt>
<dd>A data point, row, or sample in a dataset. Another term for <a class="reference internal" href="#glossary-instance"><span class="std std-ref">instance</span></a>.</dd>
</dl>
<dl class="docutils" id="glossary-outlier">
<dt>Outlier</dt>
<dd>An observation that deviates significantly from other observations in the dataset.</dd>
</dl>
<dl class="docutils" id="glossary-overfitting">
<dt>Overfitting</dt>
<dd>Overfitting occurs when your model learns the training data too well and incorporates details and noise specific to your dataset. You can tell a model is overfitting when it performs great on your training/validation set, but poorly on your test set (or new real-world data).</dd>
</dl>
<dl class="docutils" id="glossary-parameters">
<dt>Parameters</dt>
<dd><p class="first">Parameters are properties of training data learned by training a machine learning model or classifier. They are adjusted using optimization algorithms and unique to each experiment.</p>
<p>Examples of parameters include:</p>
<ul class="last simple">
<li>weights in an artificial neural network</li>
<li>support vectors in a support vector machine</li>
<li>coefficients in a linear or logistic regression</li>
</ul>
</dd>
</dl>
<dl class="docutils" id="glossary-precision">
<dt>Precision</dt>
<dd><p class="first">In the context of binary classification (Yes/No), precision measures the model’s performance at classifying positive observations (i.e. “Yes”). In other words, when a positive value is predicted, how often is the prediction correct? We could game this metric by only returning positive for the single observation we are most confident in.</p>
<div class="last math notranslate nohighlight">
\[P = \frac{True Positives}{True Positives + False Positives}\]</div>
</dd>
</dl>
<dl class="docutils" id="glossary-recall">
<dt>Recall</dt>
<dd><p class="first">Also called sensitivity. In the context of binary classification (Yes/No), recall measures how “sensitive” the classifier is at detecting positive instances. In other words, for all the true observations in our sample, how many did we “catch.” We could game this metric by always classifying observations as positive.</p>
<div class="last math notranslate nohighlight">
\[R = \frac{True Positives}{True Positives + False Negatives}\]</div>
</dd>
</dl>
<dl class="docutils" id="glossary-recall-vs-precision">
<dt>Recall vs Precision</dt>
<dd><p class="first">Say we are analyzing Brain scans and trying to predict whether a person has a tumor (True) or not (False). We feed it into our model and our model starts guessing.</p>
<ul class="last simple">
<li><strong>Precision</strong> is the % of True guesses that were actually correct! If we guess 1 image is True out of 100 images and that image is actually True, then our precision is 100%! Our results aren’t helpful however because we missed 10 brain tumors! We were super precise when we tried, but we didn’t try hard enough.</li>
<li><strong>Recall</strong>, or Sensitivity, provides another lens which with to view how good our model is. Again let’s say there are 100 images, 10 with brain tumors, and we correctly guessed 1 had a brain tumor. Precision is 100%, but recall is 10%. Perfect recall requires that we catch all 10 tumors!</li>
</ul>
</dd>
</dl>
<dl class="docutils" id="glossary-regression">
<dt>Regression</dt>
<dd>Predicting a continuous output (e.g. price, sales).</dd>
</dl>
<dl class="docutils" id="glossary-regularization">
<dt>Regularization</dt>
<dd>Regularization is a technique utilized to combat the overfitting problem. This is achieved by adding a complexity term to the loss function that gives a bigger loss for more complex models</dd>
</dl>
<dl class="docutils" id="glossary-reinforcement-learning">
<dt>Reinforcement Learning</dt>
<dd>Training a model to maximize a reward via iterative trial and error.</dd>
</dl>
<dl class="docutils" id="glossary-roc-curve">
<dt>ROC (Receiver Operating Characteristic) Curve</dt>
<dd>A plot of the <a class="reference internal" href="#glossary-true-positive-rate"><span class="std std-ref">true positive rate</span></a> against the <a class="reference internal" href="#glossary-false-positive-rate"><span class="std std-ref">false positive rate</span></a> at all <a class="reference internal" href="#glossary-classification-threshold"><span class="std std-ref">classification thresholds</span></a>. This is used to evaluate the performance of a classification model at different classification thresholds. The area under the ROC curve can be interpreted as the probability that the model correctly distinguishes between a randomly chosen positive observation (e.g. “spam”) and a randomly chosen negative observation (e.g. “not spam”).</dd>
</dl>
<dl class="docutils" id="glossary-segmentation">
<dt>Segmentation</dt>
<dd>It is the process of partitioning a data set into multiple distinct sets. This separation is done such that the members of the same set are similar to each otherand different from the members of other sets.</dd>
</dl>
<dl class="docutils" id="glossary-specificity">
<dt>Specificity</dt>
<dd><p class="first">In the context of binary classification (Yes/No), specificity measures the model’s performance at classifying negative observations (i.e. “No”). In other words, when the correct label is negative, how often is the prediction correct? We could game this metric if we predict everything as negative.</p>
<div class="last math notranslate nohighlight">
\[S = \frac{True Negatives}{True Negatives + False Positives}\]</div>
</dd>
</dl>
<dl class="docutils" id="glossary-supervised-learning">
<dt>Supervised Learning</dt>
<dd>Training a model using a labeled dataset.</dd>
</dl>
<dl class="docutils" id="glossary-test-set">
<dt>Test Set</dt>
<dd>A set of observations used at the end of model training and validation to assess the predictive power of your model. How generalizable is your model to unseen data?</dd>
</dl>
<dl class="docutils" id="glossary-training-set">
<dt>Training Set</dt>
<dd>A set of observations used to generate machine learning models.</dd>
</dl>
<dl class="docutils" id="glossary-transfer-learning">
<dt>Transfer Learning</dt>
<dd>A machine learning method where a model developed for a task is reused as the starting point for a model on a second task. In transfer learning, we take the pre-trained weights of an already trained model (one that has been trained on millions of images belonging to 1000’s of classes, on several high power GPU’s for several days) and use these already learned features to predict new classes.</dd>
</dl>
<dl class="docutils" id="glossary-true-positive-rate">
<dt>True Positive Rate</dt>
<dd><p class="first">Another term for <a class="reference internal" href="#glossary-recall"><span class="std std-ref">recall</span></a>, i.e.</p>
<div class="math notranslate nohighlight">
\[TPR = \frac{True Positives}{True Positives + False Negatives}\]</div>
<p class="last">The True Positive Rate forms the y-axis of the <a class="reference internal" href="#glossary-roc-curve"><span class="std std-ref">ROC curve</span></a>.</p>
</dd>
</dl>
<dl class="docutils" id="glossary-type-1-error">
<dt>Type 1 Error</dt>
<dd>False Positives. Consider a company optimizing hiring practices to reduce false positives in job offers. A type 1 error occurs when candidate seems good and they hire him, but he is actually bad.</dd>
</dl>
<dl class="docutils" id="glossary-type-2-error">
<dt>Type 2 Error</dt>
<dd>False Negatives. The candidate was great but the company passed on him.</dd>
</dl>
<dl class="docutils" id="glossary-underfitting">
<dt>Underfitting</dt>
<dd>Underfitting occurs when your model over-generalizes and fails to incorporate relevant variations in your data that would give your model more predictive power. You can tell a model is underfitting when it performs poorly on both training and test sets.</dd>
</dl>
<dl class="docutils" id="glossary-uat">
<dt>Universal Approximation Theorem</dt>
<dd>A neural network with one hidden layer can approximate any continuous function but only for inputs in a specific range. If you train a network on inputs between -2 and 2, then it will work well for inputs in the same range, but you can’t expect it to generalize to other inputs without retraining the model or adding more hidden neurons.</dd>
</dl>
<dl class="docutils" id="glossary-unsupervised-learning">
<dt>Unsupervised Learning</dt>
<dd>Training a model to find patterns in an unlabeled dataset (e.g. clustering).</dd>
</dl>
<dl class="docutils" id="glossary-validation-set">
<dt>Validation Set</dt>
<dd>A set of observations used during model training to provide feedback on how well the current parameters generalize beyond the training set. If training error decreases but validation error increases, your model is likely overfitting and you should pause training.</dd>
</dl>
<dl class="docutils" id="glossary-variance">
<dt>Variance</dt>
<dd><p class="first">How tightly packed are your predictions for a particular observation relative to each other?</p>
<ul class="last simple">
<li><strong>Low variance</strong> suggests your model is internally consistent, with predictions varying little from each other after every iteration.</li>
<li><strong>High variance</strong> (with low bias) suggests your model may be overfitting and reading too deeply into the noise found in every training set.</li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="http://robotics.stanford.edu/~ronnyk/glossary.html">http://robotics.stanford.edu/~ronnyk/glossary.html</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="https://developers.google.com/machine-learning/glossary">https://developers.google.com/machine-learning/glossary</a></td></tr>
</tbody>
</table>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-calculus"></span><div class="section" id="calculus-1">
<span id="calculus"></span><h2>Calculus<a class="headerlink" href="#calculus-1" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#introduction-1" id="toc-entry-1">Introduction</a></li>
<li><a class="reference internal" href="#derivatives" id="toc-entry-2">Derivatives</a><ul>
<li><a class="reference internal" href="#geometric-definition" id="toc-entry-3">Geometric definition</a></li>
<li><a class="reference internal" href="#taking-the-derivative" id="toc-entry-4">Taking the derivative</a></li>
<li><a class="reference internal" href="#step-by-step" id="toc-entry-5">Step-by-step</a></li>
<li><a class="reference internal" href="#machine-learning-use-cases" id="toc-entry-6">Machine learning use cases</a></li>
</ul>
</li>
<li><a class="reference internal" href="#chain-rule-1" id="toc-entry-7">Chain rule</a><ul>
<li><a class="reference internal" href="#how-it-works" id="toc-entry-8">How It Works</a></li>
<li><a class="reference internal" href="#step-by-step-1" id="toc-entry-9">Step-by-step</a></li>
<li><a class="reference internal" href="#multiple-functions" id="toc-entry-10">Multiple functions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#gradients" id="toc-entry-11">Gradients</a><ul>
<li><a class="reference internal" href="#partial-derivatives" id="toc-entry-12">Partial derivatives</a></li>
<li><a class="reference internal" href="#step-by-step-2" id="toc-entry-13">Step-by-step</a></li>
<li><a class="reference internal" href="#directional-derivatives" id="toc-entry-14">Directional derivatives</a></li>
<li><a class="reference internal" href="#useful-properties" id="toc-entry-15">Useful properties</a></li>
</ul>
</li>
<li><a class="reference internal" href="#integrals-1" id="toc-entry-16">Integrals</a><ul>
<li><a class="reference internal" href="#computing-integrals" id="toc-entry-17">Computing integrals</a></li>
<li><a class="reference internal" href="#applications-of-integration" id="toc-entry-18">Applications of integration</a><ul>
<li><a class="reference internal" href="#computing-probabilities" id="toc-entry-19">Computing probabilities</a></li>
<li><a class="reference internal" href="#expected-value" id="toc-entry-20">Expected value</a></li>
<li><a class="reference internal" href="#variance" id="toc-entry-21">Variance</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="introduction-1">
<span id="introduction"></span><h3><a class="toc-backref" href="#toc-entry-1">Introduction</a><a class="headerlink" href="#introduction-1" title="Permalink to this headline">¶</a></h3>
<p>You need to know some basic calculus in order to understand how functions change over time (derivatives), and to calculate the total amount of a quantity that accumulates over a time period (integrals). The language of calculus will allow you to speak precisely about the properties of functions and better understand their behaviour.</p>
<p>Normally taking a calculus course involves doing lots of tedious calculations by hand, but having the power of computers on your side can make the process much more fun. This section describes the key ideas of calculus which you’ll need to know to understand machine learning concepts.</p>
</div>
<div class="section" id="derivatives">
<span id="derivative"></span><h3><a class="toc-backref" href="#toc-entry-2">Derivatives</a><a class="headerlink" href="#derivatives" title="Permalink to this headline">¶</a></h3>
<p>A derivative can be defined in two ways:</p>
<blockquote>
<div><ol class="arabic simple">
<li>Instantaneous rate of change (Physics)</li>
<li>Slope of a line at a specific point (Geometry)</li>
</ol>
</div></blockquote>
<p>Both represent the same principle, but for our purposes it’s easier to explain using the geometric definition.</p>
<div class="section" id="geometric-definition">
<h4><a class="toc-backref" href="#toc-entry-3">Geometric definition</a><a class="headerlink" href="#geometric-definition" title="Permalink to this headline">¶</a></h4>
<p>In geometry slope represents the steepness of a line. It answers the question: how much does <span class="math notranslate nohighlight">\(y\)</span> or <span class="math notranslate nohighlight">\(f(x)\)</span> change given a specific change in <span class="math notranslate nohighlight">\(x\)</span>?</p>
<img alt="_images/slope_formula.png" class="align-center" src="_images/slope_formula.png" />
<p>Using this definition we can easily calculate the slope between two points. But what if I asked you, instead of the slope between two points, what is the slope at a single point on the line? In this case there isn’t any obvious “rise-over-run” to calculate. Derivatives help us answer this question.</p>
<p>A derivative outputs an expression we can use to calculate the <em>instantaneous rate of change</em>, or slope, at a single point on a line. After solving for the derivative you can use it to calculate the slope at every other point on the line.</p>
</div>
<div class="section" id="taking-the-derivative">
<h4><a class="toc-backref" href="#toc-entry-4">Taking the derivative</a><a class="headerlink" href="#taking-the-derivative" title="Permalink to this headline">¶</a></h4>
<p>Consider the graph below, where <span class="math notranslate nohighlight">\(f(x) = x^2 + 3\)</span>.</p>
<img alt="_images/calculus_slope_intro.png" class="align-center" src="_images/calculus_slope_intro.png" />
<p>The slope between (1,4) and (3,12) would be:</p>
<div class="math notranslate nohighlight">
\[slope = \frac{y2-y1}{x2-x1} = \frac{12-4}{3-1} = 4\]</div>
<p>But how do we calculate the slope at point (1,4) to reveal the change in slope at that specific point?</p>
<p>One way would be to find the two nearest points, calculate their slopes relative to <span class="math notranslate nohighlight">\(x\)</span> and take the average. But calculus provides an easier, more precise way: compute the derivative. Computing the derivative of a function is essentially the same as our original proposal, but instead of finding the two closest points, we make up an imaginary point an infinitesimally small distance away from <span class="math notranslate nohighlight">\(x\)</span> and compute the slope between <span class="math notranslate nohighlight">\(x\)</span> and the new point.</p>
<p>In this way, derivatives help us answer the question: how does <span class="math notranslate nohighlight">\(f(x)\)</span> change if we make a very very tiny increase to x? In other words, derivatives help <em>estimate</em> the slope between two points that are an infinitesimally small distance away from each other. A very, very, very small distance, but large enough to calculate the slope.</p>
<p>In math language we represent this infinitesimally small increase using a limit. A limit is defined as the output value a function approaches as the input value approaches another value. In our case the target value is the specific point at which we want to calculate slope.</p>
</div>
<div class="section" id="step-by-step">
<h4><a class="toc-backref" href="#toc-entry-5">Step-by-step</a><a class="headerlink" href="#step-by-step" title="Permalink to this headline">¶</a></h4>
<p>Calculating the derivative is the same as calculating normal slope, however in this case we calculate the slope between our point and a point infinitesimally close to it. We use the variable <span class="math notranslate nohighlight">\(h\)</span> to represent this infinitesimally distance. Here are the steps:</p>
<ol class="arabic simple">
<li>Given the function:</li>
</ol>
<div class="math notranslate nohighlight">
\[f(x) = x^2\]</div>
<ol class="arabic simple" start="2">
<li>Increment <span class="math notranslate nohighlight">\(x\)</span> by a very small value <span class="math notranslate nohighlight">\(h (h = Δx)\)</span></li>
</ol>
<div class="math notranslate nohighlight">
\[f(x + h) = (x + h)^2\]</div>
<ol class="arabic simple" start="3">
<li>Apply the slope formula</li>
</ol>
<div class="math notranslate nohighlight">
\[\frac{f(x + h) - f(x)}{h}\]</div>
<ol class="arabic simple" start="4">
<li>Simplify the equation</li>
</ol>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}\frac{x^2 + 2xh + h^2 - x^2}{h} \\\end{split}\\\frac{2xh+h^2}{h} = 2x+h\end{aligned}\end{align} \]</div>
<ol class="arabic simple" start="5">
<li>Set <span class="math notranslate nohighlight">\(h\)</span> to 0 (the limit as <span class="math notranslate nohighlight">\(h\)</span> heads toward 0)</li>
</ol>
<div class="math notranslate nohighlight">
\[{2x + 0} = {2x}\]</div>
<p>So what does this mean? It means for the function <span class="math notranslate nohighlight">\(f(x) = x^2\)</span>, the slope at any point equals <span class="math notranslate nohighlight">\(2x\)</span>. The formula is defined as:</p>
<div class="math notranslate nohighlight">
\[\lim_{h\to0}\frac{f(x+h) - f(x)}{h}\]</div>
<p class="rubric">Code</p>
<p>Let’s write code to calculate the derivative of any function <span class="math notranslate nohighlight">\(f(x)\)</span>. We test our function works as expected on the input <span class="math notranslate nohighlight">\(f(x)=x^2\)</span> producing a value close to the actual derivative <span class="math notranslate nohighlight">\(2x\)</span>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_derivative</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the derivative of `func` at the location `x`.&quot;&quot;&quot;</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">0.0001</span>                          <span class="c1"># step size</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">/</span> <span class="n">h</span>    <span class="c1"># rise-over-run</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>                   <span class="c1"># some test function f(x)=x^2</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">3</span>                                   <span class="c1"># the location of interest</span>
<span class="n">computed</span> <span class="o">=</span> <span class="n">get_derivative</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>

<span class="n">computed</span><span class="p">,</span> <span class="n">actual</span>   <span class="c1"># = 6.0001, 6        # pretty close if you ask me...</span>
</pre></div>
</div>
<p>In general it’s preferable to use the math to obtain exact <a class="reference external" href="https://www.teachoo.com/9722/1227/Differentiation-Formulas/category/Finding-derivative-of-a-function-by-chain-rule/">derivative formulas</a>, but keep in mind you can always compute derivatives numerically by computing the rise-over-run for a “small step” <span class="math notranslate nohighlight">\(h\)</span>.</p>
</div>
<div class="section" id="machine-learning-use-cases">
<h4><a class="toc-backref" href="#toc-entry-6">Machine learning use cases</a><a class="headerlink" href="#machine-learning-use-cases" title="Permalink to this headline">¶</a></h4>
<p>Machine learning uses derivatives in optimization problems. Optimization algorithms like <em>gradient descent</em> use derivatives to decide whether to increase or decrease weights in order to maximize or minimize some objective (e.g. a model’s accuracy or error functions). Derivatives also help us approximate nonlinear functions as linear functions (tangent lines), which have constant slopes. With a constant slope we can decide whether to move up or down the slope (increase or decrease our weights) to get closer to the target value (class label).</p>
</div>
</div>
<div class="section" id="chain-rule-1">
<span id="chain-rule"></span><h3><a class="toc-backref" href="#toc-entry-7">Chain rule</a><a class="headerlink" href="#chain-rule-1" title="Permalink to this headline">¶</a></h3>
<p>The chain rule is a formula for calculating the derivatives of composite functions. Composite functions are functions composed of functions inside other function(s).</p>
<div class="section" id="how-it-works">
<h4><a class="toc-backref" href="#toc-entry-8">How It Works</a><a class="headerlink" href="#how-it-works" title="Permalink to this headline">¶</a></h4>
<p>Given a composite function <span class="math notranslate nohighlight">\(f(x) = A(B(x))\)</span>, the derivative of <span class="math notranslate nohighlight">\(f(x)\)</span> equals the product of the derivative of <span class="math notranslate nohighlight">\(A\)</span> with respect to <span class="math notranslate nohighlight">\(B(x)\)</span> and the derivative of <span class="math notranslate nohighlight">\(B\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="math notranslate nohighlight">
\[\mbox{composite function derivative} = \mbox{outer function derivative} * \mbox{inner function derivative}\]</div>
<p>For example, given a composite function <span class="math notranslate nohighlight">\(f(x)\)</span>, where:</p>
<div class="math notranslate nohighlight">
\[f(x) = h(g(x))\]</div>
<p>The chain rule tells us that the derivative of <span class="math notranslate nohighlight">\(f(x)\)</span> equals:</p>
<div class="math notranslate nohighlight">
\[\frac{df}{dx} = \frac{dh}{dg} \cdot \frac{dg}{dx}\]</div>
</div>
<div class="section" id="step-by-step-1">
<h4><a class="toc-backref" href="#toc-entry-9">Step-by-step</a><a class="headerlink" href="#step-by-step-1" title="Permalink to this headline">¶</a></h4>
<p>Say <span class="math notranslate nohighlight">\(f(x)\)</span> is composed of two functions <span class="math notranslate nohighlight">\(h(x) = x^3\)</span> and <span class="math notranslate nohighlight">\(g(x) = x^2\)</span>. And that:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
f(x) &amp;= h(g(x)) \\
     &amp;= (x^2)^3 \\
\end{align}\end{split}\]</div>
<p>The derivative of <span class="math notranslate nohighlight">\(f(x)\)</span> would equal:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\frac{df}{dx} &amp;=  \frac{dh}{dg} \frac{dg}{dx} \\
              &amp;=  \frac{dh}{d(x^2)} \frac{dg}{dx}
\end{align}\end{split}\]</div>
<p class="rubric">Steps</p>
<ol class="arabic simple">
<li>Solve for the inner derivative of <span class="math notranslate nohighlight">\(g(x) = x^2\)</span></li>
</ol>
<div class="math notranslate nohighlight">
\[\frac{dg}{dx} = 2x\]</div>
<ol class="arabic simple" start="2">
<li>Solve for the outer derivative of <span class="math notranslate nohighlight">\(h(x) = x^3\)</span>, using a placeholder <span class="math notranslate nohighlight">\(b\)</span> to represent the inner function <span class="math notranslate nohighlight">\(x^2\)</span></li>
</ol>
<div class="math notranslate nohighlight">
\[\frac{dh}{db} = 3b^2\]</div>
<ol class="arabic simple" start="3">
<li>Swap out the placeholder variable (b) for the inner function (g(x))</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gathered}
3(x^2)^2 \\
3x^4
\end{gathered}\end{split}\]</div>
<ol class="arabic simple" start="4">
<li>Return the product of the two derivatives</li>
</ol>
<div class="math notranslate nohighlight">
\[3x^4 \cdot 2x = 6x^5\]</div>
</div>
<div class="section" id="multiple-functions">
<h4><a class="toc-backref" href="#toc-entry-10">Multiple functions</a><a class="headerlink" href="#multiple-functions" title="Permalink to this headline">¶</a></h4>
<p>In the above example we assumed a composite function containing a single inner function. But the chain rule can also be applied to higher-order functions like:</p>
<div class="math notranslate nohighlight">
\[f(x) = A(B(C(x)))\]</div>
<p>The chain rule tells us that the derivative of this function equals:</p>
<div class="math notranslate nohighlight">
\[\frac{df}{dx} = \frac{dA}{dB} \frac{dB}{dC} \frac{dC}{dx}\]</div>
<p>We can also write this derivative equation <span class="math notranslate nohighlight">\(f'\)</span> notation:</p>
<div class="math notranslate nohighlight">
\[f' = A'(B(C(x)) \cdot B'(C(x)) \cdot C'(x)\]</div>
<p class="rubric">Steps</p>
<p>Given the function <span class="math notranslate nohighlight">\(f(x) = A(B(C(x)))\)</span>, lets assume:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
A(x) &amp; = sin(x) \\
B(x) &amp; = x^2 \\
C(x) &amp; = 4x
\end{align}\end{split}\]</div>
<p>The derivatives of these functions would be:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
A'(x) &amp;= cos(x) \\
B'(x) &amp;= 2x \\
C'(x) &amp;= 4
\end{align}\end{split}\]</div>
<p>We can calculate the derivative of <span class="math notranslate nohighlight">\(f(x)\)</span> using the following formula:</p>
<div class="math notranslate nohighlight">
\[f'(x) = A'( (4x)^2) \cdot B'(4x) \cdot C'(x)\]</div>
<p>We then input the derivatives and simplify the expression:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
f'(x) &amp;= cos((4x)^2) \cdot 2(4x) \cdot 4 \\
      &amp;= cos(16x^2) \cdot 8x \cdot 4 \\
      &amp;= cos(16x^2)32x
\end{align}\end{split}\]</div>
</div>
</div>
<div class="section" id="gradients">
<span id="gradient"></span><h3><a class="toc-backref" href="#toc-entry-11">Gradients</a><a class="headerlink" href="#gradients" title="Permalink to this headline">¶</a></h3>
<p>A gradient is a vector that stores the partial derivatives of multivariable functions. It helps us calculate the slope at a specific point on a curve for functions with multiple independent variables. In order to calculate this more complex slope, we need to isolate each variable to determine how it impacts the output on its own. To do this we iterate through each of the variables and calculate the derivative of the function after holding all other variables constant. Each iteration produces a partial derivative which we store in the gradient.</p>
<div class="section" id="partial-derivatives">
<h4><a class="toc-backref" href="#toc-entry-12">Partial derivatives</a><a class="headerlink" href="#partial-derivatives" title="Permalink to this headline">¶</a></h4>
<p>In functions with 2 or more variables, the partial derivative is the derivative of one variable with respect to the others. If we change <span class="math notranslate nohighlight">\(x\)</span>, but hold all other variables constant, how does <span class="math notranslate nohighlight">\(f(x,z)\)</span> change? That’s one partial derivative. The next variable is <span class="math notranslate nohighlight">\(z\)</span>. If we change <span class="math notranslate nohighlight">\(z\)</span> but hold <span class="math notranslate nohighlight">\(x\)</span> constant, how does <span class="math notranslate nohighlight">\(f(x,z)\)</span> change? We store partial derivatives in a gradient, which represents the full derivative of the multivariable function.</p>
</div>
<div class="section" id="step-by-step-2">
<h4><a class="toc-backref" href="#toc-entry-13">Step-by-step</a><a class="headerlink" href="#step-by-step-2" title="Permalink to this headline">¶</a></h4>
<p>Here are the steps to calculate the gradient for a multivariable function:</p>
<ol class="arabic simple">
<li>Given a multivariable function</li>
</ol>
<div class="math notranslate nohighlight">
\[f(x,z) = 2z^3x^2\]</div>
<ol class="arabic simple" start="2">
<li>Calculate the derivative with respect to <span class="math notranslate nohighlight">\(x\)</span></li>
</ol>
<div class="math notranslate nohighlight">
\[\frac{df}{dx}(x,z)\]</div>
<ol class="arabic simple" start="3">
<li>Swap <span class="math notranslate nohighlight">\(2z^3\)</span> with a constant value <span class="math notranslate nohighlight">\(b\)</span></li>
</ol>
<div class="math notranslate nohighlight">
\[f(x,z) = bx^2\]</div>
<ol class="arabic simple" start="4">
<li>Calculate the derivative with <span class="math notranslate nohighlight">\(b\)</span> constant</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\frac{df}{dx} &amp; = \lim_{h\to0}\frac{f(x+h) - f(x)}{h} \\
              &amp; = \lim_{h\to0}\frac{b(x+h)^2 - b(x^2)}{h} \\
              &amp; = \lim_{h\to0}\frac{b((x+h)(x+h)) - bx^2}{h} \\
              &amp; = \lim_{h\to0}\frac{b((x^2 + xh + hx + h^2)) - bx^2}{h} \\
              &amp; = \lim_{h\to0}\frac{bx^2 + 2bxh + bh^2 - bx^2}{h} \\
              &amp; = \lim_{h\to0}\frac{2bxh + bh^2}{h} \\
              &amp; = \lim_{h\to0} 2bx + bh \\
\end{align}\end{split}\]</div>
<p>As <span class="math notranslate nohighlight">\(h —&gt; 0\)</span>…</p>
<blockquote>
<div>2bx + 0</div></blockquote>
<ol class="arabic simple" start="5">
<li>Swap <span class="math notranslate nohighlight">\(2z^3\)</span> back into the equation, to find the derivative with respect to <span class="math notranslate nohighlight">\(x\)</span>.</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\frac{df}{dx}(x,z) &amp;= 2(2z^3)x \\
                   &amp;= 4z^3x
\end{align}\end{split}\]</div>
<ol class="arabic simple" start="6">
<li>Repeat the above steps to calculate the derivative with respect to <span class="math notranslate nohighlight">\(z\)</span></li>
</ol>
<div class="math notranslate nohighlight">
\[\frac{df}{dz}(x,z) = 6x^2z^2\]</div>
<ol class="arabic simple" start="7">
<li>Store the partial derivatives in a gradient</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla f(x,z)=\begin{bmatrix}
    \frac{df}{dx} \\
    \frac{df}{dz} \\
   \end{bmatrix}
=\begin{bmatrix}
    4z^3x \\
    6x^2z^2 \\
   \end{bmatrix}\end{split}\]</div>
</div>
<div class="section" id="directional-derivatives">
<h4><a class="toc-backref" href="#toc-entry-14">Directional derivatives</a><a class="headerlink" href="#directional-derivatives" title="Permalink to this headline">¶</a></h4>
<p>Another important concept is directional derivatives. When calculating the partial derivatives of multivariable functions we use our old technique of analyzing the impact of infinitesimally small increases to each of our independent variables. By increasing each variable we alter the function output in the direction of the slope.</p>
<p>But what if we want to change directions? For example, imagine we’re traveling north through mountainous terrain on a 3-dimensional plane. The gradient we calculated above tells us we’re traveling north at our current location. But what if we wanted to travel southwest? How can we determine the steepness of the hills in the southwest direction? Directional derivatives help us find the slope if we move in a direction different from the one specified by the gradient.</p>
<p class="rubric">Math</p>
<p>The directional derivative is computed by taking the dot product <a class="footnote-reference" href="#footnote-11" id="footnote-reference-1">[11]</a> of the gradient of <span class="math notranslate nohighlight">\(f\)</span> and a unit vector <span class="math notranslate nohighlight">\(\vec{v}\)</span> of “tiny nudges” representing the direction. The unit vector describes the proportions we want to move in each direction. The output of this calculation is a scalar number representing how much <span class="math notranslate nohighlight">\(f\)</span> will change if the current input moves with vector <span class="math notranslate nohighlight">\(\vec{v}\)</span>.</p>
<p>Let’s say you have the function <span class="math notranslate nohighlight">\(f(x,y,z)\)</span> and you want to compute its directional derivative along the following vector <a class="footnote-reference" href="#footnote-2" id="footnote-reference-2">[2]</a>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\vec{v}=\begin{bmatrix}
  2 \\
  3 \\
  -1  \\
 \end{bmatrix}\end{split}\]</div>
<p>As described above, we take the dot product of the gradient and the directional vector:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
  \frac{df}{dx} \\
  \frac{df}{dy} \\
  \frac{df}{dz} \\
 \end{bmatrix}
 \cdot
 \begin{bmatrix}
    2 \\
    3 \\
    -1  \\
 \end{bmatrix}\end{split}\]</div>
<p>We can rewrite the dot product as:</p>
<div class="math notranslate nohighlight">
\[\nabla_\vec{v} f = 2 \frac{df}{dx} + 3 \frac{df}{dy} - 1 \frac{df}{dz}\]</div>
<p>This should make sense because a tiny nudge along <span class="math notranslate nohighlight">\(\vec{v}\)</span> can be broken down into two tiny nudges in the x-direction, three tiny nudges in the y-direction, and a tiny nudge backwards, by −1 in the z-direction.</p>
</div>
<div class="section" id="useful-properties">
<h4><a class="toc-backref" href="#toc-entry-15">Useful properties</a><a class="headerlink" href="#useful-properties" title="Permalink to this headline">¶</a></h4>
<p>There are two additional properties of gradients that are especially useful in deep learning. The gradient of a function:</p>
<blockquote>
<div><ol class="arabic simple">
<li>Always points in the direction of greatest increase of a function (<a class="reference external" href="https://betterexplained.com/articles/understanding-pythagorean-distance-and-the-gradient">explained here</a>)</li>
<li>Is zero at a local maximum or local minimum</li>
</ol>
</div></blockquote>
</div>
</div>
<div class="section" id="integrals-1">
<span id="integrals"></span><h3><a class="toc-backref" href="#toc-entry-16">Integrals</a><a class="headerlink" href="#integrals-1" title="Permalink to this headline">¶</a></h3>
<p>The integral of <span class="math notranslate nohighlight">\(f(x)\)</span> corresponds to the computation of the area under the graph of <span class="math notranslate nohighlight">\(f(x)\)</span>. The area under <span class="math notranslate nohighlight">\(f(x)\)</span> between the points <span class="math notranslate nohighlight">\(x=a\)</span> and <span class="math notranslate nohighlight">\(x=b\)</span> is denoted as follows:</p>
<div class="math notranslate nohighlight">
\[A(a,b) = \int_a^b f(x) \: dx.\]</div>
<img alt="_images/integral_definition.png" class="align-center" src="_images/integral_definition.png" />
<p>The area <span class="math notranslate nohighlight">\(A(a,b)\)</span> is bounded by the function <span class="math notranslate nohighlight">\(f(x)\)</span> from above, by the <span class="math notranslate nohighlight">\(x\)</span>-axis from below, and by two vertical lines at <span class="math notranslate nohighlight">\(x=a\)</span> and <span class="math notranslate nohighlight">\(x=b\)</span>. The points <span class="math notranslate nohighlight">\(x=a\)</span> and <span class="math notranslate nohighlight">\(x=b\)</span> are called the limits of integration. The <span class="math notranslate nohighlight">\(\int\)</span> sign comes from the Latin word summa. The integral is the “sum” of the values of <span class="math notranslate nohighlight">\(f(x)\)</span> between the two limits of integration.</p>
<p>The <em>integral function</em> <span class="math notranslate nohighlight">\(F(c)\)</span> corresponds to the area calculation as a function of the upper limit of integration:</p>
<div class="math notranslate nohighlight">
\[F(c) \equiv \int_0^c \! f(x)\:dx\,.\]</div>
<p>There are two variables and one constant in this formula. The input variable <span class="math notranslate nohighlight">\(c\)</span> describes the upper limit of integration. The <em>integration variable</em> <span class="math notranslate nohighlight">\(x\)</span> performs a sweep from <span class="math notranslate nohighlight">\(x=0\)</span> until <span class="math notranslate nohighlight">\(x=c\)</span>. The constant <span class="math notranslate nohighlight">\(0\)</span> describes the lower limit of integration. Note that choosing <span class="math notranslate nohighlight">\(x=0\)</span> for the starting point of the integral function was an arbitrary choice.</p>
<p>The integral function <span class="math notranslate nohighlight">\(F(c)\)</span> contains the “precomputed” information about the area under the graph of <span class="math notranslate nohighlight">\(f(x)\)</span>.  The derivative function <span class="math notranslate nohighlight">\(f'(x)\)</span> tells us the “slope of the graph” property of the function <span class="math notranslate nohighlight">\(f(x)\)</span> for all values of <span class="math notranslate nohighlight">\(x\)</span>. Similarly, the integral function <span class="math notranslate nohighlight">\(F(c)\)</span> tells us the “area under the graph” property of the function <span class="math notranslate nohighlight">\(f(x)\)</span> for <em>all</em> possible limits of integration.</p>
<p>The area under <span class="math notranslate nohighlight">\(f(x)\)</span> between <span class="math notranslate nohighlight">\(x=a\)</span> and <span class="math notranslate nohighlight">\(x=b\)</span> is obtained by calculating the <em>change</em> in the integral function as follows:</p>
<div class="math notranslate nohighlight">
\[A(a,b) = \int_a^b \! f(x)\:dx
     =  F(b)-F(a).\]</div>
<img alt="_images/integral_as_change_in_antriderivative.png" class="align-center" src="_images/integral_as_change_in_antriderivative.png" />
<div class="section" id="computing-integrals">
<h4><a class="toc-backref" href="#toc-entry-17">Computing integrals</a><a class="headerlink" href="#computing-integrals" title="Permalink to this headline">¶</a></h4>
<p>We can approximate the total area under the function <span class="math notranslate nohighlight">\(f(x)\)</span> between <span class="math notranslate nohighlight">\(x=a\)</span> and <span class="math notranslate nohighlight">\(x=b\)</span> by splitting the region into tiny vertical strips of width <span class="math notranslate nohighlight">\(h\)</span>, then adding up the areas of the rectangular strips. The figure below shows how to compute the area under <span class="math notranslate nohighlight">\(f(x)=x^2\)</span> between <span class="math notranslate nohighlight">\(x=1\)</span> and <span class="math notranslate nohighlight">\(x=3\)</span> by approximating it as four rectangular strips of width <span class="math notranslate nohighlight">\(h=0.5\)</span>.</p>
<img alt="_images/integral_as_rectangular_strips.png" class="align-center" src="_images/integral_as_rectangular_strips.png" />
<p>Usually we want to choose <span class="math notranslate nohighlight">\(h\)</span> to be a small number so that the approximation is accurate. Here is some sample code that performs integration.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_integral</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the area under `func` between x=a and x=b.&quot;&quot;&quot;</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">0.0001</span>               <span class="c1"># width of small rectangle</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">a</span>                    <span class="c1"># start at x=a</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="n">b</span><span class="p">:</span>            <span class="c1"># continue until x=b</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">h</span><span class="o">*</span><span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># area of rect is base*height</span>
        <span class="n">x</span> <span class="o">+=</span> <span class="n">h</span>
    <span class="k">return</span> <span class="n">total</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>                    <span class="c1"># some test function f(x)=x^2</span>
<span class="n">computed</span> <span class="o">=</span> <span class="n">get_integral</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">actualF</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="mf">1.0</span><span class="o">/</span><span class="mf">3.0</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">actualF</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">-</span> <span class="n">actualF</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">computed</span><span class="p">,</span> <span class="n">actual</span>    <span class="c1"># = 8.6662, 8.6666   # pretty close if you ask me...</span>
</pre></div>
</div>
<p>You can find integral functions using the derivative formulas and some reverse engineering. To find an integral function of the function <span class="math notranslate nohighlight">\(f(x)\)</span>, we must find a function <span class="math notranslate nohighlight">\(F(x)\)</span> such that <span class="math notranslate nohighlight">\(F'(x)=f(x)\)</span>. Suppose you’re given a function <span class="math notranslate nohighlight">\(f(x)\)</span> and asked to find its integral function <span class="math notranslate nohighlight">\(F(x)\)</span>:</p>
<div class="math notranslate nohighlight">
\[F(x) = \int \! f(x)\: dx.\]</div>
<p>This problem is equivalent to finding a function <span class="math notranslate nohighlight">\(F(x)\)</span> whose derivative is <span class="math notranslate nohighlight">\(f(x)\)</span>:</p>
<div class="math notranslate nohighlight">
\[F'(x) = f(x).\]</div>
<p>For example, suppose you want to find the indefinite integral <span class="math notranslate nohighlight">\(\int \!x^2\:dx\)</span>. We can rephrase this problem as the search for some function <span class="math notranslate nohighlight">\(F(x)\)</span> such that</p>
<div class="math notranslate nohighlight">
\[F'(x) = x^2.\]</div>
<p>Remembering the derivative formulas we saw above, you guess that <span class="math notranslate nohighlight">\(F(x)\)</span> must contain an <span class="math notranslate nohighlight">\(x^3\)</span> term. Taking the derivative of a cubic term results in a quadratic term. Therefore, the function you are looking for has the form <span class="math notranslate nohighlight">\(F(x)=cx^3\)</span>, for some constant <span class="math notranslate nohighlight">\(c\)</span>. Pick the constant <span class="math notranslate nohighlight">\(c\)</span> that makes this equation true:</p>
<div class="math notranslate nohighlight">
\[F'(x) = 3cx^2 = x^2.\]</div>
<p>Solving <span class="math notranslate nohighlight">\(3c=1\)</span>, we find <span class="math notranslate nohighlight">\(c=\frac{1}{3}\)</span> and so the integral function is</p>
<div class="math notranslate nohighlight">
\[F(x) = \int x^2 \:dx = \frac{1}{3}x^3 + C.\]</div>
<p>You can verify that <span class="math notranslate nohighlight">\(\frac{d}{dx}\left[\frac{1}{3}x^3 + C\right] = x^2\)</span>.</p>
<p>You can also verify Integrals using maths. Here is a set of <a class="reference external" href="https://www.teachoo.com/5643/728/Integration-Formulas---Trig--Definite-Integrals-Properties-and-more/category/Miscellaneous/">formulas</a> for your reference</p>
</div>
<div class="section" id="applications-of-integration">
<h4><a class="toc-backref" href="#toc-entry-18">Applications of integration</a><a class="headerlink" href="#applications-of-integration" title="Permalink to this headline">¶</a></h4>
<p>Integral calculations have widespread applications to more areas of science than are practical to list here. Let’s explore a few examples related to probabilities.</p>
<div class="section" id="computing-probabilities">
<h5><a class="toc-backref" href="#toc-entry-19">Computing probabilities</a><a class="headerlink" href="#computing-probabilities" title="Permalink to this headline">¶</a></h5>
<p>A continuous random variable <span class="math notranslate nohighlight">\(X\)</span> is described by its probability density function <span class="math notranslate nohighlight">\(p(x)\)</span>. A probability density function <span class="math notranslate nohighlight">\(p(x)\)</span> is a positive function for which the total area under the curve is <span class="math notranslate nohighlight">\(1\)</span>:</p>
<div class="math notranslate nohighlight">
\[      p(x) \geq 0, \forall x
\qquad
       \textrm{and}
       \qquad
       \int_{-\infty}^\infty p(x)\; dx = 1.\]</div>
<p>The probability of observing a value of <span class="math notranslate nohighlight">\(X\)</span> between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> is given by the integral</p>
<div class="math notranslate nohighlight">
\[      \textrm{Pr}(a \leq X \leq b)
=
      \int_a^b p(x)\; dx.\]</div>
<p>Thus, the notion of integration is central to probability theory with continuous random variables.</p>
<p>We also use integration to compute certain characteristic properties of the random variable. The <em>expected value</em> and the <em>variance</em> are two properties of any random variable <span class="math notranslate nohighlight">\(X\)</span> that capture important aspects of its behaviour.</p>
</div>
<div class="section" id="expected-value">
<h5><a class="toc-backref" href="#toc-entry-20">Expected value</a><a class="headerlink" href="#expected-value" title="Permalink to this headline">¶</a></h5>
<p>The <em>expected value</em> of the random variable <span class="math notranslate nohighlight">\(X\)</span> is computed using the formula</p>
<div class="math notranslate nohighlight">
\[\mu
      % \equiv \mathbb{E}_X[X]
      = \int_{-\infty}^\infty x\, p(x).\]</div>
<p>The expected value is a single number that tells us what value of <span class="math notranslate nohighlight">\(X\)</span> we can expect to obtain on average from the random variable <span class="math notranslate nohighlight">\(X\)</span>. The expected value is also called the <em>average</em> or the <em>mean</em> of the random variable <span class="math notranslate nohighlight">\(X\)</span>.</p>
</div>
<div class="section" id="variance">
<h5><a class="toc-backref" href="#toc-entry-21">Variance</a><a class="headerlink" href="#variance" title="Permalink to this headline">¶</a></h5>
<p>The <em>variance</em> of the random variable <span class="math notranslate nohighlight">\(X\)</span> is defined as follows:</p>
<div class="math notranslate nohighlight">
\[\sigma^2
      % \equiv  \mathbb{E}_X\!\big[(X-\mu)^2\big]
      = \int_{-\infty}^\infty (x-\mu)^2 \, p(x).\]</div>
<p>The variance formula computes the expectation of the squared distance of the random variable <span class="math notranslate nohighlight">\(X\)</span> from its expected value. The variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, also denoted <span class="math notranslate nohighlight">\(\textrm{var}(X)\)</span>, gives us an indication of how clustered or spread the values of <span class="math notranslate nohighlight">\(X\)</span> are. A small variance indicates the outcomes of <span class="math notranslate nohighlight">\(X\)</span> are tightly clustered near the expected value <span class="math notranslate nohighlight">\(\mu\)</span>, while a large variance indicates the outcomes of <span class="math notranslate nohighlight">\(X\)</span> are widely spread. The square root of the variance is called the <em>standard deviation</em> and is usually denoted <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<p>The expected value <span class="math notranslate nohighlight">\(\mu\)</span> and the variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> are two central concepts in probability theory and statistics because they allow us to characterize any random variable. The expected value is a measure of the <em>central tendency</em> of the random variable,  while the variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> measures its <em>dispersion</em>.
Readers familiar with concepts from physics can think of the expected value as the <em>centre of mass</em> of the distribution, and the variance as the <em>moment of inertia</em> of the distribution.</p>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="https://en.wikipedia.org/wiki/Derivative">https://en.wikipedia.org/wiki/Derivative</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-2">[2]</a></td><td><a class="reference external" href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/directional-derivative-introduction">https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/directional-derivative-introduction</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><a class="reference external" href="https://en.wikipedia.org/wiki/Partial_derivative">https://en.wikipedia.org/wiki/Partial_derivative</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td><a class="reference external" href="https://en.wikipedia.org/wiki/Gradient">https://en.wikipedia.org/wiki/Gradient</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[5]</td><td><a class="reference external" href="https://betterexplained.com/articles/vector-calculus-understanding-the-gradient">https://betterexplained.com/articles/vector-calculus-understanding-the-gradient</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[6]</td><td><a class="reference external" href="https://www.mathsisfun.com/calculus/derivatives-introduction.html">https://www.mathsisfun.com/calculus/derivatives-introduction.html</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[7]</td><td><a class="reference external" href="http://tutorial.math.lamar.edu/Classes/CalcI/DefnOfDerivative.aspx">http://tutorial.math.lamar.edu/Classes/CalcI/DefnOfDerivative.aspx</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[8]</td><td><a class="reference external" href="https://www.khanacademy.org/math/calculus-home/taking-derivatives-calc/chain-rule-calc/v/chain-rule-introduction">https://www.khanacademy.org/math/calculus-home/taking-derivatives-calc/chain-rule-calc/v/chain-rule-introduction</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[9]</td><td><a class="reference external" href="http://tutorial.math.lamar.edu/Classes/CalcI/ChainRule.aspx">http://tutorial.math.lamar.edu/Classes/CalcI/ChainRule.aspx</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[10]</td><td><a class="reference external" href="https://youtu.be/pHMzNW8Agq4?t=1m5s">https://youtu.be/pHMzNW8Agq4?t=1m5s</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-11" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-1">[11]</a></td><td><a class="reference external" href="https://en.wikipedia.org/wiki/Dot_product">https://en.wikipedia.org/wiki/Dot_product</a></td></tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<span id="document-linear_algebra"></span><div class="section" id="linear-algebra-1">
<span id="linear-algebra"></span><h2>Linear Algebra<a class="headerlink" href="#linear-algebra-1" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#vectors" id="toc-entry-1">Vectors</a><ul>
<li><a class="reference internal" href="#notation" id="toc-entry-2">Notation</a></li>
<li><a class="reference internal" href="#vectors-in-geometry" id="toc-entry-3">Vectors in&nbsp;geometry</a></li>
<li><a class="reference internal" href="#scalar-operations" id="toc-entry-4">Scalar operations</a></li>
<li><a class="reference internal" href="#elementwise-operations" id="toc-entry-5">Elementwise operations</a></li>
<li><a class="reference internal" href="#dot-product" id="toc-entry-6">Dot product</a></li>
<li><a class="reference internal" href="#hadamard-product" id="toc-entry-7">Hadamard product</a></li>
<li><a class="reference internal" href="#vector-fields" id="toc-entry-8">Vector fields</a></li>
</ul>
</li>
<li><a class="reference internal" href="#matrices" id="toc-entry-9">Matrices</a><ul>
<li><a class="reference internal" href="#dimensions" id="toc-entry-10">Dimensions</a></li>
<li><a class="reference internal" href="#scalar-operations-1" id="toc-entry-11">Scalar operations</a></li>
<li><a class="reference internal" href="#elementwise-operations-1" id="toc-entry-12">Elementwise operations</a></li>
<li><a class="reference internal" href="#hadamard-product-1" id="toc-entry-13">Hadamard&nbsp;product</a></li>
<li><a class="reference internal" href="#matrix-transpose" id="toc-entry-14">Matrix transpose</a></li>
<li><a class="reference internal" href="#matrix-multiplication" id="toc-entry-15">Matrix multiplication</a></li>
<li><a class="reference internal" href="#test-yourself" id="toc-entry-16">Test yourself</a></li>
</ul>
</li>
<li><a class="reference internal" href="#numpy" id="toc-entry-17">Numpy</a><ul>
<li><a class="reference internal" href="#dot-product-1" id="toc-entry-18">Dot product</a></li>
<li><a class="reference internal" href="#broadcasting" id="toc-entry-19">Broadcasting</a></li>
</ul>
</li>
</ul>
</div>
<p>Linear algebra is a mathematical toolbox that offers helpful techniques for manipulating groups of numbers simultaneously. It provides structures like vectors and matrices (spreadsheets) to hold these numbers and new rules for how to add, subtract, multiply, and divide them. Here is a brief overview of basic linear algebra concepts taken from my linear algebra <a class="reference external" href="https://medium.com/p/cd67aba4526c">post</a> on Medium.</p>
<div class="section" id="vectors">
<h3><a class="toc-backref" href="#toc-entry-1">Vectors</a><a class="headerlink" href="#vectors" title="Permalink to this headline">¶</a></h3>
<p>Vectors are 1-dimensional arrays of numbers or terms. In geometry, vectors store the magnitude and direction of a potential change to a point. The vector [3, -2] says go right 3 and down 2. A vector with more than one dimension is called a matrix.</p>
<div class="section" id="notation">
<h4><a class="toc-backref" href="#toc-entry-2">Notation</a><a class="headerlink" href="#notation" title="Permalink to this headline">¶</a></h4>
<p>There are a variety of ways to represent vectors. Here are a few you might come across in your reading.</p>
<div class="math notranslate nohighlight">
\[\begin{split}v = \begin{bmatrix}
1 \\
2 \\
3 \\
\end{bmatrix}
=
\begin{pmatrix}
1 \\
2 \\
3 \\
\end{pmatrix}
=
\begin{bmatrix}
1 &amp; 2 &amp; 3\\
\end{bmatrix}\end{split}\]</div>
</div>
<div class="section" id="vectors-in-geometry">
<h4><a class="toc-backref" href="#toc-entry-3">Vectors in&nbsp;geometry</a><a class="headerlink" href="#vectors-in-geometry" title="Permalink to this headline">¶</a></h4>
<p>Vectors typically represent movement from a point. They store both the magnitude and direction of potential changes to a point. The vector [-2,5] says move left 2 units and up 5 units <a class="footnote-reference" href="#footnote-1" id="footnote-reference-1">[1]</a>.</p>
<img alt="_images/vectors_geometry.png" class="align-center" src="_images/vectors_geometry.png" />
<p>A vector can be applied to any point in space. The vector’s direction equals the slope of the hypotenuse created moving up 5 and left 2. Its magnitude equals the length of the hypotenuse.</p>
</div>
<div class="section" id="scalar-operations">
<h4><a class="toc-backref" href="#toc-entry-4">Scalar operations</a><a class="headerlink" href="#scalar-operations" title="Permalink to this headline">¶</a></h4>
<p>Scalar operations involve a vector and a number. You modify the vector in-place by adding, subtracting, or multiplying the number from all the values in the vector.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
2 \\
2 \\
2 \\
\end{bmatrix}
+
1
=
\begin{bmatrix}
3 \\
3 \\
3 \\
\end{bmatrix}\end{split}\]</div>
</div>
<div class="section" id="elementwise-operations">
<h4><a class="toc-backref" href="#toc-entry-5">Elementwise operations</a><a class="headerlink" href="#elementwise-operations" title="Permalink to this headline">¶</a></h4>
<p>In elementwise operations like addition, subtraction, and division, values that correspond positionally are combined to produce a new vector. The 1st value in vector A is paired with the 1st value in vector B. The 2nd value is paired with the 2nd, and so on. This means the vectors must have equal dimensions to complete the operation.*</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
a_1 \\
a_2 \\
\end{bmatrix}
+
\begin{bmatrix}
b_1 \\
b_2 \\
\end{bmatrix}
=
\begin{bmatrix}
a_1+b_1 \\
a_2+b_2 \\
\end{bmatrix}\end{split}\]</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="n">y</span> <span class="o">+</span> <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="n">y</span> <span class="o">-</span> <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span> <span class="o">/</span> <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">67</span><span class="p">,</span> <span class="o">.</span><span class="mi">75</span><span class="p">]</span>
</pre></div>
</div>
<p>See below for details on broadcasting in numpy.</p>
</div>
<div class="section" id="dot-product">
<h4><a class="toc-backref" href="#toc-entry-6">Dot product</a><a class="headerlink" href="#dot-product" title="Permalink to this headline">¶</a></h4>
<p>The dot product of two vectors is a scalar. Dot product of vectors and matrices (matrix multiplication) is one of the most important operations in deep learning.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
a_1 \\
a_2 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
b_1 \\
b_2 \\
\end{bmatrix}
= a_1 b_1+a_2 b_2\end{split}\]</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="mi">20</span>
</pre></div>
</div>
</div>
<div class="section" id="hadamard-product">
<h4><a class="toc-backref" href="#toc-entry-7">Hadamard product</a><a class="headerlink" href="#hadamard-product" title="Permalink to this headline">¶</a></h4>
<p>Hadamard Product is elementwise multiplication and it outputs a vector.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
a_1 \\
a_2 \\
\end{bmatrix}
 \odot
\begin{bmatrix}
b_1 \\
b_2 \\
\end{bmatrix}
=
\begin{bmatrix}
a_1 \cdot b_1 \\
a_2 \cdot b_2 \\
\end{bmatrix}\end{split}\]</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="n">y</span> <span class="o">*</span> <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="vector-fields">
<h4><a class="toc-backref" href="#toc-entry-8">Vector fields</a><a class="headerlink" href="#vector-fields" title="Permalink to this headline">¶</a></h4>
<p>A vector field shows how far the point (x,y) would hypothetically move if we applied a vector function to it like addition or multiplication. Given a point in space, a vector field shows the power and direction of our proposed change at a variety of points in a graph <a class="footnote-reference" href="#footnote-2" id="footnote-reference-2">[2]</a>.</p>
<img alt="_images/vector_field.png" class="align-center" src="_images/vector_field.png" />
<p>This vector field is an interesting one since it moves in different directions depending the starting point. The reason is that the vector behind this field stores terms like <span class="math notranslate nohighlight">\(2x\)</span> or <span class="math notranslate nohighlight">\(x^2\)</span> instead of scalar values like -2 and 5. For each point on the graph, we plug the x-coordinate into <span class="math notranslate nohighlight">\(2x\)</span> or <span class="math notranslate nohighlight">\(x^2\)</span> and draw an arrow from the starting point to the new location. Vector fields are extremely useful for visualizing machine learning techniques like Gradient Descent.</p>
</div>
</div>
<div class="section" id="matrices">
<h3><a class="toc-backref" href="#toc-entry-9">Matrices</a><a class="headerlink" href="#matrices" title="Permalink to this headline">¶</a></h3>
<p>A matrix is a rectangular grid of numbers or terms (like an Excel spreadsheet) with special rules for addition, subtraction, and multiplication.</p>
<div class="section" id="dimensions">
<h4><a class="toc-backref" href="#toc-entry-10">Dimensions</a><a class="headerlink" href="#dimensions" title="Permalink to this headline">¶</a></h4>
<p>We describe the dimensions of a matrix in terms of rows by columns.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
2 &amp; 4 \\
5 &amp; -7 \\
12 &amp; 5 \\
\end{bmatrix}
\begin{bmatrix}
a² &amp; 2a &amp; 8\\
18 &amp; 7a-4 &amp; 10\\
\end{bmatrix}\end{split}\]</div>
<p>The first has dimensions (3,2). The second (2,3).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
 <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
 <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">b</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="scalar-operations-1">
<h4><a class="toc-backref" href="#toc-entry-11">Scalar operations</a><a class="headerlink" href="#scalar-operations-1" title="Permalink to this headline">¶</a></h4>
<p>Scalar operations with matrices work the same way as they do for vectors. Simply apply the scalar to every element in the matrix — add, subtract, divide, multiply, etc.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
2 &amp; 3 \\
2 &amp; 3 \\
2 &amp; 3 \\
\end{bmatrix}
+
1
=
\begin{bmatrix}
3 &amp; 4 \\
3 &amp; 4 \\
3 &amp; 4 \\
\end{bmatrix}\end{split}\]</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Addition</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]])</span>
<span class="n">a</span> <span class="o">+</span> <span class="mi">1</span>
<span class="p">[[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="section" id="elementwise-operations-1">
<h4><a class="toc-backref" href="#toc-entry-12">Elementwise operations</a><a class="headerlink" href="#elementwise-operations-1" title="Permalink to this headline">¶</a></h4>
<p>In order to add, subtract, or divide two matrices they must have equal dimensions. We combine corresponding values in an elementwise fashion to produce a new matrix.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
a &amp; b \\
c &amp; d \\
\end{bmatrix}
+
\begin{bmatrix}
1 &amp; 2\\
3 &amp; 4 \\
\end{bmatrix}
=
\begin{bmatrix}
a+1 &amp; b+2\\
c+3 &amp; d+4 \\
\end{bmatrix}\end{split}\]</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>a = np.array([
 [1,2],
 [3,4]])
b = np.array([
 [1,2],
 [3,4]])

a + b
[[2, 4],
 [6, 8]]

a — b
[[0, 0],
 [0, 0]]
</pre></div>
</div>
</div>
<div class="section" id="hadamard-product-1">
<h4><a class="toc-backref" href="#toc-entry-13">Hadamard&nbsp;product</a><a class="headerlink" href="#hadamard-product-1" title="Permalink to this headline">¶</a></h4>
<p>Hadamard product of matrices is an elementwise operation. Values that correspond positionally are multiplied to produce a new matrix.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
a_1 &amp; a_2 \\
a_3 &amp; a_4 \\
\end{bmatrix}
\odot
\begin{bmatrix}
b_1 &amp; b_2 \\
b_3 &amp; b_4 \\
\end{bmatrix}
=
\begin{bmatrix}
a_1 \cdot b_1 &amp; a_2 \cdot b_2 \\
a_3 \cdot b_3 &amp; a_4 \cdot b_4 \\
\end{bmatrix}\end{split}\]</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span class="p">[[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span class="p">[[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>

<span class="c1"># Uses python&#39;s multiply operator</span>
<span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
<span class="p">[[</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">18</span><span class="p">]]</span>
</pre></div>
</div>
<p>In numpy you can take the Hadamard product of a matrix and vector as long as their dimensions meet the requirements of broadcasting.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
{a_1} \\
{a_2} \\
\end{bmatrix}
\odot
\begin{bmatrix}
b_1 &amp; b_2 \\
b_3 &amp; b_4 \\
\end{bmatrix}
=
\begin{bmatrix}
a_1 \cdot b_1 &amp; a_1 \cdot b_2 \\
a_2 \cdot b_3 &amp; a_2 \cdot b_4 \\
\end{bmatrix}\end{split}\]</div>
</div>
<div class="section" id="matrix-transpose">
<h4><a class="toc-backref" href="#toc-entry-14">Matrix transpose</a><a class="headerlink" href="#matrix-transpose" title="Permalink to this headline">¶</a></h4>
<p>Neural networks frequently process weights and inputs of different sizes where the dimensions do not meet the requirements of matrix multiplication. Matrix transposition (often denoted by a superscript ‘T’ e.g. M^T) provides a way to “rotate” one of the matrices so that the operation complies with multiplication requirements and can continue. There are two steps to transpose a matrix:</p>
<blockquote>
<div><ol class="arabic simple">
<li>Rotate the matrix right 90°</li>
<li>Reverse the order of elements in each row (e.g. [a b c] becomes [c b a])</li>
</ol>
</div></blockquote>
<p>As an example, transpose matrix M into T:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
a &amp; b \\
c &amp; d \\
e &amp; f \\
\end{bmatrix}
\quad \Rightarrow \quad
\begin{bmatrix}
a &amp; c &amp; e \\
b &amp; d &amp; f \\
\end{bmatrix}\end{split}\]</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
   <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
   <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>

<span class="n">a</span><span class="o">.</span><span class="n">T</span>
<span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="section" id="matrix-multiplication">
<h4><a class="toc-backref" href="#toc-entry-15">Matrix multiplication</a><a class="headerlink" href="#matrix-multiplication" title="Permalink to this headline">¶</a></h4>
<p>Matrix multiplication specifies a set of rules for multiplying matrices together to produce a new matrix.</p>
<p><strong>Rules</strong></p>
<p>Not all matrices are eligible for multiplication. In addition, there is a requirement on the dimensions of the resulting matrix output. Source.</p>
<blockquote>
<div><ol class="arabic simple">
<li>The number of columns of the 1st matrix must equal the number of rows of the 2nd</li>
<li>The product of an M x N matrix and an N x K matrix is an M x K matrix. The new matrix takes the rows of the 1st and columns of the 2nd</li>
</ol>
</div></blockquote>
<p><strong>Steps</strong></p>
<p>Matrix multiplication relies on dot product to multiply various combinations of rows and columns. In the image below, taken from Khan Academy’s excellent linear algebra course, each entry in Matrix C is the dot product of a row in matrix A and a column in matrix B <a class="footnote-reference" href="#footnote-3" id="footnote-reference-3">[3]</a>.</p>
<img alt="_images/khan_academy_matrix_product.png" class="align-center" src="_images/khan_academy_matrix_product.png" />
<p>The operation a1 · b1 means we take the dot product of the 1st row in matrix A (1, 7) and the 1st column in matrix B (3, 5).</p>
<div class="math notranslate nohighlight">
\[\begin{split}a_1 \cdot b_1 =
\begin{bmatrix}
1 \\
7 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
3 \\
5 \\
\end{bmatrix}
= (1 \cdot 3) + (7 \cdot 5) = 38\end{split}\]</div>
<p>Here’s another way to look at it:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
a &amp; b \\
c &amp; d \\
e &amp; f \\
\end{bmatrix}
\cdot
\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4 \\
\end{bmatrix}
=
\begin{bmatrix}
1a + 3b &amp; 2a + 4b \\
1c + 3d &amp; 2c + 4d \\
1e + 3f &amp; 2e + 4f \\
\end{bmatrix}\end{split}\]</div>
</div>
<div class="section" id="test-yourself">
<h4><a class="toc-backref" href="#toc-entry-16">Test yourself</a><a class="headerlink" href="#test-yourself" title="Permalink to this headline">¶</a></h4>
<ol class="arabic simple">
<li>What are the dimensions of the matrix product?</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
1 &amp; 2 \\
5 &amp; 6 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
5 &amp; 6 &amp; 7 \\
\end{bmatrix}
= \text{2 x 3}\end{split}\]</div>
<ol class="arabic simple" start="2">
<li>What are the dimensions of the matrix product?</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
1 &amp; 2 &amp; 3 &amp; 4 \\
5 &amp; 6 &amp; 7 &amp; 8 \\
9 &amp; 10 &amp; 11 &amp; 12 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
1 &amp; 2 \\
5 &amp; 6 \\
3 &amp; 0 \\
2 &amp; 1 \\
\end{bmatrix}
= \text{3 x 2}\end{split}\]</div>
<ol class="arabic simple" start="3">
<li>What is the matrix product?</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
2 &amp; 3 \\
1 &amp; 4 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
5 &amp; 4 \\
3 &amp; 5 \\
\end{bmatrix}
=
\begin{bmatrix}
19 &amp; 23 \\
17 &amp; 24 \\
\end{bmatrix}\end{split}\]</div>
<ol class="arabic simple" start="4">
<li>What is the matrix product?}</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
3 \\
5 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
1 &amp; 2 &amp; 3\\
\end{bmatrix}
=
\begin{bmatrix}
3 &amp; 6 &amp; 9 \\
5 &amp; 10 &amp; 15 \\
\end{bmatrix}\end{split}\]</div>
<ol class="arabic simple" start="5">
<li>What is the matrix product?</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
1 &amp; 2 &amp; 3\\
\end{bmatrix}
\cdot
\begin{bmatrix}
4 \\
5 \\
6 \\
\end{bmatrix}
=
\begin{bmatrix}
32 \\
\end{bmatrix}\end{split}\]</div>
</div>
</div>
<div class="section" id="numpy">
<h3><a class="toc-backref" href="#toc-entry-17">Numpy</a><a class="headerlink" href="#numpy" title="Permalink to this headline">¶</a></h3>
<div class="section" id="dot-product-1">
<h4><a class="toc-backref" href="#toc-entry-18">Dot product</a><a class="headerlink" href="#dot-product-1" title="Permalink to this headline">¶</a></h4>
<p>Numpy uses the function np.dot(A,B) for both vector and matrix multiplication. It has some other interesting features and gotchas so I encourage you to read the documentation here before use. Also, to multiply two matrices A and B, you can use the expression A &#64; B.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
 <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
 <span class="p">])</span>
<span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
 <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>
 <span class="p">])</span>
<span class="n">b</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Multiply</span>
<span class="n">mm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="c1"># or a @ b</span>
<span class="n">mm</span> <span class="o">==</span> <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">16</span><span class="p">]</span>
<span class="n">mm</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="broadcasting">
<h4><a class="toc-backref" href="#toc-entry-19">Broadcasting</a><a class="headerlink" href="#broadcasting" title="Permalink to this headline">¶</a></h4>
<p>In numpy the dimension requirements for elementwise operations are relaxed via a mechanism called broadcasting. Two matrices are compatible if the corresponding dimensions in each matrix (rows vs rows, columns vs columns) meet the following requirements:</p>
<blockquote>
<div><ol class="arabic simple">
<li>The dimensions are equal, or</li>
<li>One dimension is of size 1</li>
</ol>
</div></blockquote>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
 <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
 <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
 <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
<span class="p">])</span>

<span class="c1"># Same no. of rows</span>
<span class="c1"># Different no. of columns</span>
<span class="c1"># but a has one column so this works</span>
<span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
<span class="p">[[</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">]]</span>

<span class="c1"># Same no. of columns</span>
<span class="c1"># Different no. of rows</span>
<span class="c1"># but c has one row so this works</span>
<span class="n">b</span> <span class="o">*</span> <span class="n">c</span>
<span class="p">[[</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">12</span><span class="p">]]</span>

<span class="c1"># Different no. of columns</span>
<span class="c1"># Different no. of rows</span>
<span class="c1"># but both a and c meet the</span>
<span class="c1"># size 1 requirement rule</span>
<span class="n">a</span> <span class="o">+</span> <span class="n">c</span>
<span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
</pre></div>
</div>
<p class="rubric">Tutorials</p>
<ul class="simple">
<li><a class="reference external" href="https://medium.com/r/?url=https%3A%2F%2Fwww.khanacademy.org%2Fmath%2Flinear-algebra">Khan Academy Linear Algebra</a></li>
<li><a class="reference external" href="https://medium.com/r/?url=http%3A%2F%2Fwww.deeplearningbook.org%2Fcontents%2Fpart_basics.html">Deep Learning Book Math</a></li>
<li><a class="reference external" href="https://medium.com/r/?url=https%3A%2F%2Fwww.coursera.org%2Flearn%2Fmachine-learning%2Fresources%2FJXWWS">Andrew Ng Course Notes</a></li>
<li><a class="reference external" href="https://medium.com/r/?url=https%3A%2F%2Fbetterexplained.com%2Farticles%2Flinear-algebra-guide%2F">Linear Algebra Better Explained</a></li>
<li><a class="reference external" href="https://medium.com/r/?url=http%3A%2F%2Fblog.stata.com%2F2011%2F03%2F03%2Funderstanding-matrices-intuitively-part-1%2F">Understanding Matrices Intuitively</a></li>
<li><a class="reference external" href="https://medium.com/r/?url=http%3A%2F%2Fwww.holehouse.org%2Fmlclass%2F03_Linear_algebra_review.html">Intro To Linear Algebra</a></li>
<li><a class="reference external" href="https://medium.com/r/?url=http%3A%2F%2Fimmersivemath.com%2Fila%2Findex.html">Immersive Math</a></li>
</ul>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-1">[1]</a></td><td><a class="reference external" href="http://mathinsight.org/vector_introduction">http://mathinsight.org/vector_introduction</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-2">[2]</a></td><td><a class="reference external" href="https://en.wikipedia.org/wiki/Vector_field">https://en.wikipedia.org/wiki/Vector_field</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-3">[3]</a></td><td><a class="reference external" href="https://www.khanacademy.org/math/precalculus/precalc-matrices/properties-of-matrix-multiplication/a/properties-of-matrix-multiplication">https://www.khanacademy.org/math/precalculus/precalc-matrices/properties-of-matrix-multiplication/a/properties-of-matrix-multiplication</a></td></tr>
</tbody>
</table>
</div>
</div>
</div>
<span id="document-probability"></span><div class="section" id="probability-1">
<span id="probability"></span><h2>Probability<a class="headerlink" href="#probability-1" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#links" id="toc-entry-1">Links</a></li>
<li><a class="reference internal" href="#screenshots" id="toc-entry-2">Screenshots</a></li>
<li><a class="reference internal" href="#license" id="toc-entry-3">License</a></li>
</ul>
</div>
<p>Basic concepts in probability for machine learning.</p>
<p>This cheatsheet is a 10-page reference in probability that covers a semester’s worth of introductory probability.</p>
<p>The cheatsheet is based off of Harvard’s introductory probability course, Stat 110. It is co-authored by former Stat 110 Teaching Fellow William Chen and Stat 110 Professor Joe Blitzstein.</p>
<div class="section" id="links">
<h3><a class="toc-backref" href="#toc-entry-1">Links</a><a class="headerlink" href="#links" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>[Probability Cheatsheet PDF](<a class="reference external" href="http://www.wzchen.com/probability-cheatsheet/">http://www.wzchen.com/probability-cheatsheet/</a>)</li>
</ul>
</div>
<div class="section" id="screenshots">
<h3><a class="toc-backref" href="#toc-entry-2">Screenshots</a><a class="headerlink" href="#screenshots" title="Permalink to this headline">¶</a></h3>
<p>![First Page](<a class="reference external" href="http://i.imgur.com/Oa73huL.jpg">http://i.imgur.com/Oa73huL.jpg</a>)
![Second Page](<a class="reference external" href="http://i.imgur.com/dyvW2rB.jpg">http://i.imgur.com/dyvW2rB.jpg</a>)</p>
</div>
<div class="section" id="license">
<h3><a class="toc-backref" href="#toc-entry-3">License</a><a class="headerlink" href="#license" title="Permalink to this headline">¶</a></h3>
<p>This work is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.][by-nc-sa].</p>
<p>[![Creative Commons License][by-nc-sa-img]][by-nc-sa]</p>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Example</td></tr>
</tbody>
</table>
</div>
</div>
<span id="document-statistics"></span><div class="section" id="statistics-1">
<span id="statistics"></span><h2>Statistics<a class="headerlink" href="#statistics-1" title="Permalink to this headline">¶</a></h2>
<p>Basic concepts in statistics for machine learning.</p>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Example</td></tr>
</tbody>
</table>
</div>
<span id="document-math_notation"></span><div class="section" id="notation">
<span id="math-notation"></span><h2>Notation<a class="headerlink" href="#notation" title="Permalink to this headline">¶</a></h2>
<p>Commonly used math symbols in machine learning texts.</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#algebra" id="toc-entry-1">Algebra</a></li>
<li><a class="reference internal" href="#calculus" id="toc-entry-2">Calculus</a></li>
<li><a class="reference internal" href="#linear-algebra" id="toc-entry-3">Linear algebra</a></li>
<li><a class="reference internal" href="#probability" id="toc-entry-4">Probability</a></li>
<li><a class="reference internal" href="#set-theory" id="toc-entry-5">Set theory</a></li>
<li><a class="reference internal" href="#statistics" id="toc-entry-6">Statistics</a></li>
</ul>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Use the <a class="reference external" href="http://www.tablesgenerator.com/text_tables">table generator</a> to quickly add new symbols.
Import current tables into tablesgenerator from <code class="docutils literal notranslate"><span class="pre">figures/*.tgn</span></code>. Export and save your changes. Also
see helpful <a class="reference external" href="https://www.sublimetext.com/docs/3/multiple_selection_with_the_keyboard.html">multiline editing</a> in Sublime.</p>
</div>
<div class="section" id="algebra">
<h3><a class="toc-backref" href="#toc-entry-1">Algebra</a><a class="headerlink" href="#algebra" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="23%" />
<col width="23%" />
<col width="26%" />
<col width="28%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Symbol</strong></td>
<td><strong>Name</strong></td>
<td><strong>Description</strong></td>
<td><strong>Example</strong></td>
</tr>
<tr class="row-even"><td><span class="math notranslate nohighlight">\((f ∘ g)\)</span></td>
<td>composite function</td>
<td>a nested function</td>
<td>(f ∘ g)(x) = f(g(x))</td>
</tr>
<tr class="row-odd"><td><span class="math notranslate nohighlight">\(∆\)</span></td>
<td>delta</td>
<td>change / difference</td>
<td>∆x = x_1 - x_0</td>
</tr>
<tr class="row-even"><td><span class="math notranslate nohighlight">\(e\)</span></td>
<td>Euler’s number</td>
<td>e = 2.718281828</td>
<td>s = frac{1}{1+e^{-z}}</td>
</tr>
<tr class="row-odd"><td><span class="math notranslate nohighlight">\(\sum\)</span></td>
<td>summation</td>
<td>sum of all values</td>
<td>∑ x_i = x_1 + x_2 + x_3</td>
</tr>
<tr class="row-even"><td><span class="math notranslate nohighlight">\(\prod\)</span></td>
<td>capital pi</td>
<td>product of all values</td>
<td>∏ x_i = x_1∙x_2∙x_3</td>
</tr>
<tr class="row-odd"><td><span class="math notranslate nohighlight">\(\epsilon\)</span></td>
<td>epsilon</td>
<td>tiny number near 0</td>
<td>lr = 1e-4</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="calculus">
<h3><a class="toc-backref" href="#toc-entry-2">Calculus</a><a class="headerlink" href="#calculus" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="23%" />
<col width="22%" />
<col width="40%" />
<col width="15%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Symbol</strong></td>
<td><strong>Name</strong></td>
<td><strong>Description</strong></td>
<td><strong>Example</strong></td>
</tr>
<tr class="row-even"><td><span class="math notranslate nohighlight">\(x'\)</span></td>
<td>derivative</td>
<td>first derivative</td>
<td>(x^2)’ = 2x</td>
</tr>
<tr class="row-odd"><td><span class="math notranslate nohighlight">\(x''\)</span></td>
<td>second derivative</td>
<td>second derivative</td>
<td>(x^2)’’ = 2</td>
</tr>
<tr class="row-even"><td><span class="math notranslate nohighlight">\(\lim\)</span></td>
<td>limit</td>
<td>function value as x approaches 0</td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td><span class="math notranslate nohighlight">\(∇\)</span></td>
<td>nabla</td>
<td>gradient</td>
<td>∇f(a,b,c)</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="linear-algebra">
<h3><a class="toc-backref" href="#toc-entry-3">Linear algebra</a><a class="headerlink" href="#linear-algebra" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="19%" />
<col width="13%" />
<col width="36%" />
<col width="33%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Symbol</strong></td>
<td><strong>Name</strong></td>
<td><strong>Description</strong></td>
<td><strong>Example</strong></td>
</tr>
<tr class="row-even"><td><span class="math notranslate nohighlight">\([ ]\)</span></td>
<td>brackets</td>
<td>matrix or vector</td>
<td><span class="math notranslate nohighlight">\(M = [1 3 5]\)</span></td>
</tr>
<tr class="row-odd"><td><span class="math notranslate nohighlight">\(\cdot\)</span></td>
<td>dot</td>
<td>dot product</td>
<td><span class="math notranslate nohighlight">\((Z = X \cdot W\)</span></td>
</tr>
<tr class="row-even"><td><span class="math notranslate nohighlight">\(\odot\)</span></td>
<td>hadamard</td>
<td>hadamard product</td>
<td><span class="math notranslate nohighlight">\(A = B \odot C\)</span></td>
</tr>
<tr class="row-odd"><td><span class="math notranslate nohighlight">\(X^T\)</span></td>
<td>transpose</td>
<td>matrix transpose</td>
<td><span class="math notranslate nohighlight">\(W^T \cdot X\)</span></td>
</tr>
<tr class="row-even"><td><span class="math notranslate nohighlight">\(\vec x\)</span></td>
<td>vector</td>
<td>vector</td>
<td><span class="math notranslate nohighlight">\(v = [1 2 3]\)</span></td>
</tr>
<tr class="row-odd"><td><span class="math notranslate nohighlight">\(X\)</span></td>
<td>matrix</td>
<td>capitalized variables are matrices</td>
<td><span class="math notranslate nohighlight">\(X, W, B\)</span></td>
</tr>
<tr class="row-even"><td><span class="math notranslate nohighlight">\(\hat x\)</span></td>
<td>unit vector</td>
<td>vector of magnitude 1</td>
<td><span class="math notranslate nohighlight">\(\hat x = [0.2 0.5 0.3]\)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="probability">
<h3><a class="toc-backref" href="#toc-entry-4">Probability</a><a class="headerlink" href="#probability" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="16%" />
<col width="25%" />
<col width="31%" />
<col width="28%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Symbol</strong></td>
<td><strong>Name</strong></td>
<td><strong>Description</strong></td>
<td><strong>Example</strong></td>
</tr>
<tr class="row-even"><td><span class="math notranslate nohighlight">\(P(A)\)</span></td>
<td>probability</td>
<td>probability of event  A</td>
<td>P(x=1) = 0.5</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="set-theory">
<h3><a class="toc-backref" href="#toc-entry-5">Set theory</a><a class="headerlink" href="#set-theory" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="14%" />
<col width="25%" />
<col width="34%" />
<col width="27%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Symbol</strong></td>
<td><strong>Name</strong></td>
<td><strong>Description</strong></td>
<td><strong>Example</strong></td>
</tr>
<tr class="row-even"><td><span class="math notranslate nohighlight">\({ }\)</span></td>
<td>set</td>
<td>list of distinct elements</td>
<td>S = {1, 5, 7, 9}</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="statistics">
<h3><a class="toc-backref" href="#toc-entry-6">Statistics</a><a class="headerlink" href="#statistics" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="19%" />
<col width="22%" />
<col width="35%" />
<col width="24%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Symbol</strong></td>
<td><strong>Name</strong></td>
<td><strong>Description</strong></td>
<td><strong>Example</strong></td>
</tr>
<tr class="row-even"><td><span class="math notranslate nohighlight">\(μ\)</span></td>
<td>population mean</td>
<td>mean of population values</td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td><span class="math notranslate nohighlight">\(\bar x\)</span></td>
<td>sample mean</td>
<td>mean of subset of population</td>
<td>&#160;</td>
</tr>
<tr class="row-even"><td><span class="math notranslate nohighlight">\(σ^2\)</span></td>
<td>population variance</td>
<td>variance of population value</td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td><span class="math notranslate nohighlight">\(s^2\)</span></td>
<td>sample variance</td>
<td>variance of subset of population</td>
<td>&#160;</td>
</tr>
<tr class="row-even"><td><span class="math notranslate nohighlight">\(σ_X\)</span></td>
<td>standard deviation</td>
<td>population standard deviation</td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td><span class="math notranslate nohighlight">\(s\)</span></td>
<td>sample std dev</td>
<td>standard deviation of sample</td>
<td>&#160;</td>
</tr>
<tr class="row-even"><td><span class="math notranslate nohighlight">\(ρX\)</span></td>
<td>correlation</td>
<td>correlation of variables X and Y</td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td><span class="math notranslate nohighlight">\(\tilde x\)</span></td>
<td>median</td>
<td>median value of variable x</td>
<td>&#160;</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="http://www.tablesgenerator.com/text_tables">http://www.tablesgenerator.com/text_tables</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="http://www.rapidtables.com/math/symbols/Basic_Math_Symbols.htm">http://www.rapidtables.com/math/symbols/Basic_Math_Symbols.htm</a></td></tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-nn_concepts"></span><div class="section" id="concepts">
<span id="nn-concepts"></span><h2>Concepts<a class="headerlink" href="#concepts" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#neural-network" id="toc-entry-1">Neural Network</a></li>
<li><a class="reference internal" href="#neuron" id="toc-entry-2">Neuron</a></li>
<li><a class="reference internal" href="#synapse" id="toc-entry-3">Synapse</a></li>
<li><a class="reference internal" href="#weights" id="toc-entry-4">Weights</a></li>
<li><a class="reference internal" href="#bias" id="toc-entry-5">Bias</a></li>
<li><a class="reference internal" href="#layers" id="toc-entry-6">Layers</a></li>
<li><a class="reference internal" href="#weighted-input" id="toc-entry-7">Weighted Input</a></li>
<li><a class="reference internal" href="#activation-functions" id="toc-entry-8">Activation Functions</a></li>
<li><a class="reference internal" href="#loss-functions" id="toc-entry-9">Loss Functions</a></li>
<li><a class="reference internal" href="#optimization-algorithms" id="toc-entry-10">Optimization Algorithms</a></li>
<li><a class="reference internal" href="#gradient-accumulation" id="toc-entry-11">Gradient Accumulation</a></li>
</ul>
</div>
<div class="section" id="neural-network">
<h3><a class="toc-backref" href="#toc-entry-1">Neural Network</a><a class="headerlink" href="#neural-network" title="Permalink to this headline">¶</a></h3>
<p>Neural networks are a class of machine learning algorithms used to model complex patterns in datasets using multiple hidden layers and non-linear activation functions. A neural network takes an input, passes it through multiple layers of hidden neurons (mini-functions with unique coefficients that must be learned), and outputs a prediction representing the combined input of all the neurons.</p>
<img alt="_images/neural_network_w_matrices.png" class="align-center" src="_images/neural_network_w_matrices.png" />
<p>Neural networks are trained iteratively using optimization techniques like gradient descent. After each cycle of training, an error metric is calculated based on the difference between prediction and target. The derivatives of this error metric are calculated and propagated back through the network using a technique called backpropagation. Each neuron’s coefficients (weights) are then adjusted relative to how much they contributed to the total error. This process is repeated iteratively until the network error drops below an acceptable threshold.</p>
</div>
<div class="section" id="neuron">
<h3><a class="toc-backref" href="#toc-entry-2">Neuron</a><a class="headerlink" href="#neuron" title="Permalink to this headline">¶</a></h3>
<p>A neuron takes a group of weighted inputs, applies an activation function, and returns an output.</p>
<img alt="_images/neuron.png" class="align-center" src="_images/neuron.png" />
<p>Inputs to a neuron can either be features from a training set or outputs from a previous layer’s neurons. Weights are applied to the inputs as they travel along synapses to reach the neuron. The neuron then applies an activation function to the “sum of weighted inputs” from each incoming synapse and passes the result on to all the neurons in the next layer.</p>
</div>
<div class="section" id="synapse">
<h3><a class="toc-backref" href="#toc-entry-3">Synapse</a><a class="headerlink" href="#synapse" title="Permalink to this headline">¶</a></h3>
<p>Synapses are like roads in a neural network. They connect inputs to neurons, neurons to neurons, and neurons to outputs. In order to get from one neuron to another, you have to travel along the synapse paying the “toll” (weight) along the way. Each connection between two neurons has a unique synapse with a unique weight attached to it. When we talk about updating weights in a network, we’re really talking about adjusting the weights on these synapses.</p>
</div>
<div class="section" id="weights">
<span id="nn-weights"></span><h3><a class="toc-backref" href="#toc-entry-4">Weights</a><a class="headerlink" href="#weights" title="Permalink to this headline">¶</a></h3>
<p>Weights are values that control the strength of the connection between two neurons. That is, inputs are typically multiplied by weights, and that defines how much influence the input will have on the output. In other words: when the inputs are transmitted between neurons, the weights are applied to the inputs along with an additional value (the bias)</p>
</div>
<div class="section" id="bias">
<span id="nn-bias"></span><h3><a class="toc-backref" href="#toc-entry-5">Bias</a><a class="headerlink" href="#bias" title="Permalink to this headline">¶</a></h3>
<p>Bias terms are additional constants attached to neurons and added to the weighted input before the activation function is applied. Bias terms help models represent patterns that do not necessarily pass through the origin. For example, if all your features were 0, would your output also be zero? Is it possible there is some base value upon which your features have an effect? Bias terms typically accompany weights and must also be learned by your model.</p>
</div>
<div class="section" id="layers">
<h3><a class="toc-backref" href="#toc-entry-6">Layers</a><a class="headerlink" href="#layers" title="Permalink to this headline">¶</a></h3>
<img alt="_images/neural_network_simple.png" class="align-center" src="_images/neural_network_simple.png" />
<p class="rubric">Input Layer</p>
<p>Holds the data your model will train on. Each neuron in the input layer represents a unique attribute in your dataset (e.g. height, hair color, etc.).</p>
<p class="rubric">Hidden Layer</p>
<p>Sits between the input and output layers and applies an activation function before passing on the results. There are often multiple hidden layers in a network. In traditional networks, hidden layers are typically fully-connected layers — each neuron receives input from all the previous layer’s neurons and sends its output to every neuron in the next layer. This contrasts with how convolutional layers work where the neurons send their output to only some of the neurons in the next layer.</p>
<p class="rubric">Output Layer</p>
<p>The final layer in a network. It receives input from the previous hidden layer, optionally applies an activation function, and returns an output representing your model’s prediction.</p>
</div>
<div class="section" id="weighted-input">
<h3><a class="toc-backref" href="#toc-entry-7">Weighted Input</a><a class="headerlink" href="#weighted-input" title="Permalink to this headline">¶</a></h3>
<p>A neuron’s input equals the sum of weighted outputs from all neurons in the previous layer. Each input is multiplied by the weight associated with the synapse connecting the input to the current neuron. If there are 3 inputs or neurons in the previous layer, each neuron in the current layer will have 3 distinct weights — one for each each synapse.</p>
<p><strong>Single Input</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}Z &amp;= Input \cdot Weight \\
  &amp;= X W\end{split}\]</div>
<p><strong>Multiple Inputs</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}Z &amp;= \sum_{i=1}^{n}x_i w_i \\
  &amp;= x_1 w_1 + x_2 w_2 + x_3 w_3\end{split}\]</div>
<p>Notice, it’s exactly the same equation we use with linear regression! In fact, a neural network with a single neuron is the same as linear regression! The only difference is the neural network post-processes the weighted input with an activation function.</p>
</div>
<div class="section" id="activation-functions">
<h3><a class="toc-backref" href="#toc-entry-8">Activation Functions</a><a class="headerlink" href="#activation-functions" title="Permalink to this headline">¶</a></h3>
<p>Activation functions live inside neural network layers and modify the data they receive before passing it to the next layer. Activation functions give neural networks their power — allowing them to model complex non-linear relationships. By modifying inputs with non-linear functions neural networks can model highly complex relationships between features. Popular activation functions include <a class="reference internal" href="index.html#activation-relu"><span class="std std-ref">relu</span></a> and <a class="reference internal" href="index.html#activation-sigmoid"><span class="std std-ref">sigmoid</span></a>.</p>
<p>Activation functions typically have the following properties:</p>
<blockquote>
<div><ul class="simple">
<li><strong>Non-linear</strong> - In linear regression we’re limited to a prediction equation that looks like a straight line. This is nice for simple datasets with a one-to-one relationship between inputs and outputs, but what if the patterns in our dataset were non-linear? (e.g. <span class="math notranslate nohighlight">\(x^2\)</span>, sin, log). To model these relationships we need a non-linear prediction equation.¹ Activation functions provide this non-linearity.</li>
<li><strong>Continuously differentiable</strong> — To improve our model with gradient descent, we need our output to have a nice slope so we can compute error derivatives with respect to weights. If our neuron instead outputted 0 or 1 (perceptron), we wouldn’t know in which direction to update our weights to reduce our error.</li>
<li><strong>Fixed Range</strong> — Activation functions typically squash the input data into a narrow range that makes training the model more stable and efficient.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="loss-functions">
<h3><a class="toc-backref" href="#toc-entry-9">Loss Functions</a><a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h3>
<p>A loss function, or cost function, is a wrapper around our model’s predict function that tells us “how good” the model is at making predictions for a given set of parameters. The loss function has its own curve and its own derivatives. The slope of this curve tells us how to change our parameters to make the model more accurate! We use the model to make predictions. We use the cost function to update our parameters. Our cost function can take a variety of forms as there are many different cost functions available. Popular loss functions include: <a class="reference internal" href="index.html#mse"><span class="std std-ref">MSE (L2)</span></a> and <a class="reference internal" href="index.html#loss-cross-entropy"><span class="std std-ref">Cross-entropy Loss</span></a>.</p>
</div>
<div class="section" id="optimization-algorithms">
<h3><a class="toc-backref" href="#toc-entry-10">Optimization Algorithms</a><a class="headerlink" href="#optimization-algorithms" title="Permalink to this headline">¶</a></h3>
<p>Be the first to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet">contribute!</a></p>
</div>
<div class="section" id="gradient-accumulation">
<h3><a class="toc-backref" href="#toc-entry-11">Gradient Accumulation</a><a class="headerlink" href="#gradient-accumulation" title="Permalink to this headline">¶</a></h3>
<p>Gradient accumulation is a mechanism to split the batch of samples—used for training a neural network—into several mini-batches of samples that will be run sequentially.</p>
<p>This is used to enable using large batch sizes that require more GPU memory than available. Gradient accumulation helps in doing so by using mini-batches that require an amount of GPU memory that can be satisfied.</p>
<p>Gradient accumulation means running all mini-batches sequentially (generally on the same GPU) while accumulating their calculated gradients and not updating the model variables - the weights and biases of the model.
The model variables must not be updated during the accumulation in order to ensure all mini-batches use the same model variable values to calculate their gradients.
Only after accumulating the gradients of all those mini-batches will we generate and apply the updates for the model variables.</p>
<p>This results in the same updates for the model parameters as if we were to use the global batch.</p>
<img alt="_images/gradient_accumulation.png" class="align-center" src="_images/gradient_accumulation.png" />
<p>More details, a technical and algorithmical deep-dive, how-to tutorials, and examples can be found at [2].</p>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="http://sebastianruder.com/optimizing-gradient-descent/">http://sebastianruder.com/optimizing-gradient-descent/</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="https://github.com/run-ai/runai/tree/master/runai/ga/">https://github.com/run-ai/runai/tree/master/runai/ga/</a></td></tr>
</tbody>
</table>
</div>
</div>
<span id="document-forwardpropagation"></span><div class="section" id="forwardpropagation-1">
<span id="forwardpropagation"></span><h2>Forwardpropagation<a class="headerlink" href="#forwardpropagation-1" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#simple-network" id="toc-entry-1">Simple Network</a><ul>
<li><a class="reference internal" href="#steps" id="toc-entry-2">Steps</a></li>
<li><a class="reference internal" href="#code" id="toc-entry-3">Code</a></li>
</ul>
</li>
<li><a class="reference internal" href="#larger-network" id="toc-entry-4">Larger Network</a><ul>
<li><a class="reference internal" href="#architecture" id="toc-entry-5">Architecture</a></li>
<li><a class="reference internal" href="#weight-initialization" id="toc-entry-6">Weight Initialization</a></li>
<li><a class="reference internal" href="#bias-terms" id="toc-entry-7">Bias Terms</a></li>
<li><a class="reference internal" href="#working-with-matrices" id="toc-entry-8">Working with Matrices</a></li>
<li><a class="reference internal" href="#dynamic-resizing" id="toc-entry-9">Dynamic Resizing</a></li>
<li><a class="reference internal" href="#refactoring-our-code" id="toc-entry-10">Refactoring Our Code</a></li>
<li><a class="reference internal" href="#final-result" id="toc-entry-11">Final Result</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="simple-network">
<h3><a class="toc-backref" href="#toc-entry-1">Simple Network</a><a class="headerlink" href="#simple-network" title="Permalink to this headline">¶</a></h3>
<img alt="_images/neural_network_simple.png" class="align-center" src="_images/neural_network_simple.png" />
<p>Forward propagation is how neural networks make predictions. Input data is “forward propagated” through the network layer by layer to the final layer which outputs a prediction. For the toy neural network above, a single pass of forward propagation translates mathematically to:</p>
<div class="math notranslate nohighlight">
\[Prediction = A(\;A(\;X W_h\;)W_o\;)\]</div>
<p>Where <span class="math notranslate nohighlight">\(A\)</span> is an activation function like <a class="reference internal" href="index.html#activation-relu"><span class="std std-ref">ReLU</span></a>, <span class="math notranslate nohighlight">\(X\)</span> is the input and <span class="math notranslate nohighlight">\(W_h\)</span> and <span class="math notranslate nohighlight">\(W_o\)</span> are weights.</p>
<div class="section" id="steps">
<h4><a class="toc-backref" href="#toc-entry-2">Steps</a><a class="headerlink" href="#steps" title="Permalink to this headline">¶</a></h4>
<ol class="arabic simple">
<li>Calculate the weighted input to the hidden layer by multiplying <span class="math notranslate nohighlight">\(X\)</span> by the hidden weight <span class="math notranslate nohighlight">\(W_h\)</span></li>
<li>Apply the activation function and pass the result to the final layer</li>
<li>Repeat step 2 except this time <span class="math notranslate nohighlight">\(X\)</span> is replaced by the hidden layer’s output, <span class="math notranslate nohighlight">\(H\)</span></li>
</ol>
</div>
<div class="section" id="code">
<h4><a class="toc-backref" href="#toc-entry-3">Code</a><a class="headerlink" href="#code" title="Permalink to this headline">¶</a></h4>
<p>Let’s write a method feed_forward() to propagate input data through our simple network of 1 hidden layer. The output of this method represents our model’s prediction.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">z</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">Wo</span><span class="p">):</span>
    <span class="c1"># Hidden layer</span>
    <span class="n">Zh</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">Wh</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Zh</span><span class="p">)</span>

    <span class="c1"># Output layer</span>
    <span class="n">Zo</span> <span class="o">=</span> <span class="n">H</span> <span class="o">*</span> <span class="n">Wo</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Zo</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">x</span></code> is the input to the network, <code class="docutils literal notranslate"><span class="pre">Zo</span></code> and <code class="docutils literal notranslate"><span class="pre">Zh</span></code> are the weighted inputs and <code class="docutils literal notranslate"><span class="pre">Wo</span></code> and <code class="docutils literal notranslate"><span class="pre">Wh</span></code> are the weights.</p>
</div>
</div>
<div class="section" id="larger-network">
<h3><a class="toc-backref" href="#toc-entry-4">Larger Network</a><a class="headerlink" href="#larger-network" title="Permalink to this headline">¶</a></h3>
<p>The simple network above is helpful for learning purposes, but in reality neural networks are much larger and more complex. Modern neural networks have many more hidden layers, more neurons per layer, more variables per input, more inputs per training set, and more output variables to predict. Here is a slightly larger network that will introduce us to matrices and the matrix operations used to train arbitrarily large neural networks.</p>
<img alt="_images/neural_network_w_matrices.png" class="align-center" src="_images/neural_network_w_matrices.png" />
<div class="section" id="architecture">
<h4><a class="toc-backref" href="#toc-entry-5">Architecture</a><a class="headerlink" href="#architecture" title="Permalink to this headline">¶</a></h4>
<p>To accomodate arbitrarily large inputs or outputs, we need to make our code more extensible by adding a few parameters to our network’s __init__ method: inputLayerSize, hiddenLayerSize, outputLayerSize. We’ll still limit ourselves to using one hidden layer, but now we can create layers of different sizes to respond to the different inputs or outputs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">INPUT_LAYER_SIZE</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">HIDDEN_LAYER_SIZE</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">OUTPUT_LAYER_SIZE</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
<div class="section" id="weight-initialization">
<h4><a class="toc-backref" href="#toc-entry-6">Weight Initialization</a><a class="headerlink" href="#weight-initialization" title="Permalink to this headline">¶</a></h4>
<p>Unlike last time where <code class="docutils literal notranslate"><span class="pre">Wh</span></code> and <code class="docutils literal notranslate"><span class="pre">Wo</span></code> were scalar numbers, our new weight variables will be numpy arrays. Each array will hold all the weights for its own layer — one weight for each synapse. Below we initialize each array with the numpy’s <code class="docutils literal notranslate"><span class="pre">np.random.randn(rows,</span> <span class="pre">cols)</span></code> method, which returns a matrix of random numbers drawn from a normal distribution with mean 0 and variance&nbsp;1.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">init_weights</span><span class="p">():</span>
    <span class="n">Wh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">INPUT_LAYER_SIZE</span><span class="p">,</span> <span class="n">HIDDEN_LAYER_SIZE</span><span class="p">)</span> <span class="o">*</span> \
                <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="o">/</span><span class="n">INPUT_LAYER_SIZE</span><span class="p">)</span>
    <span class="n">Wo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">HIDDEN_LAYER_SIZE</span><span class="p">,</span> <span class="n">OUTPUT_LAYER_SIZE</span><span class="p">)</span> <span class="o">*</span> \
                <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="o">/</span><span class="n">HIDDEN_LAYER_SIZE</span><span class="p">)</span>
</pre></div>
</div>
<p>Here’s an example calling <code class="docutils literal notranslate"><span class="pre">random.randn()</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="o">&gt;&gt;</span> <span class="p">[[</span><span class="o">-</span><span class="mf">0.36094661</span> <span class="o">-</span><span class="mf">1.30447338</span><span class="p">]]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="o">&gt;&gt;</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>As you’ll soon see, there are strict requirements on the dimensions of these weight matrices. The number of <em>rows</em> must equal the number of neurons in the previous layer. The number of <em>columns</em> must match the number of neurons in the next layer.</p>
<p>A good explanation of random weight initalization can be found in the Stanford CS231 course notes <a class="footnote-reference" href="#footnote-1" id="footnote-reference-1">[1]</a> chapter on neural networks.</p>
</div>
<div class="section" id="bias-terms">
<h4><a class="toc-backref" href="#toc-entry-7">Bias Terms</a><a class="headerlink" href="#bias-terms" title="Permalink to this headline">¶</a></h4>
<p><a class="reference internal" href="index.html#nn-bias"><span class="std std-ref">Bias</span></a> terms allow us to shift our neuron’s activation outputs left and right. This helps us model datasets that do not necessarily pass through the origin.</p>
<p>Using the numpy method <code class="docutils literal notranslate"><span class="pre">np.full()</span></code> below, we create two 1-dimensional bias arrays filled with the default value <code class="docutils literal notranslate"><span class="pre">0.2</span></code>. The first argument to <code class="docutils literal notranslate"><span class="pre">np.full</span></code> is a tuple of array dimensions. The second is the default value for cells in the array.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">init_bias</span><span class="p">():</span>
    <span class="n">Bh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">HIDDEN_LAYER_SIZE</span><span class="p">),</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">Bo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">OUTPUT_LAYER_SIZE</span><span class="p">),</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Bh</span><span class="p">,</span> <span class="n">Bo</span>
</pre></div>
</div>
</div>
<div class="section" id="working-with-matrices">
<h4><a class="toc-backref" href="#toc-entry-8">Working with Matrices</a><a class="headerlink" href="#working-with-matrices" title="Permalink to this headline">¶</a></h4>
<p>To take advantage of fast linear algebra techniques and GPUs, we need to store our inputs, weights, and biases in matrices. Here is our neural network diagram again with its underlying matrix representation.</p>
<img alt="_images/nn_with_matrices_displayed.png" class="align-center" src="_images/nn_with_matrices_displayed.png" />
<p>What’s happening here? To better understand, let’s walk through each of the matrices in the diagram with an emphasis on their dimensions and why the dimensions are what they are. The matrix dimensions above flow naturally from the architecture of our network and the number of samples in our training set.</p>
<p class="rubric">Matrix dimensions</p>
<table border="1" class="docutils">
<colgroup>
<col width="2%" />
<col width="5%" />
<col width="4%" />
<col width="89%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Var</strong></td>
<td><strong>Name</strong></td>
<td><strong>Dimensions</strong></td>
<td><strong>Explanation</strong></td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">X</span></code></td>
<td>Input</td>
<td>(3, 1)</td>
<td>Includes 3 rows of training data, and each row has 1 attribute (height, price, etc.)</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">Wh</span></code></td>
<td>Hidden weights</td>
<td>(1, 2)</td>
<td>These dimensions are based on number of rows equals the number of attributes for the observations in our training set. The number columns equals the number of neurons in the hidden layer. The dimensions of the weights matrix between two layers is determined by the sizes of the two layers it connects. There is one weight for every input-to-neuron connection between the layers.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">Bh</span></code></td>
<td>Hidden bias</td>
<td>(1, 2)</td>
<td>Each neuron in the hidden layer has is own bias constant. This bias matrix is added to the weighted input matrix before the hidden layer applies ReLU.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">Zh</span></code></td>
<td>Hidden weighted input</td>
<td>(1, 2)</td>
<td>Computed by taking the dot product of X and Wh. The dimensions (1,2) are required by the rules of matrix multiplication. Zh takes the rows of in the inputs matrix and the columns of weights matrix. We then add the hidden layer bias matrix Bh.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">H</span></code></td>
<td>Hidden activations</td>
<td>(3, 2)</td>
<td>Computed by applying the Relu function to Zh. The dimensions are (3,2) — the number of rows matches the number of training samples and the number of columns equals the number of neurons. Each column holds all the activations for a specific neuron.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">Wo</span></code></td>
<td>Output weights</td>
<td>(2, 2)</td>
<td>The number of rows matches the number of hidden layer neurons and the number of columns equals the number of output layer neurons. There is one weight for every hidden-neuron-to-output-neuron connection between the layers.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">Bo</span></code></td>
<td>Output bias</td>
<td>(1, 2)</td>
<td>There is one column for every neuron in the output layer.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">Zo</span></code></td>
<td>Output weighted input</td>
<td>(3, 2)</td>
<td>Computed by taking the dot product of H and Wo and then adding the output layer bias Bo. The dimensions are (3,2) representing the rows of in the hidden layer matrix and the columns of output layer weights matrix.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">O</span></code></td>
<td>Output activations</td>
<td>(3, 2)</td>
<td>Each row represents a prediction for a single observation in our training set. Each column is a unique attribute we want to predict. Examples of two-column output predictions could be a company’s sales and units sold, or a person’s height and weight.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="dynamic-resizing">
<h4><a class="toc-backref" href="#toc-entry-9">Dynamic Resizing</a><a class="headerlink" href="#dynamic-resizing" title="Permalink to this headline">¶</a></h4>
<p>Before we continue I want to point out how the matrix dimensions change with changes to the network architecture or size of the training set. For example, let’s build a network with 2 input neurons, 3 hidden neurons, 2 output neurons, and 4 observations in our training set.</p>
<img alt="_images/dynamic_resizing_neural_network_4_obs.png" class="align-center" src="_images/dynamic_resizing_neural_network_4_obs.png" />
<p>Now let’s use same number of layers and neurons but reduce the number of observations in our dataset to <strong>1 instance</strong>:</p>
<img alt="_images/dynamic_resizing_neural_network_1_obs.png" class="align-center" src="_images/dynamic_resizing_neural_network_1_obs.png" />
<p>As you can see, the number of columns in all matrices remains the same. The only thing that changes is the number of rows the layer matrices, which fluctuate with the size of the training set. The dimensions of the weight matrices remain unchanged. This shows us we can use the same network, the same lines of code, to process any number of observations.</p>
</div>
<div class="section" id="refactoring-our-code">
<h4><a class="toc-backref" href="#toc-entry-10">Refactoring Our Code</a><a class="headerlink" href="#refactoring-our-code" title="Permalink to this headline">¶</a></h4>
<p>Here is our new feed forward code which accepts matrices instead of scalar inputs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">feed_forward</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    X    - input matrix</span>
<span class="sd">    Zh   - hidden layer weighted input</span>
<span class="sd">    Zo   - output layer weighted input</span>
<span class="sd">    H    - hidden layer activation</span>
<span class="sd">    y    - output layer</span>
<span class="sd">    yHat - output layer predictions</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="c1"># Hidden layer</span>
    <span class="n">Zh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Wh</span><span class="p">)</span> <span class="o">+</span> <span class="n">Bh</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Zh</span><span class="p">)</span>

    <span class="c1"># Output layer</span>
    <span class="n">Zo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">Wo</span><span class="p">)</span> <span class="o">+</span> <span class="n">Bo</span>
    <span class="n">yHat</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Zo</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">yHat</span>
</pre></div>
</div>
<p class="rubric">Weighted input</p>
<p>The first change is to update our weighted input calculation to handle matrices. Using dot product, we multiply the input matrix by the weights connecting them to the neurons in the next layer. Next we add the bias vector using matrix addition.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Zh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Wh</span><span class="p">)</span> <span class="o">+</span> <span class="n">Bh</span>
</pre></div>
</div>
<img alt="_images/neural_network_matrix_weighted_input.png" class="align-center" src="_images/neural_network_matrix_weighted_input.png" />
<p>The first column in <code class="docutils literal notranslate"><span class="pre">Bh</span></code> is added to all the rows in the first column of resulting dot product of <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">Wh</span></code>. The second value in <code class="docutils literal notranslate"><span class="pre">Bh</span></code> is added to all the elements in the second column. The result is a new matrix, <code class="docutils literal notranslate"><span class="pre">Zh</span></code> which has a column for every neuron in the hidden layer and a row for every observation in our dataset. Given all the layers in our network are <em>fully-connected</em>, there is one weight for every neuron-to-neuron connection between the layers.</p>
<p>The same process is repeated for the output layer, except the input is now the hidden layer activation <code class="docutils literal notranslate"><span class="pre">H</span></code> and the weights <code class="docutils literal notranslate"><span class="pre">Wo</span></code>.</p>
<p class="rubric">ReLU activation</p>
<p>The second change is to refactor ReLU to use elementwise multiplication on matrices. It’s only a small change, but its necessary if we want to work with matrices. <code class="docutils literal notranslate"><span class="pre">np.maximum()</span></code> is actually extensible and can handle both scalar and array inputs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">Z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>
</pre></div>
</div>
<p>In the hidden layer activation step, we apply the ReLU activation function <code class="docutils literal notranslate"><span class="pre">np.maximum(0,Z)</span></code> to every cell in the new matrix. The result is a matrix where all negative values have been replaced by 0. The same process is repeated for the output layer, except the input is <code class="docutils literal notranslate"><span class="pre">Zo</span></code>.</p>
</div>
<div class="section" id="final-result">
<h4><a class="toc-backref" href="#toc-entry-11">Final Result</a><a class="headerlink" href="#final-result" title="Permalink to this headline">¶</a></h4>
<p>Putting it all together we have the following code for forward propagation with matrices.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">INPUT_LAYER_SIZE</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">HIDDEN_LAYER_SIZE</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">OUTPUT_LAYER_SIZE</span> <span class="o">=</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">init_weights</span><span class="p">():</span>
    <span class="n">Wh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">INPUT_LAYER_SIZE</span><span class="p">,</span> <span class="n">HIDDEN_LAYER_SIZE</span><span class="p">)</span> <span class="o">*</span> \
                <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="o">/</span><span class="n">INPUT_LAYER_SIZE</span><span class="p">)</span>
    <span class="n">Wo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">HIDDEN_LAYER_SIZE</span><span class="p">,</span> <span class="n">OUTPUT_LAYER_SIZE</span><span class="p">)</span> <span class="o">*</span> \
                <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="o">/</span><span class="n">HIDDEN_LAYER_SIZE</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">init_bias</span><span class="p">():</span>
    <span class="n">Bh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">HIDDEN_LAYER_SIZE</span><span class="p">),</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">Bo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">OUTPUT_LAYER_SIZE</span><span class="p">),</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Bh</span><span class="p">,</span> <span class="n">Bo</span>

<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">Z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">relu_prime</span><span class="p">(</span><span class="n">Z</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Z - weighted input matrix</span>

<span class="sd">    Returns gradient of Z where all</span>
<span class="sd">    negative values are set to 0 and</span>
<span class="sd">    all positive values set to 1</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">Z</span><span class="p">[</span><span class="n">Z</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">Z</span><span class="p">[</span><span class="n">Z</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">Z</span>

<span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">yHat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">yHat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span>
    <span class="k">return</span> <span class="n">cost</span>

<span class="k">def</span> <span class="nf">cost_prime</span><span class="p">(</span><span class="n">yHat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">yHat</span> <span class="o">-</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">feed_forward</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    X    - input matrix</span>
<span class="sd">    Zh   - hidden layer weighted input</span>
<span class="sd">    Zo   - output layer weighted input</span>
<span class="sd">    H    - hidden layer activation</span>
<span class="sd">    y    - output layer</span>
<span class="sd">    yHat - output layer predictions</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="c1"># Hidden layer</span>
    <span class="n">Zh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Wh</span><span class="p">)</span> <span class="o">+</span> <span class="n">Bh</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Zh</span><span class="p">)</span>

    <span class="c1"># Output layer</span>
    <span class="n">Zo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">Wo</span><span class="p">)</span> <span class="o">+</span> <span class="n">Bo</span>
    <span class="n">yHat</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Zo</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-1">[1]</a></td><td><a class="reference external" href="http://cs231n.github.io/neural-networks-2/#init">http://cs231n.github.io/neural-networks-2/#init</a></td></tr>
</tbody>
</table>
</div>
</div>
</div>
<span id="document-backpropagation"></span><div class="section" id="backpropagation-1">
<span id="backpropagation"></span><h2>Backpropagation<a class="headerlink" href="#backpropagation-1" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#chain-rule-refresher" id="toc-entry-1">Chain rule refresher</a></li>
<li><a class="reference internal" href="#applying-the-chain-rule" id="toc-entry-2">Applying the chain rule</a></li>
<li><a class="reference internal" href="#saving-work-with-memoization" id="toc-entry-3">Saving work with memoization</a></li>
<li><a class="reference internal" href="#code-example" id="toc-entry-4">Code example</a></li>
</ul>
</div>
<p>The goals of backpropagation are straightforward: adjust each weight in the network in proportion to how much it contributes to overall error. If we iteratively reduce each weight’s error, eventually we’ll have a series of weights that produce good predictions.</p>
<div class="section" id="chain-rule-refresher">
<h3><a class="toc-backref" href="#toc-entry-1">Chain rule refresher</a><a class="headerlink" href="#chain-rule-refresher" title="Permalink to this headline">¶</a></h3>
<p>As seen above, foward propagation can be viewed as a long series of nested equations. If you think of feed forward this way, then backpropagation is merely an application of <a class="reference internal" href="index.html#chain-rule"><span class="std std-ref">Chain rule</span></a> to find the <a class="reference internal" href="index.html#derivative"><span class="std std-ref">Derivatives</span></a> of cost with respect to any variable in the nested equation. Given a forward propagation function:</p>
<div class="math notranslate nohighlight">
\[f(x) = A(B(C(x)))\]</div>
<p>A, B, and C are activation functions at different layers. Using the chain rule we easily calculate the derivative of <span class="math notranslate nohighlight">\(f(x)\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[f'(x) = f'(A) \cdot A'(B) \cdot B'(C) \cdot C'(x)\]</div>
<p>How about the derivative with respect to B? To find the derivative with respect to B you can pretend <span class="math notranslate nohighlight">\(B(C(x))\)</span> is a constant, replace it with a placeholder variable B, and proceed to find the derivative normally with respect to B.</p>
<div class="math notranslate nohighlight">
\[f'(B) = f'(A) \cdot A'(B)\]</div>
<p>This simple technique extends to any variable within a function and allows us to precisely pinpoint the exact impact each variable has on the total output.</p>
</div>
<div class="section" id="applying-the-chain-rule">
<h3><a class="toc-backref" href="#toc-entry-2">Applying the chain rule</a><a class="headerlink" href="#applying-the-chain-rule" title="Permalink to this headline">¶</a></h3>
<p>Let’s use the chain rule to calculate the derivative of cost with respect to any weight in the network. The chain rule will help us identify how much each weight contributes to our overall error and the direction to update each weight to reduce our error. Here are the equations we need to make a prediction and calculate total error, or cost:</p>
<img alt="_images/backprop_ff_equations.png" class="align-center" src="_images/backprop_ff_equations.png" />
<p>Given a network consisting of a single neuron, total cost could be calculated as:</p>
<div class="math notranslate nohighlight">
\[Cost = C(R(Z(X W)))\]</div>
<p>Using the chain rule we can easily find the derivative of Cost with respect to weight W.</p>
<div class="math notranslate nohighlight">
\[\begin{split}C'(W) &amp;= C'(R) \cdot R'(Z) \cdot Z'(W) \\
      &amp;= (\hat{y} -y) \cdot R'(Z) \cdot X\end{split}\]</div>
<p>Now that we have an equation to calculate the derivative of cost with respect to any weight, let’s go back to our toy neural network example above</p>
<img alt="_images/simple_nn_diagram_zo_zh_defined.png" class="align-center" src="_images/simple_nn_diagram_zo_zh_defined.png" />
<p>What is the derivative of cost with respect to <span class="math notranslate nohighlight">\(W_o\)</span>?</p>
<div class="math notranslate nohighlight">
\[\begin{split}C'(W_O) &amp;= C'(\hat{y}) \cdot \hat{y}'(Z_O) \cdot Z_O'(W_O) \\
        &amp;= (\hat{y} - y) \cdot R'(Z_O) \cdot H\end{split}\]</div>
<p>And how about with respect to <span class="math notranslate nohighlight">\(W_h\)</span>? To find out we just keep going further back in our function applying the chain rule recursively until we get to the function that has the Wh term.</p>
<div class="math notranslate nohighlight">
\[\begin{split}C'(W_h) &amp;= C'(\hat{y}) \cdot O'(Z_o) \cdot Z_o'(H) \cdot H'(Z_h) \cdot Z_h'(W_h) \\
        &amp;= (\hat{y} - y) \cdot R'(Z_o) \cdot W_o \cdot R'(Z_h) \cdot X\end{split}\]</div>
<p>And just for fun, what if our network had 10 hidden layers. What is the derivative of cost for the first weight <span class="math notranslate nohighlight">\(w_1\)</span>?</p>
<div class="math notranslate nohighlight">
\[\begin{split}C'(w_1) = \frac{dC}{d\hat{y}} \cdot \frac{d\hat{y}}{dZ_{11}} \cdot \frac{dZ_{11}}{dH_{10}} \cdot \\ \frac{dH_{10}}{dZ_{10}} \cdot \frac{dZ_{10}}{dH_9} \cdot \frac{dH_9}{dZ_9} \cdot \frac{dZ_9}{dH_8} \cdot \frac{dH_8}{dZ_8} \cdot \frac{dZ_8}{dH_7} \cdot \frac{dH_7}{dZ_7} \cdot \\ \frac{dZ_7}{dH_6} \cdot \frac{dH_6}{dZ_6} \cdot \frac{dZ_6}{dH_5} \cdot \frac{dH_5}{dZ_5} \cdot \frac{dZ_5}{dH_4} \cdot \frac{dH_4}{dZ_4} \cdot \frac{dZ_4}{dH_3} \cdot \\ \frac{dH_3}{dZ_3} \cdot \frac{dZ_3}{dH_2} \cdot \frac{dH_2}{dZ_2} \cdot \frac{dZ_2}{dH_1} \cdot \frac{dH_1}{dZ_1} \cdot \frac{dZ_1}{dW_1}\end{split}\]</div>
<p>See the pattern? The number of calculations required to compute cost derivatives increases as our network grows deeper. Notice also the redundancy in our derivative calculations. Each layer’s cost derivative appends two new terms to the terms that have already been calculated by the layers above it. What if there was a way to save our work somehow and avoid these duplicate calculations?</p>
</div>
<div class="section" id="saving-work-with-memoization">
<h3><a class="toc-backref" href="#toc-entry-3">Saving work with memoization</a><a class="headerlink" href="#saving-work-with-memoization" title="Permalink to this headline">¶</a></h3>
<p>Memoization is a computer science term which simply means: don’t recompute the same thing over and over. In memoization we store previously computed results to avoid recalculating the same function. It’s handy for speeding up recursive functions of which backpropagation is one. Notice the pattern in the derivative equations below.</p>
<img alt="_images/memoization.png" class="align-center" src="_images/memoization.png" />
<p>Each of these layers is recomputing the same derivatives! Instead of writing out long derivative equations for every weight, we can use memoization to save our work as we backprop error through the network. To do this, we define 3 equations (below), which together encapsulate all the calculations needed for backpropagation. The math is the same, but the equations provide a nice shorthand we can use to track which calculations we’ve already performed and save our work as we move backwards through the network.</p>
<img alt="_images/backprop_3_equations.png" class="align-center" src="_images/backprop_3_equations.png" />
<p>We first calculate the output layer error and pass the result to the hidden layer before it. After calculating the hidden layer error, we pass its error value back to the previous hidden layer before it. And so on and so forth. As we move back through the network we apply the 3rd formula at every layer to calculate the derivative of cost with respect that layer’s weights. This resulting derivative tells us in which direction to adjust our weights to reduce overall cost.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The term <em>layer error</em> refers to the derivative of cost with respect to a layer’s <em>input</em>. It answers the question: how does the cost function output change when the input to that layer changes?</p>
</div>
<p class="rubric">Output layer&nbsp;error</p>
<p>To calculate output layer error we need to find the derivative of cost with respect to the output layer input, <span class="math notranslate nohighlight">\(Z_o\)</span>. It answers the question — how are the final layer’s weights impacting overall error in the network? The derivative is then:</p>
<div class="math notranslate nohighlight">
\[C'(Z_o) = (\hat{y} - y) \cdot R'(Z_o)\]</div>
<p>To simplify notation, ml practitioners typically replace the <span class="math notranslate nohighlight">\((\hat{y}-y) * R'(Zo)\)</span> sequence with the term <span class="math notranslate nohighlight">\(E_o\)</span>. So our formula for output layer error equals:</p>
<div class="math notranslate nohighlight">
\[E_o = (\hat{y} - y) \cdot R'(Z_o)\]</div>
<p class="rubric">Hidden layer&nbsp;error</p>
<p>To calculate hidden layer error we need to find the derivative of cost with respect to the hidden layer input, Zh.</p>
<div class="math notranslate nohighlight">
\[C'(Z_h) = (\hat{y} - y) \cdot R'(Z_o) \cdot W_o \cdot R'(Z_h)\]</div>
<p>Next we can swap in the <span class="math notranslate nohighlight">\(E_o\)</span> term above to avoid duplication and create a new simplified equation for Hidden layer error:</p>
<div class="math notranslate nohighlight">
\[E_h = E_o \cdot W_o \cdot R'(Z_h)\]</div>
<p>This formula is at the core of backpropagation. We calculate the current layer’s error, and pass the weighted error back to the previous layer, continuing the process until we arrive at our first hidden layer. Along the way we update the weights using the derivative of cost with respect to each weight.</p>
<p class="rubric">Derivative of cost with respect to&nbsp;any weight</p>
<p>Let’s return to our formula for the derivative of cost with respect to the output layer weight <span class="math notranslate nohighlight">\(W_o\)</span>.</p>
<div class="math notranslate nohighlight">
\[C'(W_O) = (\hat{y} - y) \cdot R'(Z_O) \cdot H\]</div>
<p>We know we can replace the first part with our equation for output layer error <span class="math notranslate nohighlight">\(E_o\)</span>. H represents the hidden layer activation.</p>
<div class="math notranslate nohighlight">
\[C'(W_o) = E_o \cdot H\]</div>
<p>So to find the derivative of cost with respect to any weight in our network, we simply multiply the corresponding layer’s error times its input (the previous layer’s output).</p>
<div class="math notranslate nohighlight">
\[C'(w) = CurrentLayerError \cdot CurrentLayerInput\]</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><em>Input</em> refers to the activation from the previous&nbsp;layer, not the weighted input, Z.</p>
</div>
<p class="rubric">Summary</p>
<p>Here are the final 3 equations that together form the foundation of backpropagation.</p>
<img alt="_images/backprop_final_3_deriv_equations.png" class="align-center" src="_images/backprop_final_3_deriv_equations.png" />
<p>Here is the process visualized using our toy neural network example above.</p>
<img alt="_images/backprop_visually.png" class="align-center" src="_images/backprop_visually.png" />
</div>
<div class="section" id="code-example">
<h3><a class="toc-backref" href="#toc-entry-4">Code example</a><a class="headerlink" href="#code-example" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">relu_prime</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">z</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">yHat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">yHat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">cost_prime</span><span class="p">(</span><span class="n">yHat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">yHat</span> <span class="o">-</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">Wo</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="n">yHat</span> <span class="o">=</span> <span class="n">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">Wo</span><span class="p">)</span>

    <span class="c1"># Layer Error</span>
    <span class="n">Eo</span> <span class="o">=</span> <span class="p">(</span><span class="n">yHat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">relu_prime</span><span class="p">(</span><span class="n">Zo</span><span class="p">)</span>
    <span class="n">Eh</span> <span class="o">=</span> <span class="n">Eo</span> <span class="o">*</span> <span class="n">Wo</span> <span class="o">*</span> <span class="n">relu_prime</span><span class="p">(</span><span class="n">Zh</span><span class="p">)</span>

    <span class="c1"># Cost derivative for weights</span>
    <span class="n">dWo</span> <span class="o">=</span> <span class="n">Eo</span> <span class="o">*</span> <span class="n">H</span>
    <span class="n">dWh</span> <span class="o">=</span> <span class="n">Eh</span> <span class="o">*</span> <span class="n">x</span>

    <span class="c1"># Update weights</span>
    <span class="n">Wh</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">dWh</span>
    <span class="n">Wo</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">dWo</span>
</pre></div>
</div>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Example</td></tr>
</tbody>
</table>
</div>
</div>
<span id="document-activation_functions"></span><div class="section" id="activation-functions-1">
<span id="activation-functions"></span><h2>Activation Functions<a class="headerlink" href="#activation-functions-1" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#linear" id="toc-entry-1">Linear</a></li>
<li><a class="reference internal" href="#elu" id="toc-entry-2">ELU</a></li>
<li><a class="reference internal" href="#relu" id="toc-entry-3">ReLU</a></li>
<li><a class="reference internal" href="#leakyrelu" id="toc-entry-4">LeakyReLU</a></li>
<li><a class="reference internal" href="#sigmoid" id="toc-entry-5">Sigmoid</a></li>
<li><a class="reference internal" href="#tanh" id="toc-entry-6">Tanh</a></li>
<li><a class="reference internal" href="#softmax" id="toc-entry-7">Softmax</a></li>
</ul>
</div>
<div class="section" id="linear">
<span id="activation-linear"></span><h3><a class="toc-backref" href="#toc-entry-1">Linear</a><a class="headerlink" href="#linear" title="Permalink to this headline">¶</a></h3>
<p>A straight line function where activation is proportional to input ( which is the weighted sum from neuron ).</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Function</td>
<td>Derivative</td>
</tr>
<tr class="row-even"><td><div class="first last math notranslate nohighlight">
\[\begin{split}R(z,m) = \begin{Bmatrix} z*m    \\
           \end{Bmatrix}\end{split}\]</div>
</td>
<td><div class="first last math notranslate nohighlight">
\[\begin{split}R'(z,m) = \begin{Bmatrix} m     \\
            \end{Bmatrix}\end{split}\]</div>
</td>
</tr>
<tr class="row-odd"><td><a class="first last reference internal image-reference" href="_images/linear.png"><img alt="_images/linear.png" class="align-center" src="_images/linear.png" style="width: 256px; height: 256px;" /></a>
</td>
<td><a class="first last reference internal image-reference" href="_images/linear_prime.png"><img alt="_images/linear_prime.png" class="align-center" src="_images/linear_prime.png" style="width: 256px; height: 256px;" /></a>
</td>
</tr>
<tr class="row-even"><td><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">m</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">m</span><span class="o">*</span><span class="n">z</span>
</pre></div>
</div>
</td>
<td><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linear_prime</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">m</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">m</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Pros</p>
<ul class="simple">
<li>It gives a range of activations, so it is not binary activation.</li>
<li>We can definitely connect a few neurons together and if more than 1 fires, we could take the max ( or softmax) and decide based on that.</li>
</ul>
<p class="rubric">Cons</p>
<ul class="simple">
<li>For this function, derivative is a constant. That means, the gradient has no relationship with X.</li>
<li>It is a constant gradient and the descent is going to be on constant gradient.</li>
<li>If there is an error in prediction, the changes made by back propagation is constant and not depending on the change in input delta(x) !</li>
</ul>
</div>
<div class="section" id="elu">
<span id="activation-elu"></span><h3><a class="toc-backref" href="#toc-entry-2">ELU</a><a class="headerlink" href="#elu" title="Permalink to this headline">¶</a></h3>
<p>Exponential Linear Unit or its widely known name ELU is a function that tend to converge cost to zero faster and produce more accurate results. Different to other activation functions, ELU has a extra alpha constant which should be positive number.</p>
<p>ELU is very similiar to RELU except negative inputs. They are both in identity function form for non-negative inputs. On the other hand, ELU becomes smooth slowly until its output equal to -α whereas RELU sharply smoothes.</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Function</td>
<td>Derivative</td>
</tr>
<tr class="row-even"><td><div class="first last math notranslate nohighlight">
\[\begin{split}R(z) = \begin{Bmatrix} z &amp; z &gt; 0 \\
 α.( e^z – 1) &amp; z &lt;= 0 \end{Bmatrix}\end{split}\]</div>
</td>
<td><div class="first last math notranslate nohighlight">
\[\begin{split}R'(z) = \begin{Bmatrix} 1 &amp; z&gt;0 \\
α.e^z &amp; z&lt;0 \end{Bmatrix}\end{split}\]</div>
</td>
</tr>
<tr class="row-odd"><td><a class="first last reference internal image-reference" href="_images/elu.png"><img alt="_images/elu.png" class="align-center" src="_images/elu.png" style="width: 256px; height: 256px;" /></a>
</td>
<td><a class="first last reference internal image-reference" href="_images/elu_prime.png"><img alt="_images/elu_prime.png" class="align-center" src="_images/elu_prime.png" style="width: 256px; height: 256px;" /></a>
</td>
</tr>
<tr class="row-even"><td><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">alpha</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">z</span> <span class="k">if</span> <span class="n">z</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">e</span><span class="o">^</span><span class="n">z</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">elu_prime</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">alpha</span><span class="p">):</span>
	<span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">z</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Pros</p>
<ul class="simple">
<li>ELU becomes smooth slowly until its output equal to -α whereas RELU sharply smoothes.</li>
<li>ELU is a strong alternative to ReLU.</li>
<li>Unlike to ReLU, ELU can produce negative outputs.</li>
</ul>
<p class="rubric">Cons</p>
<ul class="simple">
<li>For x &gt; 0, it can blow up the activation with the output range of [0, inf].</li>
</ul>
</div>
<div class="section" id="relu">
<span id="activation-relu"></span><h3><a class="toc-backref" href="#toc-entry-3">ReLU</a><a class="headerlink" href="#relu" title="Permalink to this headline">¶</a></h3>
<p>A recent invention which stands for Rectified Linear Units. The formula is deceptively simple: <span class="math notranslate nohighlight">\(max(0,z)\)</span>. Despite its name and appearance, it’s not linear and provides the same benefits as Sigmoid (i.e. the ability to learn nonlinear functions), but with better performance.</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Function</td>
<td>Derivative</td>
</tr>
<tr class="row-even"><td><div class="first last math notranslate nohighlight">
\[\begin{split}R(z) = \begin{Bmatrix} z &amp; z &gt; 0 \\
 0 &amp; z &lt;= 0 \end{Bmatrix}\end{split}\]</div>
</td>
<td><div class="first last math notranslate nohighlight">
\[\begin{split}R'(z) = \begin{Bmatrix} 1 &amp; z&gt;0 \\
0 &amp; z&lt;0 \end{Bmatrix}\end{split}\]</div>
</td>
</tr>
<tr class="row-odd"><td><a class="first last reference internal image-reference" href="_images/relu.png"><img alt="_images/relu.png" class="align-center" src="_images/relu.png" style="width: 256px; height: 256px;" /></a>
</td>
<td><a class="first last reference internal image-reference" href="_images/relu_prime.png"><img alt="_images/relu_prime.png" class="align-center" src="_images/relu_prime.png" style="width: 256px; height: 256px;" /></a>
</td>
</tr>
<tr class="row-even"><td><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">relu_prime</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">z</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Pros</p>
<ul class="simple">
<li>It avoids and rectifies vanishing gradient problem.</li>
<li>ReLu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations.</li>
</ul>
<p class="rubric">Cons</p>
<ul class="simple">
<li>One of its limitations is that it should only be used within hidden layers of a neural network model.</li>
<li>Some gradients can be fragile during training and can die. It can cause a weight update which will makes it never activate on any data point again. In other words, ReLu can result in dead neurons.</li>
<li>In another words, For activations in the region (x&lt;0) of ReLu, gradient will be 0 because of which the weights will not get adjusted during descent. That means, those neurons which go into that state will stop responding to variations in error/ input (simply because gradient is 0, nothing changes). This is called the dying ReLu problem.</li>
<li>The range of ReLu is <span class="math notranslate nohighlight">\([0, \infty)\)</span>. This means it can blow up the activation.</li>
</ul>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><a class="reference external" href="http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf">Deep Sparse Rectifier Neural Networks</a> Glorot et al., (2011)</li>
<li><a class="reference external" href="https://medium.com/&#64;karpathy/yes-you-should-understand-backprop-e2f06eab496b">Yes You Should Understand Backprop</a>, Karpathy (2016)</li>
</ul>
</div>
<div class="section" id="leakyrelu">
<span id="activation-leakyrelu"></span><h3><a class="toc-backref" href="#toc-entry-4">LeakyReLU</a><a class="headerlink" href="#leakyrelu" title="Permalink to this headline">¶</a></h3>
<p>LeakyRelu is a variant of ReLU. Instead of being 0 when <span class="math notranslate nohighlight">\(z &lt; 0\)</span>, a leaky ReLU allows a small, non-zero, constant gradient <span class="math notranslate nohighlight">\(\alpha\)</span> (Normally, <span class="math notranslate nohighlight">\(\alpha = 0.01\)</span>). However, the consistency of the benefit across tasks is presently unclear. <a class="footnote-reference" href="#footnote-1" id="footnote-reference-1">[1]</a></p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Function</td>
<td>Derivative</td>
</tr>
<tr class="row-even"><td><div class="first last math notranslate nohighlight">
\[\begin{split}R(z) = \begin{Bmatrix} z &amp; z &gt; 0 \\
 \alpha z &amp; z &lt;= 0 \end{Bmatrix}\end{split}\]</div>
</td>
<td><div class="first last math notranslate nohighlight">
\[\begin{split}R'(z) = \begin{Bmatrix} 1 &amp; z&gt;0 \\
\alpha &amp; z&lt;0 \end{Bmatrix}\end{split}\]</div>
</td>
</tr>
<tr class="row-odd"><td><a class="first last reference internal image-reference" href="_images/leakyrelu.png"><img alt="_images/leakyrelu.png" class="align-center" src="_images/leakyrelu.png" style="width: 256px; height: 256px;" /></a>
</td>
<td><a class="first last reference internal image-reference" href="_images/leakyrelu_prime.png"><img alt="_images/leakyrelu_prime.png" class="align-center" src="_images/leakyrelu_prime.png" style="width: 256px; height: 256px;" /></a>
</td>
</tr>
<tr class="row-even"><td><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">leakyrelu</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
	<span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">z</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">leakyrelu_prime</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
	<span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">z</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">alpha</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Pros</p>
<ul class="simple">
<li>Leaky ReLUs are one attempt to fix the “dying ReLU” problem by having a small negative slope (of 0.01, or so).</li>
</ul>
<p class="rubric">Cons</p>
<ul class="simple">
<li>As it possess linearity, it can’t be used for the complex Classification. It lags behind the Sigmoid and Tanh for some of the use cases.</li>
</ul>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/pdf/1502.01852.pdf">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>, Kaiming He et al. (2015)</li>
</ul>
</div>
<div class="section" id="sigmoid">
<span id="activation-sigmoid"></span><h3><a class="toc-backref" href="#toc-entry-5">Sigmoid</a><a class="headerlink" href="#sigmoid" title="Permalink to this headline">¶</a></h3>
<p>Sigmoid takes a real value as input and outputs another value between 0 and 1. It’s easy to work with and has all the nice properties of activation functions: it’s non-linear, continuously differentiable, monotonic, and has a fixed output range.</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Function</td>
<td>Derivative</td>
</tr>
<tr class="row-even"><td><div class="first last math notranslate nohighlight">
\[S(z) = \frac{1} {1 + e^{-z}}\]</div>
</td>
<td><div class="first last math notranslate nohighlight">
\[S'(z) = S(z) \cdot (1 - S(z))\]</div>
</td>
</tr>
<tr class="row-odd"><td><a class="first last reference internal image-reference" href="_images/sigmoid.png"><img alt="_images/sigmoid.png" class="align-center" src="_images/sigmoid.png" style="width: 256px;" /></a>
</td>
<td><a class="first last reference internal image-reference" href="_images/sigmoid_prime.png"><img alt="_images/sigmoid_prime.png" class="align-center" src="_images/sigmoid_prime.png" style="width: 256px;" /></a>
</td>
</tr>
<tr class="row-even"><td><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
</pre></div>
</div>
</td>
<td><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid_prime</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Pros</p>
<ul class="simple">
<li>It is nonlinear in nature. Combinations of this function are also nonlinear!</li>
<li>It will give an analog activation unlike step function.</li>
<li>It has a smooth gradient too.</li>
<li>It’s good for a classifier.</li>
<li>The output of the activation function is always going to be in range (0,1) compared to (-inf, inf) of linear function. So we have our activations bound in a range. Nice, it won’t blow up the activations then.</li>
</ul>
<p class="rubric">Cons</p>
<ul class="simple">
<li>Towards either end of the sigmoid function, the Y values tend to respond very less to changes in X.</li>
<li>It gives rise to a problem of “vanishing gradients”.</li>
<li>Its output isn’t zero centered. It makes the gradient updates go too far in different directions. 0 &lt; output &lt; 1, and it makes optimization harder.</li>
<li>Sigmoids saturate and kill gradients.</li>
<li>The network refuses to learn further or is drastically slow ( depending on use case and until gradient /computation gets hit by floating point value limits ).</li>
</ul>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><a class="reference external" href="https://medium.com/&#64;karpathy/yes-you-should-understand-backprop-e2f06eab496b">Yes You Should Understand Backprop</a>, Karpathy (2016)</li>
</ul>
</div>
<div class="section" id="tanh">
<span id="activation-tanh"></span><h3><a class="toc-backref" href="#toc-entry-6">Tanh</a><a class="headerlink" href="#tanh" title="Permalink to this headline">¶</a></h3>
<p>Tanh squashes a real-valued number to the range [-1, 1]. It’s non-linear. But unlike Sigmoid, its output is zero-centered.
Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity. <a class="footnote-reference" href="#footnote-1" id="footnote-reference-2">[1]</a></p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Function</td>
<td>Derivative</td>
</tr>
<tr class="row-even"><td><div class="first last math notranslate nohighlight">
\[tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\]</div>
</td>
<td><div class="first last math notranslate nohighlight">
\[tanh'(z) = 1 - tanh(z)^{2}\]</div>
</td>
</tr>
<tr class="row-odd"><td><a class="first last reference internal image-reference" href="_images/tanh.png"><img alt="_images/tanh.png" class="align-center" src="_images/tanh.png" style="width: 256px;" /></a>
</td>
<td><a class="first last reference internal image-reference" href="_images/tanh_prime.png"><img alt="_images/tanh_prime.png" class="align-center" src="_images/tanh_prime.png" style="width: 256px;" /></a>
</td>
</tr>
<tr class="row-even"><td><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
	<span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
</pre></div>
</div>
</td>
<td><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tanh_prime</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
	<span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">tanh</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Pros</p>
<ul class="simple">
<li>The gradient is stronger for tanh than sigmoid ( derivatives are steeper).</li>
</ul>
<p class="rubric">Cons</p>
<ul class="simple">
<li>Tanh also has the vanishing gradient problem.</li>
</ul>
</div>
<div class="section" id="softmax">
<h3><a class="toc-backref" href="#toc-entry-7">Softmax</a><a class="headerlink" href="#softmax" title="Permalink to this headline">¶</a></h3>
<p>Softmax function calculates the probabilities distribution of the event over ‘n’ different events. In general way of saying, this function will calculate the probabilities of each target class over all possible target classes. Later the calculated probabilities will be helpful for determining the target class for the given inputs.</p>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><em>(<a class="fn-backref" href="#footnote-reference-1">1</a>, <a class="fn-backref" href="#footnote-reference-2">2</a>)</em> <a class="reference external" href="http://cs231n.github.io/neural-networks-1/">http://cs231n.github.io/neural-networks-1/</a></td></tr>
</tbody>
</table>
</div>
</div>
<span id="document-layers"></span><div class="section" id="layers-1">
<span id="layers"></span><h2>Layers<a class="headerlink" href="#layers-1" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#batchnorm" id="toc-entry-1">BatchNorm</a></li>
<li><a class="reference internal" href="#convolution" id="toc-entry-2">Convolution</a></li>
<li><a class="reference internal" href="#dropout" id="toc-entry-3">Dropout</a></li>
<li><a class="reference internal" href="#pooling" id="toc-entry-4">Pooling</a></li>
<li><a class="reference internal" href="#fully-connected-linear" id="toc-entry-5">Fully-connected/Linear</a></li>
<li><a class="reference internal" href="#rnn" id="toc-entry-6">RNN</a></li>
<li><a class="reference internal" href="#gru" id="toc-entry-7">GRU</a></li>
<li><a class="reference internal" href="#lstm" id="toc-entry-8">LSTM</a></li>
</ul>
</div>
<div class="section" id="batchnorm">
<h3><a class="toc-backref" href="#toc-entry-1">BatchNorm</a><a class="headerlink" href="#batchnorm" title="Permalink to this headline">¶</a></h3>
<p>BatchNorm accelerates convergence by reducing internal covariate shift inside each batch.
If the individual observations in the batch are widely different, the gradient
updates will be choppy and take longer to converge.</p>
<p>The batch norm layer normalizes the incoming activations and outputs a new batch
where the mean equals 0 and standard deviation equals 1. It subtracts the mean
and divides by the standard deviation of the batch.</p>
<p class="rubric">Code</p>
<p>Code example from <a class="reference external" href="https://wiseodd.github.io/techblog/2016/07/04/batchnorm/">Agustinus Kristiadi</a></p>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/abs/1502.03167">Original Paper</a></li>
<li><a class="reference external" href="https://wiseodd.github.io/techblog/2016/07/04/batchnorm/">Implementing BatchNorm in Neural Net</a></li>
<li><a class="reference external" href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html">Understanding the backward pass through Batch Norm</a></li>
</ul>
</div>
<div class="section" id="convolution">
<h3><a class="toc-backref" href="#toc-entry-2">Convolution</a><a class="headerlink" href="#convolution" title="Permalink to this headline">¶</a></h3>
<p>In CNN, a convolution is a linear operation that involves multiplication of weight (kernel/filter) with the input and it does most of the heavy lifting job.</p>
<p>Convolution layer consists of 2 major component 1. Kernel(Filter) 2. Stride</p>
<ol class="arabic simple">
<li>Kernel (Filter): A convolution layer can have more than one filter. The size of the filter should be smaller than the size of input dimension. It is intentional as it allows filter to be applied multiple times at difference point (position) on the input.Filters are helpful in understanding and identifying important features from given input. By applying different filters (more than one filter) on the same input helps in extracting different features from given input. Output from multiplying filter with the input gives Two dimensional array. As such, the output array from this operation is called “Feature Map”.</li>
<li>Stride: This property controls the movement of filter over input. when the value is set to 1, then filter moves 1 column at a time over input. When the value is set to 2 then the filer jump 2 columns at a time as filter moves over the input.</li>
</ol>
<p class="rubric">Code</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># this code demonstate on how Convolution works</span>
<span class="c1"># Assume we have a image of 4 X 4 and a filter fo 2 X 2 and Stride = 1</span>

<span class="k">def</span> <span class="nf">conv_filter_ouput</span><span class="p">(</span><span class="n">input_img_section</span><span class="p">,</span><span class="n">filter_value</span><span class="p">):</span>
      <span class="c1"># this method perfromas the multiplication of input and filter</span>
      <span class="c1"># returns singular value</span>

      <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">filter_value</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">filter_value</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
                  <span class="n">value</span> <span class="o">=</span> <span class="n">value</span> <span class="o">+</span> <span class="p">(</span><span class="n">input_img_section</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="n">filter_value</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">value</span>

<span class="n">img_input</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">260.745</span><span class="p">,</span> <span class="mf">261.332</span><span class="p">,</span> <span class="mf">112.27</span> <span class="p">,</span> <span class="mf">262.351</span><span class="p">],</span>
 <span class="p">[</span><span class="mf">260.302</span><span class="p">,</span> <span class="mf">208.802</span><span class="p">,</span> <span class="mf">139.05</span> <span class="p">,</span> <span class="mf">230.709</span><span class="p">],</span>
 <span class="p">[</span><span class="mf">261.775</span><span class="p">,</span>  <span class="mf">93.73</span> <span class="p">,</span> <span class="mf">166.118</span><span class="p">,</span> <span class="mf">122.847</span><span class="p">],</span>
 <span class="p">[</span><span class="mf">259.56</span> <span class="p">,</span> <span class="mf">232.038</span><span class="p">,</span> <span class="mf">262.351</span><span class="p">,</span> <span class="mf">228.937</span><span class="p">]]</span>

<span class="nb">filter</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
   <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]]</span>

<span class="n">filterX</span><span class="p">,</span><span class="n">filterY</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">filter</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="nb">filter</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">filtered_result</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">img_mx</span><span class="p">)</span><span class="o">-</span><span class="n">filterX</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
<span class="n">clm</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">img_mx</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">-</span><span class="n">filterY</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
      <span class="n">clm</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">conv_filter_ouput</span><span class="p">(</span><span class="n">img_mx</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">filterX</span><span class="p">,</span><span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">filterY</span><span class="p">],</span><span class="nb">filter</span><span class="p">))</span>
<span class="n">filtered_result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clm</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">filtered_result</span><span class="p">)</span>
</pre></div>
</div>
<img alt="_images/cnn_filter_output.png" class="align-center" src="_images/cnn_filter_output.png" />
<p class="rubric">Further reading</p>
<ul class="simple">
<li><a class="reference external" href="http://cs231n.github.io/convolutional-networks/">cs231n reference</a></li>
</ul>
</div>
<div class="section" id="dropout">
<h3><a class="toc-backref" href="#toc-entry-3">Dropout</a><a class="headerlink" href="#dropout" title="Permalink to this headline">¶</a></h3>
<p>A dropout layer takes the output of the previous layer’s activations and randomly sets a certain fraction (dropout rate) of the activatons to 0, cancelling or ‘dropping’ them out.</p>
<p>It is a common regularization technique used to prevent overfitting in Neural Networks.</p>
<img alt="_images/dropout_net.png" class="align-center" src="_images/dropout_net.png" />
<p>The dropout rate is the tunable hyperparameter that is adjusted to measure performance with different values. It is typically set between 0.2 and 0.5 (but may be arbitrarily set).</p>
<p>Dropout is only used during training; At test time, no activations are dropped, but scaled down by a factor of dropout rate. This is to account for more units being active during test time than training time.</p>
<p>For example:</p>
<blockquote>
<div><ul class="simple">
<li>A layer in a neural net outputs a tensor (matrix) A of shape (batch_size, num_features).</li>
<li>The dropout rate of the layer is set to 0.5 (50%).</li>
<li>A random 50% of the values in A will be set to 0.</li>
<li>These will then be multiplied with the weight matrix to form the inputs to the next layer.</li>
</ul>
</div></blockquote>
<p>The premise behind dropout is to introduce noise into a layer in order to disrupt any interdependent learning or coincidental patterns that may occur between units in the layer, that aren’t significant.</p>
<p class="rubric">Code</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># layer_output is a 2D numpy matrix of activations</span>

<span class="n">layer_output</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">layer_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># dropping out values</span>

<span class="c1"># scaling up by dropout rate during TRAINING time, so no scaling needs to be done at test time</span>
<span class="n">layer_output</span> <span class="o">/=</span> <span class="mf">0.5</span>
<span class="c1"># OR</span>
<span class="n">layer_output</span> <span class="o">*=</span> <span class="mf">0.5</span> <span class="c1"># Scaling down during TEST time.</span>
</pre></div>
</div>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td></td></tr>
</tbody>
</table>
<p>This results in the following operation.</p>
<img alt="_images/dropout.png" class="align-center" src="_images/dropout.png" />
<p>All reference, images and code examples, unless mentioned otherwise, are from section 4.4.3 of <a class="reference external" href="https://www.manning.com/books/deep-learning-with-python">Deep Learning for Python</a> by François Chollet.</p>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td></td></tr>
</tbody>
</table>
</div>
<div class="section" id="pooling">
<h3><a class="toc-backref" href="#toc-entry-4">Pooling</a><a class="headerlink" href="#pooling" title="Permalink to this headline">¶</a></h3>
<p>Pooling layers often take convolution layers as input. A complicated dataset with many object will require a large number of filters, each responsible finding pattern in an image so the dimensionally of convolutional layer can get large. It will cause an increase of parameters, which can lead to over-fitting. Pooling layers are methods for reducing this high dimensionally. Just like the convolution layer, there is kernel size and stride. The size of the kernel is smaller than the feature map. For most of the cases the size of the kernel will be 2X2 and the stride of 2. There are mainly two types of pooling layers.</p>
<p>The first type is max pooling layer.
Max pooling layer will take a stack of feature maps (convolution layer) as input. The value of the node in the max pooling layer is calculated by just the maximum of the pixels contained&nbsp;in the window.</p>
<p>The other type of&nbsp;pooling layer is the Average Pooling layer.
Average pooling layer calculates the average of pixels contained&nbsp;in the window. Its not used often but you may see this used in applications for which smoothing an image is preferable.</p>
<p class="rubric">Code</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">max_pooling</span><span class="p">(</span><span class="n">feature_map</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    :param feature_map: Feature matrix of shape (height, width, layers)</span>
<span class="sd">    :param size: size of kernal</span>
<span class="sd">    :param stride: movement speed of kernal</span>
<span class="sd">    :return: max-pooled feature vector</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pool_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">feature_map</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="n">stride</span><span class="p">,</span> <span class="n">feature_map</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">//</span><span class="n">stride</span><span class="p">,</span> <span class="n">feature_map</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1">#shape of output</span>
    <span class="n">pool_out</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">pool_shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">feature_map</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
            <span class="c1">#for each layer</span>
            <span class="n">row</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">feature_map</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">stride</span><span class="p">):</span>
                <span class="n">col</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">feature_map</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">stride</span><span class="p">):</span>
                    <span class="n">pool_out</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="n">feature_map</span><span class="p">[</span><span class="n">c</span><span class="p">:</span><span class="n">c</span><span class="o">+</span><span class="n">size</span><span class="p">,</span>  <span class="n">r</span><span class="p">:</span><span class="n">r</span><span class="o">+</span><span class="n">size</span><span class="p">,</span> <span class="n">layer</span><span class="p">]])</span>
                    <span class="n">col</span> <span class="o">=</span> <span class="n">col</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="n">row</span> <span class="o">=</span> <span class="n">row</span> <span class="o">+</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">pool_out</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="_images/maxpool.png"><img alt="_images/maxpool.png" class="align-center" src="_images/maxpool.png" style="width: 512px;" /></a>
</div>
<div class="section" id="fully-connected-linear">
<h3><a class="toc-backref" href="#toc-entry-5">Fully-connected/Linear</a><a class="headerlink" href="#fully-connected-linear" title="Permalink to this headline">¶</a></h3>
<p>In a neural network, a <em>fully-connected layer</em>, also known as <em>linear</em> layer,
is a type of layer where all the inputs from one layer are connected to every
activation unit of the next layer.
In most popular machine learning models, the last few layers in the network are
fully-connected ones. Indeed, this type of layer performs the task of
outputting a class prediction, based on the features learned in the previous
layers.</p>
<div class="figure align-center" id="figure-1">
<a class="reference internal image-reference" href="_images/fc_layer.png"><img alt="_images/fc_layer.png" src="_images/fc_layer.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-text">Example of a fully-connected layer, with four input nodes and eight
output nodes. Source [4].</span></p>
</div>
<p>The fully-connected layer receives in input a vector of nodes, activated in
the previous convolutional layers. This vector passes through one or more
dense layers, before being sent to the output layer.
Before it reaches the output layer, an activation function is used for
making a prediction. While the convolutional and pooling layers generally use
a ReLU function, the fully-connected layer can use <em>two types</em> of activation
functions, based on the type of the classification problem:</p>
<ul class="simple">
<li><strong>Sigmoid:</strong> A logistic function, used for binary classification problems.</li>
<li><strong>Softmax:</strong> A more generalized logistic activation function, it ensures that
the values in the output layer sum up to 1. Commonly used for multi-class
classification.</li>
</ul>
<p>The activation function outputs a vector whose dimension is equal to the number
of classes to be predicted. The output vector yields a probability from 1
to 0 for each class.</p>
</div>
<div class="section" id="rnn">
<h3><a class="toc-backref" href="#toc-entry-6">RNN</a><a class="headerlink" href="#rnn" title="Permalink to this headline">¶</a></h3>
<p>RNN (Recurrent Neural Network) is the neural network with hidden state, which captures the historical information up to current timestep. Because the hidden state of current state uses the same definition as that in previous timestep, which means the computation is recurrent, hence it is called recurrent neural network.(Ref 2)</p>
<p>The structure is as follows:</p>
<a class="reference internal image-reference" href="_images/rnn_layer.png"><img alt="_images/rnn_layer.png" class="align-center" src="_images/rnn_layer.png" style="width: 512px;" /></a>
<p class="rubric">Code</p>
<p>For detail code, refer to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet/blob/master/code/layers.py">layers.py</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RNN</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="c1"># initialization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_hidden_state</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_init_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">]:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="mf">0.01</span>
        <span class="n">Waa</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">])</span>
        <span class="n">Wax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">])</span>
        <span class="n">Wy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">])</span>
        <span class="n">ba</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">by</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">Waa</span><span class="p">,</span> <span class="n">Wax</span><span class="p">,</span> <span class="n">Wy</span><span class="p">,</span> <span class="n">ba</span><span class="p">,</span> <span class="n">by</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_init_hidden_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_vector</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        input_vector:</span>
<span class="sd">            dimension: [num_steps, self.input_dim, self.batch_size]</span>
<span class="sd">        out_vector:</span>
<span class="sd">            dimension: [num_steps, self.output_dim, self.batch_size]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">Waa</span><span class="p">,</span> <span class="n">Wax</span><span class="p">,</span> <span class="n">Wy</span><span class="p">,</span> <span class="n">ba</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
        <span class="n">output_vector</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">vector</span> <span class="ow">in</span> <span class="n">input_vector</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Waa</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wax</span><span class="p">,</span> <span class="n">vector</span><span class="p">)</span> <span class="o">+</span> <span class="n">ba</span>
            <span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wy</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">)</span> <span class="o">+</span> <span class="n">by</span>
            <span class="p">)</span>
            <span class="n">output_vector</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">output_vector</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
        <span class="p">[</span>
            <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
            <span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
            <span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>
        <span class="p">]</span>
        <span class="p">,</span> <span class="p">[</span>
            <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
            <span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
            <span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
        <span class="p">]</span>
    <span class="p">])</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">input_dim</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">output_dim</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">time_step</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">)</span>
    <span class="n">output_vector</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_vector</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RNN:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input data dimensions: </span><span class="si">{input_data.shape}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output data dimensions </span><span class="si">{output_vector.shape}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1">## We will get the following output:</span>
    <span class="c1">##  RNN:</span>
    <span class="c1">## Input data dimensions: (2, 3, 2)</span>
    <span class="c1">## Output data dimensions (2, 4, 2)</span>
</pre></div>
</div>
</div>
<div class="section" id="gru">
<h3><a class="toc-backref" href="#toc-entry-7">GRU</a><a class="headerlink" href="#gru" title="Permalink to this headline">¶</a></h3>
<p>GRU (Gated Recurrent Unit) supports the gating of hidden state:</p>
<ol class="arabic simple">
<li>Reset gate controls how much of previous hidden state we might still want to remember</li>
<li>Update gate controls how much of current hidden state is just a copy of previous state</li>
</ol>
<p>The structure and math are as follow:</p>
<a class="reference internal image-reference" href="_images/gru_structure.png"><img alt="_images/gru_structure.png" class="align-center" src="_images/gru_structure.png" style="width: 512px;" /></a>
<p class="rubric">Code</p>
<p>For detail code, refer to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet/blob/master/code/layers.py">layers.py</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GRU</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="c1"># initialization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_hidden_state</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_init_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">]:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="mf">0.01</span>
        <span class="k">def</span> <span class="nf">param_single_layer</span><span class="p">():</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="o">+</span><span class="n">input_dim</span><span class="p">))</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span>

        <span class="c1"># reset, update gate</span>
        <span class="n">Wr</span><span class="p">,</span> <span class="n">br</span> <span class="o">=</span> <span class="n">param_single_layer</span><span class="p">()</span>
        <span class="n">Wu</span><span class="p">,</span> <span class="n">bu</span> <span class="o">=</span> <span class="n">param_single_layer</span><span class="p">()</span>
        <span class="c1"># output layer</span>
        <span class="n">Wy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">])</span>
        <span class="n">by</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">Wr</span><span class="p">,</span> <span class="n">br</span><span class="p">,</span> <span class="n">Wu</span><span class="p">,</span> <span class="n">bu</span><span class="p">,</span> <span class="n">Wy</span><span class="p">,</span> <span class="n">by</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_init_hidden_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_vector</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        input_vector:</span>
<span class="sd">            dimension: [num_steps, self.input_dim, self.batch_size]</span>
<span class="sd">        out_vector:</span>
<span class="sd">            dimension: [num_steps, self.output_dim, self.batch_size]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">Wr</span><span class="p">,</span> <span class="n">br</span><span class="p">,</span> <span class="n">Wu</span><span class="p">,</span> <span class="n">bu</span><span class="p">,</span> <span class="n">Wy</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
        <span class="n">output_vector</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">vector</span> <span class="ow">in</span> <span class="n">input_vector</span><span class="p">:</span>
            <span class="c1"># expit in scipy is sigmoid function</span>
            <span class="n">reset_gate</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wr</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">vector</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">+</span> <span class="n">br</span>
            <span class="p">)</span>
            <span class="n">update_gate</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wu</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">vector</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">+</span> <span class="n">bu</span>
            <span class="p">)</span>
            <span class="n">candidate_hidden</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span>
                <span class="n">reset_gate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span> <span class="o">=</span> <span class="n">update_gate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">update_gate</span><span class="p">)</span> <span class="o">*</span> <span class="n">candidate_hidden</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wy</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">)</span> <span class="o">+</span> <span class="n">by</span>
            <span class="p">)</span>
            <span class="n">output_vector</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">output_vector</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="lstm">
<h3><a class="toc-backref" href="#toc-entry-8">LSTM</a><a class="headerlink" href="#lstm" title="Permalink to this headline">¶</a></h3>
<p>In order to address the <strong>long-term information preservation</strong> and <strong>shor-term skipping</strong> in latent variable model, we introduced LSTM. In LSTM, we introduce the memory cell that has the same shape as the hidden state, which is actually a fancy version of a hidden state, engineered to record additional information.</p>
<p>The structure and math are as follow:</p>
<a class="reference internal image-reference" href="_images/lstm_structure.png"><img alt="_images/lstm_structure.png" class="align-center" src="_images/lstm_structure.png" style="width: 512px;" /></a>
<p class="rubric">Code</p>
<p>For detail code, refer to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet/blob/master/code/layers.py">layers.py</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LSTM</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="c1"># initialization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_hidden_state</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_hidden_state</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_init_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">]:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="mf">0.01</span>
        <span class="k">def</span> <span class="nf">param_single_layer</span><span class="p">():</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="o">+</span><span class="n">input_dim</span><span class="p">))</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span>

        <span class="c1"># forget, input, output gate + candidate memory state</span>
        <span class="n">Wf</span><span class="p">,</span> <span class="n">bf</span> <span class="o">=</span> <span class="n">param_single_layer</span><span class="p">()</span>
        <span class="n">Wi</span><span class="p">,</span> <span class="n">bi</span> <span class="o">=</span> <span class="n">param_single_layer</span><span class="p">()</span>
        <span class="n">Wo</span><span class="p">,</span> <span class="n">bo</span> <span class="o">=</span> <span class="n">param_single_layer</span><span class="p">()</span>
        <span class="n">Wc</span><span class="p">,</span> <span class="n">bc</span> <span class="o">=</span> <span class="n">param_single_layer</span><span class="p">()</span>
        <span class="c1"># output layer</span>
        <span class="n">Wy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">])</span>
        <span class="n">by</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">Wf</span><span class="p">,</span> <span class="n">bf</span><span class="p">,</span> <span class="n">Wi</span><span class="p">,</span> <span class="n">bi</span><span class="p">,</span> <span class="n">Wo</span><span class="p">,</span> <span class="n">bo</span><span class="p">,</span> <span class="n">Wc</span><span class="p">,</span> <span class="n">bc</span><span class="p">,</span> <span class="n">Wy</span><span class="p">,</span> <span class="n">by</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_init_hidden_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_vector</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        input_vector:</span>
<span class="sd">            dimension: [num_steps, self.input_dim, self.batch_size]</span>
<span class="sd">        out_vector:</span>
<span class="sd">            dimension: [num_steps, self.output_dim, self.batch_size]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">Wf</span><span class="p">,</span> <span class="n">bf</span><span class="p">,</span> <span class="n">Wi</span><span class="p">,</span> <span class="n">bi</span><span class="p">,</span> <span class="n">Wo</span><span class="p">,</span> <span class="n">bo</span><span class="p">,</span> <span class="n">Wc</span><span class="p">,</span> <span class="n">bc</span><span class="p">,</span> <span class="n">Wy</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
        <span class="n">output_vector</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">vector</span> <span class="ow">in</span> <span class="n">input_vector</span><span class="p">:</span>
            <span class="c1"># expit in scipy is sigmoid function</span>
            <span class="n">foget_gate</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wf</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">vector</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">+</span> <span class="n">bf</span>
            <span class="p">)</span>
            <span class="n">input_gate</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wi</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">vector</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">+</span> <span class="n">bi</span>
            <span class="p">)</span>
            <span class="n">output_gate</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wo</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">vector</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">+</span> <span class="n">bo</span>
            <span class="p">)</span>
            <span class="n">candidate_memory</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wc</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">vector</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">+</span> <span class="n">bc</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">memory_state</span> <span class="o">=</span> <span class="n">foget_gate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_state</span> <span class="o">+</span> <span class="n">input_gate</span> <span class="o">*</span> <span class="n">candidate_memory</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span> <span class="o">=</span> <span class="n">output_gate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory_state</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wy</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">)</span> <span class="o">+</span> <span class="n">by</span>
            <span class="p">)</span>
            <span class="n">output_vector</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">output_vector</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="http://www.deeplearningbook.org/contents/convnets.html">http://www.deeplearningbook.org/contents/convnets.html</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>“4.4.3, Fundamentals of Machine Learning: Adding Dropout.” <a class="reference external" href="https://www.manning.com/books/deep-learning-with-python">Deep Learning for Python</a>, by Chollet, François. Manning Publications Co., 2018, pp. 109–110.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><a href="#system-message-1"><span class="problematic" id="problematic-1">`Dive into Deep Learning https://d2l.ai/index.html`_</span></a>, by Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td><a class="reference external" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#fullyconnected-layer">https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#fullyconnected-layer</a></td></tr>
</tbody>
</table>
</div>
</div>
<span id="document-loss_functions"></span><div class="section" id="loss-functions">
<span id="cost-function"></span><h2>Loss Functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#cross-entropy" id="toc-entry-1">Cross-Entropy</a></li>
<li><a class="reference internal" href="#hinge" id="toc-entry-2">Hinge</a></li>
<li><a class="reference internal" href="#huber" id="toc-entry-3">Huber</a></li>
<li><a class="reference internal" href="#kullback-leibler" id="toc-entry-4">Kullback-Leibler</a></li>
<li><a class="reference internal" href="#rmse-1" id="toc-entry-5">RMSE</a></li>
<li><a class="reference internal" href="#mae-l1" id="toc-entry-6">MAE (L1)</a></li>
<li><a class="reference internal" href="#mse-l2" id="toc-entry-7">MSE (L2)</a></li>
</ul>
</div>
<div class="section" id="cross-entropy">
<span id="loss-cross-entropy"></span><h3><a class="toc-backref" href="#toc-entry-1">Cross-Entropy</a><a class="headerlink" href="#cross-entropy" title="Permalink to this headline">¶</a></h3>
<p>Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.</p>
<img alt="_images/cross_entropy.png" class="align-center" src="_images/cross_entropy.png" />
<p>The graph above shows the range of possible loss values given a true observation (isDog = 1). As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predictions that are confident and wrong!</p>
<p>Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing.</p>
<p class="rubric">Code</p>
<p class="rubric">Math</p>
<p>In binary classification, where the number of classes <span class="math notranslate nohighlight">\(M\)</span> equals 2, cross-entropy can be calculated as:</p>
<div class="math notranslate nohighlight">
\[-{(y\log(p) + (1 - y)\log(1 - p))}\]</div>
<p>If <span class="math notranslate nohighlight">\(M &gt; 2\)</span> (i.e. multiclass classification), we calculate a separate loss for each class label per observation and sum the result.</p>
<div class="math notranslate nohighlight">
\[-\sum_{c=1}^My_{o,c}\log(p_{o,c})\]</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last simple">
<li>M - number of classes (dog, cat, fish)</li>
<li>log - the natural log</li>
<li>y - binary indicator (0 or 1) if class label <span class="math notranslate nohighlight">\(c\)</span> is the correct classification for observation <span class="math notranslate nohighlight">\(o\)</span></li>
<li>p - predicted probability observation <span class="math notranslate nohighlight">\(o\)</span> is of class <span class="math notranslate nohighlight">\(c\)</span></li>
</ul>
</div>
</div>
<div class="section" id="hinge">
<span id="hinge-loss"></span><h3><a class="toc-backref" href="#toc-entry-2">Hinge</a><a class="headerlink" href="#hinge" title="Permalink to this headline">¶</a></h3>
<p>Used for classification.</p>
<p class="rubric">Code</p>
</div>
<div class="section" id="huber">
<span id="huber-loss"></span><h3><a class="toc-backref" href="#toc-entry-3">Huber</a><a class="headerlink" href="#huber" title="Permalink to this headline">¶</a></h3>
<p>Typically used for regression. It’s less sensitive to outliers than the MSE as it treats error as square only inside an interval.</p>
<div class="math notranslate nohighlight">
\[\begin{split}L_{\delta}=\left\{\begin{matrix}
\frac{1}{2}(y - \hat{y})^{2} &amp; if \left | (y - \hat{y})  \right | &lt; \delta\\
\delta ((y - \hat{y}) - \frac1 2 \delta) &amp; otherwise
\end{matrix}\right.\end{split}\]</div>
<p class="rubric">Code</p>
<p>Further information can be found at <a class="reference external" href="https://en.wikipedia.org/wiki/Huber_loss">Huber Loss in Wikipedia</a>.</p>
</div>
<div class="section" id="kullback-leibler">
<span id="kl-divergence"></span><h3><a class="toc-backref" href="#toc-entry-4">Kullback-Leibler</a><a class="headerlink" href="#kullback-leibler" title="Permalink to this headline">¶</a></h3>
<p class="rubric">Code</p>
</div>
<div class="section" id="rmse-1">
<span id="rmse"></span><h3><a class="toc-backref" href="#toc-entry-5">RMSE</a><a class="headerlink" href="#rmse-1" title="Permalink to this headline">¶</a></h3>
<p>Root Mean Square Error</p>
<div class="math notranslate nohighlight">
\[RMSE = \sqrt{\frac{1}{m}\sum^{m}_{i=1}(h(x^{(i)})-y^{(i)})^2}\]</div>
<div class="line-block">
<div class="line">RMSE - root mean square error</div>
<div class="line">m - number of samples</div>
<div class="line"><span class="math notranslate nohighlight">\(x^{(i)}\)</span> - i-th sample from dataset</div>
<div class="line"><span class="math notranslate nohighlight">\(h(x^{(i)})\)</span> - prediction for i-th sample (thesis)</div>
<div class="line"><span class="math notranslate nohighlight">\(y^{(i)}\)</span> - ground truth label for i-th sample</div>
</div>
<p class="rubric">Code</p>
</div>
<div class="section" id="mae-l1">
<span id="mae"></span><h3><a class="toc-backref" href="#toc-entry-6">MAE (L1)</a><a class="headerlink" href="#mae-l1" title="Permalink to this headline">¶</a></h3>
<p>Mean Absolute Error, or L1 loss. Excellent overview below [6] and [10].</p>
<div class="math notranslate nohighlight">
\[MAE = \frac{1}{m}\sum^{m}_{i=1}|h(x^{(i)})-y^{(i)}|\]</div>
<div class="line-block">
<div class="line">MAE - mean absolute error</div>
<div class="line">m - number of samples</div>
<div class="line"><span class="math notranslate nohighlight">\(x^{(i)}\)</span> - i-th sample from dataset</div>
<div class="line"><span class="math notranslate nohighlight">\(h(x^{(i)})\)</span> - prediction for i-th sample (thesis)</div>
<div class="line"><span class="math notranslate nohighlight">\(y^{(i)}\)</span> - ground truth label for i-th sample</div>
</div>
<p class="rubric">Code</p>
</div>
<div class="section" id="mse-l2">
<span id="mse"></span><h3><a class="toc-backref" href="#toc-entry-7">MSE (L2)</a><a class="headerlink" href="#mse-l2" title="Permalink to this headline">¶</a></h3>
<p>Mean Squared Error, or L2 loss. Excellent overview below [6] and [10].</p>
<div class="math notranslate nohighlight">
\[MSE = \frac{1}{m}\sum^{m}_{i=1}(y^{(i)} - \hat{y}^{(i)})^2\]</div>
<div class="line-block">
<div class="line">MSE - mean square error</div>
<div class="line">m - number of samples</div>
<div class="line"><span class="math notranslate nohighlight">\(y^{(i)}\)</span> - ground truth label for i-th sample</div>
<div class="line"><span class="math notranslate nohighlight">\(\hat{y}^{(i)}\)</span> - predicted label for i-th sample</div>
</div>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="https://en.m.wikipedia.org/wiki/Cross_entropy">https://en.m.wikipedia.org/wiki/Cross_entropy</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="https://www.kaggle.com/wiki/LogarithmicLoss">https://www.kaggle.com/wiki/LogarithmicLoss</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><a class="reference external" href="https://en.wikipedia.org/wiki/Loss_functions_for_classification">https://en.wikipedia.org/wiki/Loss_functions_for_classification</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td><a class="reference external" href="http://www.exegetic.biz/blog/2015/12/making-sense-logarithmic-loss/">http://www.exegetic.biz/blog/2015/12/making-sense-logarithmic-loss/</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[5]</td><td><a class="reference external" href="http://neuralnetworksanddeeplearning.com/chap3.html">http://neuralnetworksanddeeplearning.com/chap3.html</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[6]</td><td><a class="reference external" href="http://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/">http://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[7]</td><td><a class="reference external" href="https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient">https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[8]</td><td><a class="reference external" href="https://en.wikipedia.org/wiki/Huber_loss">https://en.wikipedia.org/wiki/Huber_loss</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[9]</td><td><a class="reference external" href="https://en.wikipedia.org/wiki/Hinge_loss">https://en.wikipedia.org/wiki/Hinge_loss</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[10]</td><td><a class="reference external" href="http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/">http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/</a></td></tr>
</tbody>
</table>
</div>
</div>
<span id="document-optimizers"></span><div class="section" id="optimizers-1">
<span id="optimizers"></span><h2>Optimizers<a class="headerlink" href="#optimizers-1" title="Permalink to this headline">¶</a></h2>
<p class="rubric">What is Optimizer ?</p>
<p>It is very important to tweak the weights of the model during the training process, to make our predictions as correct and optimized as possible. But how exactly do you do that? How do you change the parameters of your model, by how much, and when?</p>
<p>Best answer to all above question is <em>optimizers</em>. They tie together the loss function and model parameters by updating the model in response to the output of the loss function. In simpler terms, optimizers shape and mold your model into its most accurate possible form by futzing with the weights. The loss function is the guide to the terrain, telling the optimizer when it’s moving in the right or wrong direction.</p>
<p>Below are list of example optimizers</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#adagrad" id="toc-entry-1">Adagrad</a></li>
<li><a class="reference internal" href="#adadelta" id="toc-entry-2">Adadelta</a></li>
<li><a class="reference internal" href="#adam" id="toc-entry-3">Adam</a></li>
<li><a class="reference internal" href="#conjugate-gradients" id="toc-entry-4">Conjugate Gradients</a></li>
<li><a class="reference internal" href="#bfgs" id="toc-entry-5">BFGS</a></li>
<li><a class="reference internal" href="#momentum" id="toc-entry-6">Momentum</a></li>
<li><a class="reference internal" href="#nesterov-momentum" id="toc-entry-7">Nesterov Momentum</a></li>
<li><a class="reference internal" href="#newton-s-method" id="toc-entry-8">Newton’s Method</a></li>
<li><a class="reference internal" href="#rmsprop" id="toc-entry-9">RMSProp</a></li>
<li><a class="reference internal" href="#sgd" id="toc-entry-10">SGD</a></li>
</ul>
</div>
<img alt="_images/optimizers.gif" class="align-center" src="_images/optimizers.gif" />
<p>Image Credit: <a class="reference external" href="https://cs231n.github.io/neural-networks-3/">CS231n</a></p>
<div class="section" id="adagrad">
<h3><a class="toc-backref" href="#toc-entry-1">Adagrad</a><a class="headerlink" href="#adagrad" title="Permalink to this headline">¶</a></h3>
<p>Adagrad (short for adaptive gradient) adaptively sets the learning rate according to a parameter.</p>
<ul class="simple">
<li>Parameters that have higher gradients or frequent updates should have slower learning rate so that we do not overshoot the minimum value.</li>
<li>Parameters that have low gradients or infrequent updates should faster learning rate so that they get trained quickly.</li>
<li>It divides the learning rate by the sum of squares of all previous gradients of the parameter.</li>
<li>When the sum of the squared past gradients has a high value, it basically divides the learning rate by a high value, so the learning rate will become less.</li>
<li>Similarly, if the sum of the squared past gradients has a low value, it divides the learning rate by a lower value, so the learning rate value will become high.</li>
<li>This implies that the learning rate is inversely proportional to the sum of the squares of all the previous gradients of the parameter.</li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}g_{t}^{i} = \frac{\partial \mathcal{J}(w_{t}^{i})}{\partial W} \\
W = W - \alpha \frac{\partial \mathcal{J}(w_{t}^{i})}{\sqrt{\sum_{r=1}^{t}\left ( g_{r}^{i} \right )^{2} + \varepsilon }}\end{split}\]</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last simple">
<li><span class="math notranslate nohighlight">\(g_{t}^{i}\)</span> - the gradient of a parameter, :math: <a href="#system-message-1"><span class="problematic" id="problematic-1">`</span></a>Theta `  at an iteration t.</li>
<li><span class="math notranslate nohighlight">\(\alpha\)</span> - the learning rate</li>
<li><span class="math notranslate nohighlight">\(\epsilon\)</span> - very small value to avoid dividing by zero</li>
</ul>
</div>
</div>
<div class="section" id="adadelta">
<h3><a class="toc-backref" href="#toc-entry-2">Adadelta</a><a class="headerlink" href="#adadelta" title="Permalink to this headline">¶</a></h3>
<p>AdaDelta belongs to the family of stochastic gradient descent algorithms, that provide adaptive techniques for hyperparameter tuning. Adadelta is probably short for ‘adaptive delta’, where delta here refers to the difference between the current weight and the newly updated weight.</p>
<p>The main disadvantage in Adagrad is its accumulation of the squared gradients. During the training process, the accumulated sum keeps growing. From the above formala we can see that, As the accumulated sum increases learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge.</p>
<p>Adadelta is a more robust extension of Adagrad that adapts learning rates based on a moving window of gradient updates, instead of accumulating all past gradients. This way, Adadelta continues learning even when many updates have been done.</p>
<p>With Adadelta, we do not even need to set a default learning rate, as it has been eliminated from the update rule.</p>
<p>Implementation is something like this,</p>
<div class="math notranslate nohighlight">
\[\begin{split}v_t = \rho v_{t-1} + (1-\rho) \nabla_\theta^2 J( \theta) \\
\Delta\theta &amp;= \dfrac{\sqrt{w_t + \epsilon}}{\sqrt{v_t + \epsilon}} \nabla_\theta J( \theta) \\
\theta &amp;= \theta - \eta \Delta\theta \\
w_t = \rho w_{t-1} + (1-\rho) \Delta\theta^2\end{split}\]</div>
</div>
<div class="section" id="adam">
<h3><a class="toc-backref" href="#toc-entry-3">Adam</a><a class="headerlink" href="#adam" title="Permalink to this headline">¶</a></h3>
<p>Adaptive Moment Estimation (Adam) combines ideas from both RMSProp and Momentum. It computes adaptive learning rates for each parameter and works as follows.</p>
<ul class="simple">
<li>First, it computes the exponentially weighted average of past gradients (<span class="math notranslate nohighlight">\(v_{dW}\)</span>).</li>
<li>Second, it computes the exponentially weighted average of the squares of past gradients (<span class="math notranslate nohighlight">\(s_{dW}\)</span>).</li>
<li>Third, these averages have a bias towards zero and to counteract this a bias correction is applied (<span class="math notranslate nohighlight">\(v_{dW}^{corrected}\)</span>, <span class="math notranslate nohighlight">\(s_{dW}^{corrected}\)</span>).</li>
<li>Lastly, the parameters are updated using the information from the calculated averages.</li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}v_{dW} = \beta_1 v_{dW} + (1 - \beta_1) \frac{\partial \mathcal{J} }{ \partial W } \\
s_{dW} = \beta_2 s_{dW} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W })^2 \\
v^{corrected}_{dW} = \frac{v_{dW}}{1 - (\beta_1)^t} \\
s^{corrected}_{dW} = \frac{s_{dW}}{1 - (\beta_1)^t} \\
W = W - \alpha \frac{v^{corrected}_{dW}}{\sqrt{s^{corrected}_{dW}} + \varepsilon}\end{split}\]</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last simple">
<li><span class="math notranslate nohighlight">\(v_{dW}\)</span> - the exponentially weighted average of past gradients</li>
<li><span class="math notranslate nohighlight">\(s_{dW}\)</span> - the exponentially weighted average of past squares of gradients</li>
<li><span class="math notranslate nohighlight">\(\beta_1\)</span> - hyperparameter to be tuned</li>
<li><span class="math notranslate nohighlight">\(\beta_2\)</span> - hyperparameter to be tuned</li>
<li><span class="math notranslate nohighlight">\(\frac{\partial \mathcal{J} }{ \partial W }\)</span> - cost gradient with respect to current layer</li>
<li><span class="math notranslate nohighlight">\(W\)</span> - the weight matrix (parameter to be updated)</li>
<li><span class="math notranslate nohighlight">\(\alpha\)</span> - the learning rate</li>
<li><span class="math notranslate nohighlight">\(\epsilon\)</span> - very small value to avoid dividing by zero</li>
</ul>
</div>
</div>
<div class="section" id="conjugate-gradients">
<h3><a class="toc-backref" href="#toc-entry-4">Conjugate Gradients</a><a class="headerlink" href="#conjugate-gradients" title="Permalink to this headline">¶</a></h3>
<p>Be the first to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet">contribute!</a></p>
</div>
<div class="section" id="bfgs">
<span id="optimizers-lbfgs"></span><h3><a class="toc-backref" href="#toc-entry-5">BFGS</a><a class="headerlink" href="#bfgs" title="Permalink to this headline">¶</a></h3>
<p>Be the first to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet">contribute!</a></p>
</div>
<div class="section" id="momentum">
<h3><a class="toc-backref" href="#toc-entry-6">Momentum</a><a class="headerlink" href="#momentum" title="Permalink to this headline">¶</a></h3>
<p>Used in conjunction Stochastic Gradient Descent (sgd) or Mini-Batch Gradient Descent, Momentum takes into account
past gradients to smooth out the update. This is seen in variable <span class="math notranslate nohighlight">\(v\)</span> which is an exponentially weighted average
of the gradient on previous steps. This results in minimizing oscillations and faster convergence.</p>
<div class="math notranslate nohighlight">
\[\begin{split}v_{dW} = \beta v_{dW} + (1 - \beta) \frac{\partial \mathcal{J} }{ \partial W } \\
W = W - \alpha v_{dW}\end{split}\]</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last simple">
<li><span class="math notranslate nohighlight">\(v\)</span> - the exponentially weighted average of past gradients</li>
<li><span class="math notranslate nohighlight">\(\frac{\partial \mathcal{J} }{ \partial W }\)</span> - cost gradient with respect to current layer weight tensor</li>
<li><span class="math notranslate nohighlight">\(W\)</span> - weight tensor</li>
<li><span class="math notranslate nohighlight">\(\beta\)</span> - hyperparameter to be tuned</li>
<li><span class="math notranslate nohighlight">\(\alpha\)</span> - the learning rate</li>
</ul>
</div>
</div>
<div class="section" id="nesterov-momentum">
<h3><a class="toc-backref" href="#toc-entry-7">Nesterov Momentum</a><a class="headerlink" href="#nesterov-momentum" title="Permalink to this headline">¶</a></h3>
<p>Be the first to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet">contribute!</a></p>
</div>
<div class="section" id="newton-s-method">
<h3><a class="toc-backref" href="#toc-entry-8">Newton’s Method</a><a class="headerlink" href="#newton-s-method" title="Permalink to this headline">¶</a></h3>
<p>Be the first to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet">contribute!</a></p>
</div>
<div class="section" id="rmsprop">
<h3><a class="toc-backref" href="#toc-entry-9">RMSProp</a><a class="headerlink" href="#rmsprop" title="Permalink to this headline">¶</a></h3>
<p>Another adaptive learning rate optimization algorithm, Root Mean Square Prop (RMSProp) works by keeping an exponentially weighted average of the squares of past gradients.
RMSProp then divides the learning rate by this average to speed up convergence.</p>
<div class="math notranslate nohighlight">
\[\begin{split}s_{dW} = \beta s_{dW} + (1 - \beta) (\frac{\partial \mathcal{J} }{\partial W })^2 \\
W = W - \alpha \frac{\frac{\partial \mathcal{J} }{\partial W }}{\sqrt{s^{corrected}_{dW}} + \varepsilon}\end{split}\]</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last simple">
<li><span class="math notranslate nohighlight">\(s\)</span> - the exponentially weighted average of past squares of gradients</li>
<li><span class="math notranslate nohighlight">\(\frac{\partial \mathcal{J} }{\partial W }\)</span> - cost gradient with respect to current layer weight tensor</li>
<li><span class="math notranslate nohighlight">\(W\)</span> - weight tensor</li>
<li><span class="math notranslate nohighlight">\(\beta\)</span> - hyperparameter to be tuned</li>
<li><span class="math notranslate nohighlight">\(\alpha\)</span> - the learning rate</li>
<li><span class="math notranslate nohighlight">\(\epsilon\)</span> - very small value to avoid dividing by zero</li>
</ul>
</div>
</div>
<div class="section" id="sgd">
<h3><a class="toc-backref" href="#toc-entry-10">SGD</a><a class="headerlink" href="#sgd" title="Permalink to this headline">¶</a></h3>
<p>SGD stands for Stochastic Gradient Descent.In Stochastic Gradient Descent, a few samples are selected randomly instead of the whole data set for each iteration. In Gradient Descent, there is a term called “batch” which denotes the total number of samples from a dataset that is used for calculating the gradient for each iteration. In typical Gradient Descent optimization, like Batch Gradient Descent, the batch is taken to be the whole dataset. Although, using the whole dataset is really useful for getting to the minima in a less noisy or less random manner, but the problem arises when our datasets get really huge.</p>
<p>This problem is solved by Stochastic Gradient Descent. In SGD, it uses only a single sample to perform each iteration. The sample is randomly shuffled and selected for performing the iteration.</p>
<p>Since only one sample from the dataset is chosen at random for each iteration, the path taken by the algorithm to reach the minima is usually noisier than your typical Gradient Descent algorithm. But that doesn’t matter all that much because the path taken by the algorithm does not matter, as long as we reach the minima and with significantly shorter training time.</p>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="https://ruder.io/optimizing-gradient-descent/">https://ruder.io/optimizing-gradient-descent/</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="http://www.deeplearningbook.org/contents/optimization.html">http://www.deeplearningbook.org/contents/optimization.html</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><a class="reference external" href="https://arxiv.org/pdf/1502.03167.pdf">https://arxiv.org/pdf/1502.03167.pdf</a></td></tr>
</tbody>
</table>
</div>
</div>
<span id="document-regularization"></span><div class="section" id="regularization-1">
<span id="regularization"></span><h2>Regularization<a class="headerlink" href="#regularization-1" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#data-augmentation" id="toc-entry-1">Data Augmentation</a></li>
<li><a class="reference internal" href="#dropout" id="toc-entry-2">Dropout</a></li>
<li><a class="reference internal" href="#early-stopping" id="toc-entry-3">Early Stopping</a></li>
<li><a class="reference internal" href="#ensembling" id="toc-entry-4">Ensembling</a></li>
<li><a class="reference internal" href="#injecting-noise" id="toc-entry-5">Injecting Noise</a></li>
<li><a class="reference internal" href="#l1-regularization" id="toc-entry-6">L1 Regularization</a></li>
<li><a class="reference internal" href="#l2-regularization" id="toc-entry-7">L2 Regularization</a></li>
</ul>
</div>
<p class="rubric">What is overfitting?</p>
<p>From Wikipedia <a class="reference external" href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> is,</p>
<p>The production of an analysis that corresponds too closely or exactly to a particular set
of data, and may therefore fail to fit additional data or predict future observations
reliably</p>
<p class="rubric">What is Regularization?</p>
<p>It is a Techniques for combating overfitting and improving training.</p>
<div class="section" id="data-augmentation">
<h3><a class="toc-backref" href="#toc-entry-1">Data Augmentation</a><a class="headerlink" href="#data-augmentation" title="Permalink to this headline">¶</a></h3>
<p>Having more data is the surest way to get better consistent estimators (ML model). Unfortunately, in the real world getting a large volume of useful data for training a model is cumbersome and labelling is an extremely tedious (or expensive) task.</p>
<p>‘Gold standard’ labelling requires more manual annotation. For example, in order to develop a better image classifier we can use Mturk and involve more man power to generate dataset, or we could crowdsource by posting on social media and asking people to contribute.
The above process can yield good datasets; however, those are difficult to carry and expensive. On the other hand, having a small dataset will lead to the well-known problem of overfitting.</p>
<p>Data Augmentation is one interesting regularization technique to resolve the above problem. The concept is very simple, this technique generates new training data from given original dataset. Dataset Augmentation provides a cheap and easy way to increase the volume of training data.</p>
<p>This technique can be used for both NLP and CV.</p>
<p>In CV we can use the techniques like Jitter, PCA and Flipping. Similarly in NLP we can use the techniques like Synonym Replacement,Random Insertion, Random Deletion and Word Embeddings.</p>
<p>Many software libraries contain tools for data augmentation. For example, Keras provides the ImageDataGenerator for augmenting image datasets.</p>
<p>Sample code for random deletion</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_deletion</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Randomly delete words from the sentence with probability p</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1">#obviously, if there&#39;s only one word, don&#39;t delete it</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">words</span>

        <span class="c1">#randomly delete words with probability p</span>
        <span class="n">new_words</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
                <span class="n">r</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">r</span> <span class="o">&gt;</span> <span class="n">p</span><span class="p">:</span>
                        <span class="n">new_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

        <span class="c1">#if you end up deleting all words, just return a random word</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_words</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">rand_int</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">return</span> <span class="p">[</span><span class="n">words</span><span class="p">[</span><span class="n">rand_int</span><span class="p">]]</span>

        <span class="k">return</span> <span class="n">new_words</span>
</pre></div>
</div>
<p>Furthermore, when comparing two machine learning algorithms, it is important to train both with either augmented or non-augmented dataset. Otherwise, no subjective decision can be made on which algorithm performed better</p>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/abs/1901.11196">NLP Data Augmentation</a></li>
<li><a class="reference external" href="https://arxiv.org/abs/1904.12848">CV Data Augmentation</a></li>
<li><a class="reference external" href="http://wavelab.uwaterloo.ca/wp-content/uploads/2017/04/Lecture_3.pdf">Regularization</a></li>
</ul>
</div>
<div class="section" id="dropout">
<h3><a class="toc-backref" href="#toc-entry-2">Dropout</a><a class="headerlink" href="#dropout" title="Permalink to this headline">¶</a></h3>
<p class="rubric">What is Dropout?</p>
<p>Dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data.</p>
<p>Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.</p>
<p>Simply put, It is the process of ignoring some of the neurons in particular forward or backward pass.</p>
<p>Dropout can be easily implemented by randomly selecting nodes to be dropped-out with a given probability (e.g. .1%) each weight update cycle.</p>
<p>Most importantly Dropout is only used during the training of a model and is not used when evaluating the model.</p>
<img alt="_images/regularization-dropout.PNG" class="align-center" src="_images/regularization-dropout.PNG" />
<p>image from <a class="reference external" href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Given input: &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">drop_probability</span><span class="p">):</span>
    <span class="n">keep_probability</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">drop_probability</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">keep_probability</span>
    <span class="k">if</span> <span class="n">keep_probability</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">keep_probability</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">return</span> <span class="n">mask</span> <span class="o">*</span> <span class="n">X</span> <span class="o">*</span> <span class="n">scale</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> After Dropout: &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dropout</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="mf">0.5</span><span class="p">))</span>
</pre></div>
</div>
<p>output from above code</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Given</span> <span class="nb">input</span><span class="p">:</span>
<span class="p">[[</span> <span class="mi">0</span>  <span class="mi">1</span>  <span class="mi">2</span>  <span class="mi">3</span><span class="p">]</span>
<span class="p">[</span> <span class="mi">4</span>  <span class="mi">5</span>  <span class="mi">6</span>  <span class="mi">7</span><span class="p">]</span>
<span class="p">[</span> <span class="mi">8</span>  <span class="mi">9</span> <span class="mi">10</span> <span class="mi">11</span><span class="p">]</span>
<span class="p">[</span><span class="mi">12</span> <span class="mi">13</span> <span class="mi">14</span> <span class="mi">15</span><span class="p">]</span>
<span class="p">[</span><span class="mi">16</span> <span class="mi">17</span> <span class="mi">18</span> <span class="mi">19</span><span class="p">]]</span>

<span class="n">After</span> <span class="n">Dropout</span><span class="p">:</span>
<span class="p">[[</span> <span class="mf">0.</span>  <span class="mf">2.</span>  <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">8.</span>  <span class="mf">0.</span>  <span class="mf">0.</span> <span class="mf">14.</span><span class="p">]</span>
<span class="p">[</span><span class="mf">16.</span> <span class="mf">18.</span>  <span class="mf">0.</span> <span class="mf">22.</span><span class="p">]</span>
<span class="p">[</span><span class="mf">24.</span>  <span class="mf">0.</span>  <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]</span>
<span class="p">[</span><span class="mf">32.</span> <span class="mf">34.</span> <span class="mf">36.</span>  <span class="mf">0.</span><span class="p">]]</span>
</pre></div>
</div>
<p class="rubric">Further reading</p>
<ul class="simple">
<li>Dropout <a class="reference external" href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a></li>
</ul>
</div>
<div class="section" id="early-stopping">
<h3><a class="toc-backref" href="#toc-entry-3">Early Stopping</a><a class="headerlink" href="#early-stopping" title="Permalink to this headline">¶</a></h3>
<p>One of the biggest problem in training neural network is how long to train the model.</p>
<p>Training too little will lead to underfit in train and test sets. Traning too much will have the overfit in training set and poor result in test sets.</p>
<p>Here the challenge is to train the network long enough that it is capable of learning the mapping from inputs to outputs, but not training the model so long that it overfits the training data.</p>
<p>One possible solution to solve this problem is to treat the number of training epochs as a hyperparameter and train the model multiple times with different values, then select the number of epochs that result in the best accuracy on the train or a holdout test dataset, But the problem is it requires multiple models to be trained and discarded.</p>
<img alt="_images/earlystopping.png" class="align-center" src="_images/earlystopping.png" />
<p>Clearly, after ‘t’ epochs, the model starts overfitting. This is clear by the increasing gap between the train and the validation error in the above plot.</p>
<p>One alternative technique to prevent overfitting is use validation error to decide when to stop. This approach is called Early Stopping.</p>
<p>While building the model, it is evaluated on the holdout validation dataset after each epoch. If the accuracy of the model on the validation dataset starts to degrade (e.g. loss begins to increase or accuracy begins to decrease), then the training process is stopped. This process is called Early stopping.</p>
<p>Python implementation for Early stopping,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">early_stopping</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="mi">100</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; The early stopping meta-algorithm for determining the best amount of time to train.</span>
<span class="sd">      REF: Algorithm 7.1 in deep learning book.</span>

<span class="sd">      Parameters:</span>
<span class="sd">      n: int; Number of steps between evaluations.</span>
<span class="sd">      p: int; &quot;patience&quot;, the number of evaluations to observe worsening validataion set.</span>
<span class="sd">      theta0: Network; initial network.</span>
<span class="sd">      x_train: iterable; The training input set.</span>
<span class="sd">      y_train: iterable; The training output set.</span>
<span class="sd">      x_valid: iterable; The validation input set.</span>
<span class="sd">      y_valid: iterable; The validation output set.</span>

<span class="sd">      Returns:</span>
<span class="sd">      theta_prime: Network object; The output network.</span>
<span class="sd">      i_prime: int; The number of iterations for the output network.</span>
<span class="sd">      v: float; The validation error for the output network.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Initialize variables</span>
  <span class="n">theta</span> <span class="o">=</span> <span class="n">theta0</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>       <span class="c1"># The active network</span>
  <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>                        <span class="c1"># The number of training steps taken</span>
  <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>                        <span class="c1"># The number of evaluations steps since last update of theta_prime</span>
  <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>                   <span class="c1"># The best evaluation error observed thusfar</span>
  <span class="n">theta_prime</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>  <span class="c1"># The best network found thusfar</span>
  <span class="n">i_prime</span> <span class="o">=</span> <span class="n">i</span>                  <span class="c1"># The index of theta_prime</span>

  <span class="k">while</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">p</span><span class="p">:</span>
      <span class="c1"># Update theta by running the training algorithm for n steps</span>
      <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

      <span class="c1"># Update Values</span>
      <span class="n">i</span> <span class="o">+=</span> <span class="n">n</span>
      <span class="n">v_new</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">x_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)</span>

      <span class="c1"># If better validation error, then reset waiting time, save the network, and update the best error value</span>
      <span class="k">if</span> <span class="n">v_new</span> <span class="o">&lt;</span> <span class="n">v</span><span class="p">:</span>
          <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>
          <span class="n">theta_prime</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
          <span class="n">i_prime</span> <span class="o">=</span> <span class="n">i</span>
          <span class="n">v</span> <span class="o">=</span> <span class="n">v_new</span>

      <span class="c1"># Otherwise, update the waiting time</span>
      <span class="k">else</span><span class="p">:</span>
          <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>

  <span class="k">return</span> <span class="n">theta_prime</span><span class="p">,</span> <span class="n">i_prime</span><span class="p">,</span> <span class="n">v</span>
</pre></div>
</div>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><a class="reference external" href="http://wavelab.uwaterloo.ca/wp-content/uploads/2017/04/Lecture_3.pdf">Regularization</a></li>
</ul>
</div>
<div class="section" id="ensembling">
<h3><a class="toc-backref" href="#toc-entry-4">Ensembling</a><a class="headerlink" href="#ensembling" title="Permalink to this headline">¶</a></h3>
<p>Ensemble methods combine several machine learning techniques into one predictive model. There are a few different methods for ensembling, but the two most common are:</p>
<p class="rubric">Bagging</p>
<ul class="simple">
<li>Bagging stands for bootstrap aggregation. One way to reduce the variance of an estimate is to average together multiple estimates.</li>
<li>It trains a large number of “strong” learners in parallel.</li>
<li>A strong learner is a model that’s relatively unconstrained.</li>
<li>Bagging then combines all the strong learners together in order to “smooth out” their predictions.</li>
</ul>
<p class="rubric">Boosting</p>
<ul class="simple">
<li>Boosting refers to a family of algorithms that are able to convert weak learners to strong learners.</li>
<li>Each one in the sequence focuses on learning from the mistakes of the one before it.</li>
<li>Boosting then combines all the weak learners into a single strong learner.</li>
</ul>
<p>Bagging uses complex base models and tries to “smooth out” their predictions, while boosting uses simple base models and tries to “boost” their aggregate complexity.</p>
</div>
<div class="section" id="injecting-noise">
<h3><a class="toc-backref" href="#toc-entry-5">Injecting Noise</a><a class="headerlink" href="#injecting-noise" title="Permalink to this headline">¶</a></h3>
<p>Noise is often introduced to the inputs as a dataset augmentation strategy. When we have a small dataset the network may effectively memorize the training dataset. Instead of learning a general mapping from inputs to outputs, the model may learn the specific input examples and their associated outputs. One approach for improving generalization error and improving the structure of the mapping problem is to add random noise.</p>
<p>Adding noise means that the network is less able to memorize training samples because they are changing all of the time, resulting in smaller network weights and a more robust network that has lower generalization error.</p>
<p>Noise is only added during training. No noise is added during the evaluation of the model or when the model is used to make predictions on new data.</p>
<p>Random noise can be added to other parts of the network during training. Some examples include:</p>
<p class="rubric">Noise Injection on Weights</p>
<ul class="simple">
<li>Noise added to weights can be interpreted as a more traditional form of regularization.</li>
<li>In other words, it pushes the model to be relatively insensitive to small variations in the weights, finding points that are not merely minima, but minima surrounded by flat regions.</li>
</ul>
<p class="rubric">Noise Injection on Outputs</p>
<ul class="simple">
<li>In the real world dataset, We can expect some amount of mistakes in the output labels.  One way to remedy this is to explicitly model the noise on labels.</li>
<li>An example for Noise Injection on Outputs is <strong>label smoothing</strong></li>
</ul>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><a class="reference external" href="http://wavelab.uwaterloo.ca/wp-content/uploads/2017/04/Lecture_3.pdf">Regularization</a></li>
</ul>
</div>
<div class="section" id="l1-regularization">
<h3><a class="toc-backref" href="#toc-entry-6">L1 Regularization</a><a class="headerlink" href="#l1-regularization" title="Permalink to this headline">¶</a></h3>
<p>A regression model that uses L1 regularization technique is called <em>Lasso Regression</em>.</p>
<p class="rubric">Mathematical formula for L1 Regularization.</p>
<p>Let’s define a model to see how L1 Regularization works. For simplicity, We define a simple linear regression model Y with one independent variable.</p>
<p>In this model, W represent Weight, b represent Bias.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}W = w_1, w_2 . . . w_n\\X = x_1, x_2 . . . x_n\end{aligned}\end{align} \]</div>
<p>and the predicted result is <span class="math notranslate nohighlight">\(\widehat{Y}\)</span></p>
<div class="math notranslate nohighlight">
\[\widehat{Y} =  w_1x_1 +  w_2x_2 + . . . w_nx_n + b\]</div>
<p>Following formula calculates the error without Regularization function</p>
<div class="math notranslate nohighlight">
\[Loss = Error(Y , \widehat{Y})\]</div>
<p>Following formula calculates the error With L1 Regularization function</p>
<div class="math notranslate nohighlight">
\[Loss = Error(Y - \widehat{Y}) + \lambda \sum_1^n |w_i|\]</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Here, If the value of lambda is Zero then above Loss function becomes Ordinary Least Square whereas very large value makes the coefficients (weights) zero hence it under-fits.</p>
</div>
<p>One thing to note is that <span class="math notranslate nohighlight">\(|w|\)</span> is differentiable when w!=0 as shown below,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\text{d}|w|}{\text{d}w} = \begin{cases}1 &amp; w &gt; 0\\-1 &amp; w &lt; 0\end{cases}\end{split}\]</div>
<p>To understand the Note above,</p>
<p>Let’s substitute the formula in finding new weights using Gradient Descent optimizer.</p>
<div class="math notranslate nohighlight">
\[w_{new} = w - \eta\frac{\partial L1}{\partial w}\]</div>
<p>When we apply the L1 in above formula it becomes,</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}w_{new} = w - \eta. (Error(Y , \widehat{Y}) + \lambda\frac{\text{d}|w|}{\text{d}w})\\\begin{split}        = \begin{cases}w - \eta . (Error(Y , \widehat{Y}) +\lambda) &amp; w &gt; 0\\w - \eta . (Error(Y , \widehat{Y}) -\lambda) &amp; w &lt; 0\end{cases}\end{split}\end{aligned}\end{align} \]</div>
<p>From the above formula,</p>
<ul class="simple">
<li>If w is positive, the regularization parameter <span class="math notranslate nohighlight">\(\lambda\)</span> &gt; 0 will push w to be less positive, by subtracting <span class="math notranslate nohighlight">\(\lambda\)</span> from w.</li>
<li>If w is negative, the regularization parameter <span class="math notranslate nohighlight">\(\lambda\)</span> &lt; 0 will push w to be less negative, by adding <span class="math notranslate nohighlight">\(\lambda\)</span> to w.  hence this has the effect of pushing w towards 0.</li>
</ul>
<p>Simple python implementation</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_weights_with_l1_regularization</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span><span class="k">lambda</span><span class="p">):</span>
     <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">     Features:(200, 3)</span>
<span class="sd">     Targets: (200, 1)</span>
<span class="sd">     Weights:(3, 1)</span>
<span class="sd">     &#39;&#39;&#39;</span>
     <span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

     <span class="c1">#Extract our features</span>
     <span class="n">x1</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
     <span class="n">x2</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
     <span class="n">x3</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>

     <span class="c1"># Use matrix cross product (*) to simultaneously</span>
     <span class="c1"># calculate the derivative for each weight</span>
     <span class="n">d_w1</span> <span class="o">=</span> <span class="o">-</span><span class="n">x1</span><span class="o">*</span><span class="p">(</span><span class="n">targets</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">)</span>
     <span class="n">d_w2</span> <span class="o">=</span> <span class="o">-</span><span class="n">x2</span><span class="o">*</span><span class="p">(</span><span class="n">targets</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">)</span>
     <span class="n">d_w3</span> <span class="o">=</span> <span class="o">-</span><span class="n">x3</span><span class="o">*</span><span class="p">(</span><span class="n">targets</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">)</span>

     <span class="c1"># Multiply the mean derivative by the learning rate</span>
     <span class="c1"># and subtract from our weights (remember gradient points in direction of steepest ASCENT)</span>

     <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d_w1</span><span class="p">)</span> <span class="o">-</span> <span class="k">lambda</span><span class="p">)</span> <span class="k">if</span> <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d_w1</span><span class="p">)</span> <span class="o">+</span> <span class="k">lambda</span><span class="p">)</span>
     <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d_w2</span><span class="p">)</span> <span class="o">-</span> <span class="k">lambda</span><span class="p">)</span> <span class="k">if</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d_w2</span><span class="p">)</span> <span class="o">+</span> <span class="k">lambda</span><span class="p">)</span>
     <span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d_w3</span><span class="p">)</span> <span class="o">-</span> <span class="k">lambda</span><span class="p">)</span> <span class="k">if</span> <span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d_w3</span><span class="p">)</span> <span class="o">+</span> <span class="k">lambda</span><span class="p">)</span>

     <span class="k">return</span> <span class="n">weights</span>
</pre></div>
</div>
<p class="rubric">Use Case</p>
<p>L1 Regularization (or varient of this concept) is a model of choice when the number of features are high, Since it provides sparse solutions. We can get computational advantage as the features with zero coefficients can simply be ignored.</p>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><a class="reference external" href="https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html">Linear Regression</a></li>
</ul>
</div>
<div class="section" id="l2-regularization">
<h3><a class="toc-backref" href="#toc-entry-7">L2 Regularization</a><a class="headerlink" href="#l2-regularization" title="Permalink to this headline">¶</a></h3>
<p>A regression model that uses L2 regularization technique is called <em>Ridge Regression</em>. Main difference between L1 and L2 regularization is, L2 regularization uses “squared magnitude” of coefficient as penalty term to the loss function.</p>
<p class="rubric">Mathematical formula for L2 Regularization.</p>
<p>Let’s define a model to see how L2 Regularization works. For simplicity, We define a simple linear regression model Y with one independent variable.</p>
<p>In this model, W represent Weight, b represent Bias.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}W = w_1, w_2 . . . w_n\\X = x_1, x_2 . . . x_n\end{aligned}\end{align} \]</div>
<p>and the predicted result is <span class="math notranslate nohighlight">\(\widehat{Y}\)</span></p>
<div class="math notranslate nohighlight">
\[\widehat{Y} =  w_1x_1 +  w_2x_2 + . . . w_nx_n + b\]</div>
<p>Following formula calculates the error without Regularization function</p>
<div class="math notranslate nohighlight">
\[Loss = Error(Y , \widehat{Y})\]</div>
<p>Following formula calculates the error With L2 Regularization function</p>
<div class="math notranslate nohighlight">
\[Loss = Error(Y - \widehat{Y}) +  \lambda \sum_1^n w_i^{2}\]</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Here, if lambda is zero then you can imagine we get back OLS. However, if lambda is very large then it will add too much weight and it leads to under-fitting.</p>
</div>
<p>To understand the Note above,</p>
<p>Let’s substitute the formula in finding new weights using Gradient Descent optimizer.</p>
<div class="math notranslate nohighlight">
\[w_{new} = w - \eta\frac{\partial L2}{\partial w}\]</div>
<p>When we apply the L2 in above formula it becomes,</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}w_{new} = w - \eta. (Error(Y , \widehat{Y}) + \lambda\frac{\partial L2}{\partial w})\\        = w - \eta . (Error(Y , \widehat{Y}) +2\lambda w)\end{aligned}\end{align} \]</div>
<p>Simple python implementation</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_weights_with_l2_regularization</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span><span class="k">lambda</span><span class="p">):</span>
     <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">     Features:(200, 3)</span>
<span class="sd">     Targets: (200, 1)</span>
<span class="sd">     Weights:(3, 1)</span>
<span class="sd">     &#39;&#39;&#39;</span>
     <span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

     <span class="c1">#Extract our features</span>
     <span class="n">x1</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
     <span class="n">x2</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
     <span class="n">x3</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>

     <span class="c1"># Use matrix cross product (*) to simultaneously</span>
     <span class="c1"># calculate the derivative for each weight</span>
     <span class="n">d_w1</span> <span class="o">=</span> <span class="o">-</span><span class="n">x1</span><span class="o">*</span><span class="p">(</span><span class="n">targets</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">)</span>
     <span class="n">d_w2</span> <span class="o">=</span> <span class="o">-</span><span class="n">x2</span><span class="o">*</span><span class="p">(</span><span class="n">targets</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">)</span>
     <span class="n">d_w3</span> <span class="o">=</span> <span class="o">-</span><span class="n">x3</span><span class="o">*</span><span class="p">(</span><span class="n">targets</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">)</span>

     <span class="c1"># Multiply the mean derivative by the learning rate</span>
     <span class="c1"># and subtract from our weights (remember gradient points in direction of steepest ASCENT)</span>

     <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d_w1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="k">lambda</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
     <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d_w2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="k">lambda</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
     <span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d_w3</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="k">lambda</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

     <span class="k">return</span> <span class="n">weights</span>
</pre></div>
</div>
<p class="rubric">Use Case</p>
<p>L2 regularization can address the multicollinearity problem by constraining the coefficient norm and keeping all the variables. L2 regression can be used to estimate the predictor importance and penalize predictors that are not important. One issue with co-linearity is that the variance of the parameter estimate is huge. In cases where the number of features are greater than the number of observations, the matrix used in the OLS may not be invertible but Ridge Regression enables this matrix to be inverted.</p>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Tikhonov_regularization">Ridge Regression</a></li>
</ul>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="http://www.deeplearningbook.org/contents/regularization.html">http://www.deeplearningbook.org/contents/regularization.html</a></td></tr>
</tbody>
</table>
</div>
</div>
<span id="document-architectures"></span><div class="section" id="architectures-1">
<span id="architectures"></span><h2>Architectures<a class="headerlink" href="#architectures-1" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#autoencoder" id="toc-entry-1">Autoencoder</a></li>
<li><a class="reference internal" href="#cnn" id="toc-entry-2">CNN</a></li>
<li><a class="reference internal" href="#gan" id="toc-entry-3">GAN</a></li>
<li><a class="reference internal" href="#mlp" id="toc-entry-4">MLP</a></li>
<li><a class="reference internal" href="#rnn" id="toc-entry-5">RNN</a></li>
<li><a class="reference internal" href="#vae" id="toc-entry-6">VAE</a></li>
</ul>
</div>
<div class="section" id="autoencoder">
<h3><a class="toc-backref" href="#toc-entry-1">Autoencoder</a><a class="headerlink" href="#autoencoder" title="Permalink to this headline">¶</a></h3>
<p>An autoencoder is a type of feedforward neural network that attempts to copy
its input to its output. Internally, it has a hidden layer, <strong>h</strong>, that
describes a <strong>code</strong>, used to represent the input. The network consists of
two parts:</p>
<ul class="simple">
<li>An <em>encoder</em> function: <span class="math notranslate nohighlight">\(h = f(x)\)</span>.</li>
<li>A <em>decoder</em> function, that produces a reconstruction: <span class="math notranslate nohighlight">\(r = g(h)\)</span>.</li>
</ul>
<p>The figure below shows the presented architecture.</p>
<div class="figure align-center" id="figure-1">
<a class="reference internal image-reference" href="_images/autoencoder_architecture.png"><img alt="_images/autoencoder_architecture.png" src="_images/autoencoder_architecture.png" style="width: 200px;" /></a>
<p class="caption"><span class="caption-text">Source <a class="footnote-reference" href="#autoenc" id="footnote-reference-1">[6]</a></span></p>
</div>
<p>The autoencoder compresses the input into a lower-dimensional code, and then
it reconstructs the output from this representation. The code is a compact
“summary”, or “compression”, of the input, and it is also called the
<em>latent-space
representation</em>.</p>
<p>If an autoencoder simply learned to set <span class="math notranslate nohighlight">\(g(f(x))=x\)</span> everywhere, then it would
not be very
useful; instead, autoencoders are designed to be unable to learn to copy
perfectly. They are restricted in ways that allow them to copy only
approximately, and to copy only input that resembles the training data.
Because the model is forced to prioritize which aspects of the input to copy,
it learns useful properties of the data.</p>
<p>In order to build an autoencoder, three things are needed: an encoding
method, a decoding method, and a loss function to compare the output with the
target.</p>
<p>Both the encoder and the decoder are fully-connected feedforward neural
networks. The code is a single layer of an artificial neural network, with
the dimensionality of our choice. The number of nodes in the code layer (the
<em>code size</em>) is a <em>hyperparameter</em> to be set before training the autoencoder.</p>
<p>The figure below shows the autoencoder architecture. First, the input passes
through the encoder, which is a fully-connected neural network, in order to
produce the code. The decoder, which has the similar neural network
structure, then produces the output by using the code only. The aim is to
get an output identical to the input.</p>
<div class="figure align-center" id="figure-2">
<a class="reference internal image-reference" href="_images/autoencoder_2.png"><img alt="_images/autoencoder_2.png" src="_images/autoencoder_2.png" style="width: 500px;" /></a>
<p class="caption"><span class="caption-text">Source <a class="footnote-reference" href="#a" id="footnote-reference-2">[5]</a></span></p>
</div>
<p>Traditionally, autoencoders were used for dimensionality reduction or feature
learning. More recently, theoretical connections between autoencoders and
latent variable models have brought autoencoders to the forefront of
generative modeling.
As a compression method, autoencoders do not perform better than their
alternatives. And the fact that autoencoders are data-specific makes them
impractical as a general technique.</p>
<p>In general, autoencoders have three common use cases:</p>
<ul class="simple">
<li><strong>Data denoising:</strong> It should be noted that denoising autoencoders are not
meant to automatically denoise an image, instead they were invented to help
the hidden layers of the autoencoder learn more robust filters, and reduce
the the risk of overfitting.</li>
<li><strong>Dimensionality reduction:</strong> Visualizing high-dimensional data is
challenging. t-SNE <a class="footnote-reference" href="#tsne" id="footnote-reference-3">[7]</a> is the most commonly used method, but struggles
with large number of dimensions (typically above 32).
Therefore, autoencoders can be used as a preprocessing step to reduce the
dimensionality, and this compressed representation is used by t-SNE to
visualize the data in 2D space.</li>
<li><strong>Variational Autoencoders (VAE):</strong> this is a more modern and complex
use-case of autoencoders. VAE learns the parameters of the probability
distribution modeling the input data, instead of learning an arbitrary
function in the case of vanilla autoencoders. By sampling points from this
distribution we can also use the VAE as a generative model <a class="footnote-reference" href="#vae-1" id="footnote-reference-4">[8]</a>.</li>
</ul>
<p class="rubric">Model</p>
<p>An example implementation in PyTorch.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Autoencoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_shape</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">c</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">w</span> <span class="o">=</span> <span class="n">in_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="o">*</span><span class="n">h</span><span class="o">*</span><span class="n">w</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">c</span><span class="o">*</span><span class="n">h</span><span class="o">*</span><span class="n">w</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">bs</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p class="rubric">Training</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><a class="reference external" href="https://pgaleone.eu/neural-networks/2016/11/24/convolutional-autoencoders/">Convolutional Autoencoders</a></li>
<li><a class="reference external" href="http://www.deeplearningbook.org/contents/autoencoders.html">Deep Learning Book</a></li>
</ul>
</div>
<div class="section" id="cnn">
<h3><a class="toc-backref" href="#toc-entry-2">CNN</a><a class="headerlink" href="#cnn" title="Permalink to this headline">¶</a></h3>
<p>The <em>convolutional neural network</em>, or <em>CNN</em>, is a feed-forward neural network
which has at least one convolutional layer. This type of deep neural network
is used for processing structured arrays of data. It is distinguished from other
neural networks by its superior performance with speech, audio, and
especially, image data. For the latter data type, CNNs are commonly employed
in computer vision tasks, like image classification, since they are
especially good at finding out patterns from the input images, such as lines,
circles, or more complex objects, e.g., human faces.</p>
<p>Convolutional neural networks comprise many convolutional layers, stacked one
on top of the other, in a sequence. The sequential architecture of CNNs
allows them to learn hierarchical features. Every layer can recognize shapes,
and the deeper the network goes, the more complex are the shapes which can be
recognized. The design of convolutional layers in a CNN reflects the
structure of the human visual cortex. In fact, our visual cortex is similarly
made of different layers, which process an image in our sight by sequentially identifying more and more complex features.</p>
<p>The CNN architecture is made up of three main distinct layers:</p>
<ol class="arabic simple">
<li>Convolutional layer</li>
<li>Pooling layer</li>
<li>Fully-connected (FC) layer</li>
</ol>
<div class="figure align-center" id="figure-3">
<a class="reference internal image-reference" href="_images/cnn.jpg"><img alt="_images/cnn.jpg" src="_images/cnn.jpg" style="width: 600px;" /></a>
<p class="caption"><span class="caption-text"><strong>Overview of CNN architecture.</strong> The architecture of CNNs follows this
structure, but with a greater number of layers for each layer’s type. The
convolutional and pooling layers are layers peculiar to CNNs, while the
fully-connected layer, activation function and output layer, are also
present in regular feed-forward neural networks. Source: [2]</span></p>
</div>
<p>When working with image data, the CNN architecture accepts as input a 3D
volume, or a 1D vector depending if the image data is in RGB format, for the
first case, or in grayscale format, for the latter. Then it transforms the input
through different equations, and it outputs a class. The convolutional layer
is the first layer of the convolutional neural network. While this first
layer can be followed by more convolutional layers, or pooling layers, the
fully-connected layer remains the last layer of the network, which outputs
the result. At every subsequent convolutional layer, the CNN increases its
complexity, and it can identify greater portions in the image. In the first
layers, the algorithm can recognize simpler features such as color or edges.
Deeper in the network, it becomes able to identify both larger objects in the
image and more complex ones. In the last layers, before the image reaches the
final FC layer, the CNN identifies the full object in the image.</p>
<p class="rubric">Model</p>
<p>An example implementation of a CNN in PyTorch.</p>
<p class="rubric">Training</p>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><a class="reference external" href="http://cs231n.github.io/convolutional-networks">CS231 Convolutional Networks</a></li>
<li><a class="reference external" href="http://www.deeplearningbook.org/contents/convnets.html">Deep Learning Book</a></li>
</ul>
</div>
<div class="section" id="gan">
<h3><a class="toc-backref" href="#toc-entry-3">GAN</a><a class="headerlink" href="#gan" title="Permalink to this headline">¶</a></h3>
<p>A Generative Adversarial Network (GAN) is a type of network which creates novel tensors (often images, voices, etc.). The generative portion of the architecture competes with the discriminator part of the architecture in a zero-sum game. The goal of the generative network is to create novel tensors which the adversarial network attempts to classify as real or fake. The goal of the generative network is generate tensors where the discriminator network determines that the tensor has a 50% chance of being fake and a 50% chance of being real.</p>
<p>Figure from [3].</p>
<img alt="_images/gan.png" class="align-center" src="_images/gan.png" />
<p class="rubric">Model</p>
<p>An example implementation in PyTorch.</p>
<p class="rubric">Generator</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">32</span> <span class="o">*</span> <span class="mi">8</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">32</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">32</span> <span class="o">*</span> <span class="mi">4</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">32</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        <span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tens</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">tens</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Discriminator</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Discriminator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">32</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">32</span> <span class="o">*</span> <span class="mi">4</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
            <span class="c1"># state size. (32*4) x 8 x 8</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">32</span> <span class="o">*</span> <span class="mi">8</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
            <span class="c1"># state size. (32*8) x 4 x 4</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tens</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">tens</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Training</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">netD</span><span class="p">,</span> <span class="n">netG</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">optimizerD</span><span class="p">,</span> <span class="n">optimizerG</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">netD</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">netG</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">netD</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">realtens</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">b_size</span> <span class="o">=</span> <span class="n">realtens</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">b_size</span><span class="p">,),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="c1"># gen labels</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">netD</span><span class="p">(</span><span class="n">realtens</span><span class="p">)</span>
            <span class="n">errD_real</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
            <span class="n">errD_real</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># backprop discriminator fake and real based on label</span>
            <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">b_size</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">fake</span> <span class="o">=</span> <span class="n">netG</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>
            <span class="n">label</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">netD</span><span class="p">(</span><span class="n">fake</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">errD_fake</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
            <span class="n">errD_fake</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># backprop discriminator fake and real based on label</span>
            <span class="n">errD</span> <span class="o">=</span> <span class="n">errD_real</span> <span class="o">+</span> <span class="n">errD_fake</span> <span class="c1"># discriminator error</span>
            <span class="n">optimizerD</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">netG</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">label</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  
            <span class="n">output</span> <span class="o">=</span> <span class="n">netD</span><span class="p">(</span><span class="n">fake</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">errG</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="c1"># generator error</span>
            <span class="n">errG</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizerG</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><a class="reference external" href="http://guertl.me/post/162759264070/generative-adversarial-networks">Generative Adversarial Networks</a></li>
<li><a class="reference external" href="http://www.deeplearningbook.org/contents/generative_models.html">Deep Learning Book</a></li>
<li><a class="reference external" href="https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html">PyTorch DCGAN Example</a></li>
<li><a class="reference external" href="https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf">Original Paper</a></li>
</ul>
</div>
<div class="section" id="mlp">
<h3><a class="toc-backref" href="#toc-entry-4">MLP</a><a class="headerlink" href="#mlp" title="Permalink to this headline">¶</a></h3>
<p>A Multi Layer Perceptron (MLP) is a neural network with only fully connected layers. Figure from [5].</p>
<img alt="_images/mlp.jpg" class="align-center" src="_images/mlp.jpg" />
<p class="rubric">Model</p>
<p>An example implementation on FMNIST dataset in PyTorch. <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet/blob/master/code/mlp.py">Full Code</a></p>
<ol class="arabic simple">
<li>The input to the network is a vector of size 28*28 i.e.(image from FashionMNIST dataset of dimension 28*28 pixels flattened to sigle dimension vector).</li>
<li>2 fully connected hidden layers.</li>
<li>Output layer with 10 outputs.(10 classes)</li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># define layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="c1"># fc1  make input 1 dimentional</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="c1"># fc2</span>
        <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="c1"># fc3</span>
        <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="c1"># output</span>
        <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">t</span>
</pre></div>
</div>
<p class="rubric">Training</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">n_batches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">loader</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
         <span class="c1"># print statistics</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training loss: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span><span class="p">(</span> <span class="n">running_loss</span><span class="p">))</span>
</pre></div>
</div>
<p class="rubric">Evaluating</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">train_set</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span>
        <span class="n">root</span> <span class="o">=</span> <span class="s1">&#39;./FMNIST&#39;</span><span class="p">,</span>
        <span class="n">train</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">download</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
            <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
        <span class="p">])</span>
    <span class="p">)</span>
    <span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">()</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">loss_func</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">15</span><span class="p">):</span>
        <span class="n">train</span><span class="p">(</span><span class="n">mlp</span><span class="p">,</span><span class="n">loader</span><span class="p">,</span><span class="n">loss_func</span><span class="p">,</span><span class="n">optimizer</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Finished Training&quot;</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&quot;./mlpmodel.pt&quot;</span><span class="p">)</span>
    <span class="n">test_set</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span>
        <span class="n">root</span> <span class="o">=</span> <span class="s1">&#39;./FMNIST&#39;</span><span class="p">,</span>
        <span class="n">train</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">download</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
            <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
        <span class="p">])</span>
    <span class="p">)</span>
    <span class="n">testloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">testloader</span><span class="p">:</span>
            <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy of the network on the 10000 test images: </span><span class="si">%d</span><span class="s1"> </span><span class="si">%%</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span>
        <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="p">))</span>
</pre></div>
</div>
<p class="rubric">Further reading</p>
<p>TODO</p>
</div>
<div class="section" id="rnn">
<h3><a class="toc-backref" href="#toc-entry-5">RNN</a><a class="headerlink" href="#rnn" title="Permalink to this headline">¶</a></h3>
<p>Description of RNN use case and basic architecture.</p>
<img alt="_images/rnn.png" class="align-center" src="_images/rnn.png" />
<p class="rubric">Model</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hid_fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">185</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">185</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">hidden</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">hid_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hid_fc</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_fc</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">hid_out</span>
</pre></div>
</div>
<p class="rubric">Training</p>
<p>In this example, our input is a list of last names, where each name is
a variable length array of one-hot encoded characters. Our target is is a list of
indices representing the class (language) of the name.</p>
<ol class="arabic simple">
<li>For each input name..</li>
<li>Initialize the hidden vector</li>
<li>Loop through the characters and predict the class</li>
<li>Pass the final character’s prediction to the loss function</li>
<li>Backprop and update the weights</li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)):</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">128</span><span class="p">))</span>
        <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">input_</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">char</span><span class="p">))</span>
            <span class="n">pred</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-.</span><span class="mi">001</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet/blob/master/notebooks/rnn.ipynb">Jupyter notebook</a></li>
<li><a class="reference external" href="http://www.deeplearningbook.org/contents/rnn.html">Deep Learning Book</a></li>
</ul>
</div>
<div class="section" id="vae">
<h3><a class="toc-backref" href="#toc-entry-6">VAE</a><a class="headerlink" href="#vae" title="Permalink to this headline">¶</a></h3>
<p>Autoencoders can encode an input image to a latent vector and decode it, but they can’t generate novel images.
Variational Autoencoders (VAE) solve this problem by adding a constraint: the latent vector representation should model a unit gaussian distribution.
The Encoder returns the mean and variance of the learned gaussian. To generate a new image, we pass a new mean and variance to the Decoder.
In other words, we “sample a latent vector” from the gaussian and pass it to the Decoder.
It also improves network generalization and avoids memorization. Figure from [4].</p>
<img alt="_images/vae.png" class="align-center" src="_images/vae.png" />
<p class="rubric">Loss Function</p>
<p>The VAE loss function combines reconstruction loss (e.g. Cross Entropy, MSE) with KL divergence.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">vae_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">):</span>
    <span class="n">recon_loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
    <span class="n">kl_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logvar</span><span class="p">)</span> <span class="o">+</span> <span class="n">mean</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">logvar</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">recon_loss</span> <span class="o">+</span> <span class="n">kl_loss</span>
</pre></div>
</div>
<p class="rubric">Model</p>
<p>An example implementation in PyTorch of a Convolutional Variational Autoencoder.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">VAE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_shape</span><span class="p">,</span> <span class="n">n_latent</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_shape</span> <span class="o">=</span> <span class="n">in_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_latent</span> <span class="o">=</span> <span class="n">n_latent</span>
        <span class="n">c</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">w</span> <span class="o">=</span> <span class="n">in_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span> <span class="o">=</span> <span class="n">h</span><span class="o">//</span><span class="mi">2</span><span class="o">**</span><span class="mi">2</span> <span class="c1"># receptive field downsampled 2 times</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">c</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># 32, 16, 16</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># 32, 8, 8</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_mean</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_latent</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_var</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_latent</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_develop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_latent</span><span class="p">,</span> <span class="mi">64</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">CenterCrop</span><span class="p">(</span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample_z</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
        <span class="n">stddev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">stddev</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">noise</span> <span class="o">*</span> <span class="n">stddev</span><span class="p">)</span> <span class="o">+</span> <span class="n">mean</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_var</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_develop</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">64</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_z</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span>
</pre></div>
</div>
<p class="rubric">Training</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="n">output</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">vae_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/abs/1312.6114">Original Paper</a></li>
<li><a class="reference external" href="http://kvfrans.com/variational-autoencoders-explained">VAE Explained</a></li>
<li><a class="reference external" href="http://www.deeplearningbook.org/contents/autoencoders.html">Deep Learning Book</a></li>
</ul>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694">https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="https://iq.opengenus.org/basics-of-machine-learning-image-classification-techniques/">https://iq.opengenus.org/basics-of-machine-learning-image-classification-techniques/</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><a class="reference external" href="http://guertl.me/post/162759264070/generative-adversarial-networks">http://guertl.me/post/162759264070/generative-adversarial-networks</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td><a class="reference external" href="http://kvfrans.com/variational-autoencoders-explained">http://kvfrans.com/variational-autoencoders-explained</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="a" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-2">[5]</a></td><td><a href="#system-message-1"><span class="problematic" id="problematic-1">`</span></a>Applied Deep Learning - Part 3: Autoencoders</td></tr>
</tbody>
</table>
<p>&lt;<a class="reference external" href="https://towardsdatascience">https://towardsdatascience</a>
.com/applied-deep-learning-part-3-autoencoders-1c083af4d798/&gt;`__</p>
<table class="docutils footnote" frame="void" id="autoenc" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-1">[6]</a></td><td><a href="#system-message-2"><span class="problematic" id="problematic-2">`</span></a>Deep Learning Book - Autoencoders &lt;<a class="reference external" href="https://www.deeplearningbook">https://www.deeplearningbook</a></td></tr>
</tbody>
</table>
<p>.org/contents/autoencoders.html/&gt;`__</p>
<table class="docutils footnote" frame="void" id="tsne" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-3">[7]</a></td><td><a class="reference external" href="https://distill.pub/2016/misread-tsne/">t-SNE</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="vae-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-4">[8]</a></td><td><a class="reference external" href="https://kvfrans.com/variational-autoencoders-explained/">VAE</a></td></tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-classification_algos"></span><div class="section" id="classification-algorithms">
<span id="classification-algos"></span><h2>Classification Algorithms<a class="headerlink" href="#classification-algorithms" title="Permalink to this headline">¶</a></h2>
<p>Classification problems is when our output Y is always in categories like positive vs negative in terms of sentiment analysis, dog vs cat in terms of image classification and disease vs no disease in terms of medical diagnosis.</p>
<div class="section" id="bayesian">
<h3>Bayesian<a class="headerlink" href="#bayesian" title="Permalink to this headline">¶</a></h3>
<p>Overlaps..</p>
</div>
<div class="section" id="decision-trees">
<h3>Decision Trees<a class="headerlink" href="#decision-trees" title="Permalink to this headline">¶</a></h3>
<p class="rubric">Intuitions</p>
<p>Decision tree works by successively splitting the dataset into small segments until the target variable are the same or until the dataset can no longer be split. It’s a greedy algorithm which make the best decision at the given time without concern for the global optimality <a class="footnote-reference" href="#mlinaction" id="footnote-reference-1">[2]</a>.</p>
<p>The concept behind decision tree is straightforward. The following flowchart show a simple email classification system based on decision tree. If the address is “myEmployer.com”, it will classify it to “Email to read when bored”. Then if the email contains the word “hockey”, this email will be classified as “Email from friends”. Otherwise, it will be identified as “Spam: don’t read”. Image source <a class="footnote-reference" href="#mlinaction" id="footnote-reference-2">[2]</a>.</p>
<a class="reference internal image-reference" href="_images/decision_tree.png"><img alt="_images/decision_tree.png" class="align-center" src="_images/decision_tree.png" style="width: 358.8px; height: 286.2px;" /></a>
<p class="rubric">Algorithm Explained</p>
<p>There are various kinds of decision tree algorithms such as ID3 (Iterative Dichotomiser 3), C4.5 and CART (Classification and Regression Trees). The constructions of decision tree are similar <a class="footnote-reference" href="#decisiontrees" id="footnote-reference-3">[6]</a>:</p>
<ol class="arabic simple">
<li>Assign all training instances to the root of the tree. Set current node to root node.</li>
<li>Find the split feature and split value based on the split criterion such as information gain, information gain ratio or gini coefficient.</li>
<li>Partition all data instances at the node based on the split feature and threshold value.</li>
<li>Denote each partition as a child node of the current node.</li>
<li><dl class="first docutils">
<dt>For each child node:</dt>
<dd><ol class="first last arabic">
<li>If the child node is “pure” (has instances from only one class), tag it as a leaf and return.</li>
<li>Else, set the child node as the current node and recurse to step 2.</li>
</ol>
</dd>
</dl>
</li>
</ol>
<p>ID3 creates a multiway tree. For each node, it trys to find the categorical feature that will yield the largest information gain for the target variable.</p>
<p>C4.5 is the successor of ID3 and remove the restriction that the feature must be categorical by dynamically define a discrete attribute that partitions the continuous attribute in the discrete set of intervals.</p>
<p>CART is similar to C4.5. But it differs in that it constructs binary tree and support regression problem <a class="footnote-reference" href="#sklearntree" id="footnote-reference-4">[3]</a>.</p>
<p>The main differences are shown in the following table:</p>
<table border="1" class="docutils">
<colgroup>
<col width="14%" />
<col width="15%" />
<col width="39%" />
<col width="33%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Dimensions</td>
<td>ID3</td>
<td>C4.5</td>
<td>CART</td>
</tr>
<tr class="row-even"><td>Split Criterion</td>
<td>Information gain</td>
<td>Information gain ratio (Normalized information gain)</td>
<td>Gini coefficient for classification problems</td>
</tr>
<tr class="row-odd"><td>Types of Features</td>
<td>Categorical feature</td>
<td>Categorical &amp; numerical features</td>
<td>Categorical &amp; numerical features</td>
</tr>
<tr class="row-even"><td>Type of Problem</td>
<td>Classification</td>
<td>Classification</td>
<td>Classification &amp; regression</td>
</tr>
<tr class="row-odd"><td>Type of Tree</td>
<td>Mltiway tree</td>
<td>Mltiway tree</td>
<td>Binary tree</td>
</tr>
</tbody>
</table>
<p class="rubric">Code Implementation</p>
<p>We used object-oriented patterns to create the code for <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet/blob/master/code/decision_tree.py#L87">ID3</a>, <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet/blob/master/code/decision_tree.py#L144">C4.5</a> and <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet/blob/master/code/decision_tree.py#L165">CART</a>. We will first introduce the base class for these three algorithms, then we explain the code of CART in details.</p>
<p>First, we create the base class <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet/blob/master/code/decision_tree.py#L7">TreeNode class</a> and  <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet/blob/master/code/decision_tree.py#L24">DecisionTree</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TreeNode</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_idx</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">child_lst</span><span class="o">=</span><span class="p">[]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_idx</span> <span class="o">=</span> <span class="n">data_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">child</span> <span class="o">=</span> <span class="n">child_lst</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">split_col</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">child_cate_order</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">set_attribute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split_col</span><span class="p">,</span> <span class="n">child_cate_order</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">split_col</span> <span class="o">=</span> <span class="n">split_col</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">child_cate_order</span> <span class="o">=</span> <span class="n">child_cate_order</span>

    <span class="k">def</span> <span class="nf">set_label</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">label</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DecisionTree</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        X: train data, dimensition [num_sample, num_feature]</span>
<span class="sd">        y: label, dimension [num_sample, ]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">y</span>
        <span class="n">num_sample</span><span class="p">,</span> <span class="n">num_feature</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_num</span> <span class="o">=</span> <span class="n">num_feature</span>
        <span class="n">data_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_sample</span><span class="p">))</span>
        <span class="c1"># Set the root of the tree</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">root</span> <span class="o">=</span> <span class="n">TreeNode</span><span class="p">(</span><span class="n">data_idx</span><span class="o">=</span><span class="n">data_idx</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">child_lst</span><span class="o">=</span><span class="p">[])</span>
        <span class="n">queue</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">root</span><span class="p">]</span>
        <span class="k">while</span> <span class="n">queue</span><span class="p">:</span>
            <span class="n">node</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># Check if the terminate criterion has been met</span>
            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">depth</span><span class="o">&gt;</span><span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">data_idx</span><span class="p">)</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Set the label for the leaf node</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Split the node</span>
                <span class="n">child_nodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_node</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">child_nodes</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">queue</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">child_nodes</span><span class="p">)</span>
</pre></div>
</div>
<p>The CART algorithm, when constructing the binary tree, will try searching for the feature and threshold that will yield the largest gain or the least impurity. The split criterion is a combination of the child nodes’ impurity. For the child nodes’ impurity, gini coefficient or information gain are adopted in classification. For regression problem, mean-square-error or mean-absolute-error are used. Example codes are showed below. For more details about the formulas, please refer to <a class="reference external" href="https://scikit-learn.org/stable/modules/tree.html#mathematical-formulation">Mathematical formulation for decision tree in scikit-learn documentation</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CART</span><span class="p">(</span><span class="n">DecisionTree</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">get_split_criterion</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">child_node_lst</span><span class="p">):</span>
        <span class="n">total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">data_idx</span><span class="p">)</span>
        <span class="n">split_criterion</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">child_node</span> <span class="ow">in</span> <span class="n">child_node_lst</span><span class="p">:</span>
            <span class="n">impurity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_impurity</span><span class="p">(</span><span class="n">child_node</span><span class="o">.</span><span class="n">data_idx</span><span class="p">)</span>
            <span class="n">split_criterion</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">child_node</span><span class="o">.</span><span class="n">data_idx</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">total</span><span class="p">)</span> <span class="o">*</span> <span class="n">impurity</span>
        <span class="k">return</span> <span class="n">split_criterion</span>

    <span class="k">def</span> <span class="nf">get_impurity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_ids</span><span class="p">):</span>
        <span class="n">target_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">data_ids</span><span class="p">]</span>
        <span class="n">total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_y</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree_type</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">mean_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">target_y</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">target_y</span><span class="p">:</span>
                <span class="n">res</span> <span class="o">+=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">mean_y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">total</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree_type</span> <span class="o">==</span> <span class="s2">&quot;classification&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_criterion</span> <span class="o">==</span> <span class="s2">&quot;gini&quot;</span><span class="p">:</span>
                <span class="n">res</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="n">unique_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">target_y</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">unique_y</span><span class="p">:</span>
                    <span class="n">num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">target_y</span><span class="o">==</span><span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
                    <span class="n">res</span> <span class="o">-=</span> <span class="p">(</span><span class="n">num</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">total</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_criterion</span> <span class="o">==</span> <span class="s2">&quot;entropy&quot;</span><span class="p">:</span>
                <span class="n">unique</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">target_y</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">count</span><span class="p">:</span>
                    <span class="n">p</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">/</span> <span class="n">total</span>
                    <span class="n">res</span> <span class="o">-=</span> <span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span>
</pre></div>
</div>
</div>
<div class="section" id="k-nearest-neighbor">
<h3>K-Nearest Neighbor<a class="headerlink" href="#k-nearest-neighbor" title="Permalink to this headline">¶</a></h3>
<p class="rubric">Introduction</p>
<p>K-Nearest Neighbor is a supervised learning algorithm both for classification and regression. The principle is to find the predefined number of training samples closest to the new point, and predict the label from these training samples <a class="footnote-reference" href="#sklearnknn" id="footnote-reference-5">[1]</a>.</p>
<p>For example, when a new point comes, the algorithm will follow these steps:</p>
<ol class="arabic simple">
<li>Calculate the Euclidean distance between the new point and all training data</li>
<li>Pick the top-K closest training data</li>
<li>For regression problem, take the average of the labels as the result; for classification problem, take the most common label of these labels as the result.</li>
</ol>
<p class="rubric">Code</p>
<p>Below is the Numpy implementation of K-Nearest Neighbor function. Refer to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet/blob/master/code/knn.py">code example</a> for details.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">KNN</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    training_data: all training data point</span>
<span class="sd">    target: new point</span>
<span class="sd">    k: user-defined constant, number of closest training data</span>
<span class="sd">    func: functions used to get the the target label</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Step one: calculate the Euclidean distance between the new point and all training data</span>
    <span class="n">neighbors</span><span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">training_data</span><span class="p">):</span>
        <span class="c1"># distance between the target data and the current example from the data.</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="n">euclidean_distance</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">neighbors</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">distance</span><span class="p">,</span> <span class="n">index</span><span class="p">))</span>

    <span class="c1"># Step two: pick the top-K closest training data</span>
    <span class="n">sorted_neighbors</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">neighbors</span><span class="p">)</span>
    <span class="n">k_nearest</span> <span class="o">=</span> <span class="n">sorted_neighbors</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span>
    <span class="n">k_nearest_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">training_data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">distance</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">k_nearest</span><span class="p">]</span>

    <span class="c1"># Step three: For regression problem, take the average of the labels as the result;</span>
    <span class="c1">#             for classification problem, take the most common label of these labels as the result.</span>
    <span class="k">return</span> <span class="n">k_nearest</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">k_nearest_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="logistic-regression">
<h3>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h3>
<p>please refer to  <a class="reference internal" href="index.html#logistic-regression"><span class="std std-ref">logistic regresion</span></a></p>
</div>
<div class="section" id="random-forests">
<h3>Random Forests<a class="headerlink" href="#random-forests" title="Permalink to this headline">¶</a></h3>
<p>Random Forest Classifier using ID3 Tree: <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet/blob/master/code/random_forest_classifier.py">code example</a></p>
</div>
<div class="section" id="boosting">
<h3>Boosting<a class="headerlink" href="#boosting" title="Permalink to this headline">¶</a></h3>
<p>Boosting is a powerful approach to increase the predictive power
of classification and regression models. However, the algorithm itself can not
predict anything. It is built above other (weak) models to boost their accuracy.
In this section we will explain it w.r.t. a classification problem.</p>
<p>In order to gain an understanding about this topic, we will go briefly over ensembles and
learning with weighted instances.</p>
<p class="rubric">Excurse:</p>
<ol class="arabic">
<li><p class="first"><strong>Ensembles</strong></p>
<blockquote>
<div><p>Boosting belongs to the ensemble family which contains other techniques like
bagging (e.i. Random Forest classifier) and Stacking (refer to <a class="reference external" href="http://rasbt.github.io/mlxtend/">mlxtend Documentations</a>).
The idea of ensembles is to use the wisdom of the crowd:</p>
<ul class="simple">
<li>a single classifier will not know everything.</li>
<li>multiple classifiers will know a lot.</li>
</ul>
<p>One example that uses the wisdom of the crowd is Wikipedia.</p>
<p>The prerequisites for this technique are:</p>
<blockquote>
<div><ul class="simple">
<li>different classifiers have different knowledge.</li>
<li>different classifiers make different mistake.</li>
</ul>
</div></blockquote>
<p>we can fulfill the first prerequisite by using different  datasets that are collected
form different resources and in different times. In practice, this is most of the time impossible.
Normally, we have only one dataset. We can go around this by using cross validation (See Figure below) and
use one fold to train a classifier at a time.
The second prerequisite means that the classifiers may make different mistakes. Since we trained our
classifiers on different datasets or using cross-validation, this condition is already fulfilled.</p>
<div class="figure align-center" id="figure-1">
<a class="reference internal image-reference" href="_images/grid_search_cross_validation.png"><img alt="_images/grid_search_cross_validation.png" src="_images/grid_search_cross_validation.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-text">Using cross-validation with ensembles.</span></p>
</div>
<p>Now, we have multiple classifiers, we need a way to combine their results. This actually
the reason we have multiple ensemble techniques, they are all based on the same concept. They may differ
in some aspects, like whether to use weighted instances or not and how they combine the results for the
different classifiers. In general, for classification we use voting and for regression we average the results
of the classifiers. There are a lot of variations for voting and average methods, like weighted average.
Some will go further and use the classifications or the results from all of the classifier(aka. base-classifiers)
as features for an extra classifier (aka. meta classifier) to  predict the final result.</p>
</div></blockquote>
</li>
<li><p class="first"><strong>learning with weighted instances</strong></p>
<blockquote>
<div><p>For classification algorithms such as KNN, we give the same weight to all instances,
which means they are equally important. In practice, instances contribute differently,
e.i., sensors that collect information have different quality and some are more
reliable than others. We want to encode this in our algorithms by assigning weights to different
instances and this can be done as follows:</p>
<ul class="simple">
<li>changing the classification algorithm (expensive)</li>
<li>duplicate instances such that an instance with wight n is duplicated n times</li>
</ul>
</div></blockquote>
</li>
</ol>
<p>Coming back to the actual topic, we can implement boosting, if we train a set of classifiers (not parallel, as
the case with Random forest) one after another. The first classifier is a created in a normal way. the  latter
classifiers have to focus on the misclassified examples by previous ones. How we can achieve this? Well, we can assign
weights to instances (learning with weighted instances). If a classifier misclassified an example, we assign higher
weight to this example to get more focus from the next classifier(s). Correct examples stay un-touched. It was important
to highlight that boosting is an ensemble technique, at the same time, something about boosting might be somehow
confusing, in boosting we break the rule of using different datasets, since we want to focus on misclassified examples
from previous models, we need to us all data we have to train all models. In this way, a misclassified instance from
the first model, will be hopefully classified correctly from the second or the subsequent ones.</p>
<div class="figure align-center" id="figure-2">
<a class="reference internal image-reference" href="_images/boosting_error_iteration.png"><img alt="_images/boosting_error_iteration.png" src="_images/boosting_error_iteration.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-text">Error decreases with an increasing number of classifiers.</span></p>
</div>
<p>An implementation of the Adaboost (one of the boosting algorithms) from scratch can
be found here (<a class="reference external" href="https://python-course.eu/machine-learning/boosting-algorithm-in-python.php/">python-course.eu</a>) with more details about the algorithm</p>
</div>
<div class="section" id="support-vector-machine">
<h3>Support Vector Machine<a class="headerlink" href="#support-vector-machine" title="Permalink to this headline">¶</a></h3>
<p><em>Support Vector Machine</em>, or <em>SVM</em>, is one of the most popular supervised
learning algorithms, and it can be used both for classification as well as
regression problems. However, in machine learning, it is primarily used for
classification problems.
In the SVM algorithm, each data item is plotted as a point in <em>n-dimensional</em>
space, where <em>n</em> is the number of features we have at hand, and the value of
each feature is the value of a particular coordinate.</p>
<p>The goal of the SVM algorithm is to create the best line, or decision
boundary, that can segregate the n-dimensional space into distinct classes, so
that we can easily put any new data point in the correct category, in the
future. This best decision boundary is called a hyperplane.
The best separation is achieved by the hyperplane that has the largest
distance to the nearest training-data point of any class. Indeed, there are
many hyperplanes that might classify the data. Aas reasonable choice for the
best hyperplane is the one that represents the largest separation, or margin,
between the two classes.</p>
<p>The SVM algorithm chooses the extreme points that help in creating the
hyperplane. These extreme cases are called support vectors, while the SVM
classifier is the frontier, or hyperplane, that best segregates the distinct
classes.</p>
<p>The diagram below shows two distinct classes, denoted respectively with blue
and green points. The <em>maximum-margin hyperplane</em> is the distance between
the two parallel hyperplanes: <em>positive hyperplane</em> and <em>negative hyperplane</em>,
shown by dashed lines. The maximum-margin hyperplane is chosen in a way that
the distance between the two classes is maximised.</p>
<div class="figure align-center" id="figure-3">
<a class="reference internal image-reference" href="_images/svm.png"><img alt="_images/svm.png" src="_images/svm.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-text"><strong>Support Vector Machine:</strong> Two different categories classified
using a decision boundary, or hyperplane. Source <a class="footnote-reference" href="#svm" id="footnote-reference-6">[7]</a></span></p>
</div>
<p>Support Vector Machine can be of two types:</p>
<ul class="simple">
<li><strong>Linear SVM:</strong> A linear SVM is used for linearly separable data, which is
the case of a dataset that can be classified into two distinct classes by
using a single straight line.</li>
<li><strong>Non-linear SVM:</strong> A non-linear SVM is used for non-linearly separated data,
which means that a dataset cannot be classified by using a straight line.</li>
</ul>
<p class="rubric">Linear SVM</p>
<p>Let’s suppose we have a dataset that has two classes, stars and circles. The
dataset has two features, <em>x1</em> and <em>x2</em>. We want a classifier that can
classify the pair (<em>x1</em>, <em>x2</em>) of coordinates in either stars or circles.
Consider the figure below.</p>
<div class="figure align-center" id="figure-4">
<a class="reference internal image-reference" href="_images/svm_linear.png"><img alt="_images/svm_linear.png" src="_images/svm_linear.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-text">Source <a class="footnote-reference" href="#svm2" id="footnote-reference-7">[8]</a></span></p>
</div>
<p>Since it is a <em>2-dimensional</em> space, we can separate these two classes by
using a straight line. The figure shows that we have three hyperplanes, A,
B, and C, which are all segregating the classes well. How can we identify the
right hyperplane?
The SVM algorithm finds the closest point of the lines from both of the
classes. These points are called support vectors.
The distance between the support vectors and the hyperplane is referred as the
<em>margin</em>. The goal of SVM is to maximize this margin. The hyperplane with
maximum margin is called the optimal hyperplane.
From the figure above, we see that the margin for hyperplane C is higher
when compared to both A and B. Therefore, we name C as the (right)
hyperplane.</p>
<p class="rubric">Non-linear SVM</p>
<p>When the data is linearly arranged, we can separate it by using a straight
line. However, for non-linear data, we cannot draw a single straight line.
Let’s consider the figure below.</p>
<div class="figure align-center" id="figure-5">
<a class="reference internal image-reference" href="_images/svm_nonlinear_1.png"><img alt="_images/svm_nonlinear_1.png" src="_images/svm_nonlinear_1.png" style="width: 300px;" /></a>
<p class="caption"><span class="caption-text">Source <a class="footnote-reference" href="#svm2" id="footnote-reference-8">[8]</a></span></p>
</div>
<p>In order to separate the circles from the stars, we need to
introduce an additional feature. In case of linear data, we would use
the
two features <em>x</em> and <em>y</em>. For this non-linear data, we will add a third
dimension, <em>z</em>. <em>z</em> is defined as <span class="math notranslate nohighlight">\(z=x^2+y^2\)</span>. By adding the third
feature, our space will become as below image.</p>
<div class="figure align-center" id="figure-6">
<a class="reference internal image-reference" href="_images/svm_nonlinear_2.png"><img alt="_images/svm_nonlinear_2.png" src="_images/svm_nonlinear_2.png" style="width: 300px;" /></a>
<p class="caption"><span class="caption-text">Source <a class="footnote-reference" href="#svm2" id="footnote-reference-9">[8]</a></span></p>
</div>
<p>In the above figure, all values for z will always be positive, because <em>z</em>
is the squared sum of <em>x</em> and <em>y</em>. Now, the SVM classifier will divide the
dataset into two distinct classes by finding a <em>linear</em> hyperplane between
these two classes.</p>
<p>Since now we are in a <em>3-dimensional</em> space, the hyperplane looks like a plane
parallel to the x-axis. If we convert it in <em>2-dimensional</em> space with
<span class="math notranslate nohighlight">\(z=1\)</span>, then it will become as the figure below.</p>
<div class="figure align-center" id="figure-7">
<a class="reference internal image-reference" href="_images/svm_nonlinear_3.png"><img alt="_images/svm_nonlinear_3.png" src="_images/svm_nonlinear_3.png" style="width: 300px;" /></a>
<p class="caption"><span class="caption-text">Source <a class="footnote-reference" href="#svm2" id="footnote-reference-10">[8]</a></span></p>
</div>
<p>(Hence, in case of non-linear data, we obtain a circumference of
<span class="math notranslate nohighlight">\(radius=1\)</span>)</p>
<p>In order to find the hyperplane with the SVM algorithm, we do not need to add
this third dimension <em>z</em> manually: the SVM algorithm uses a technique called
the “kernel trick”. The SVM kernel is a function which takes a low
dimensional input, and it transforms it to a higher dimensional space, i.e.,
it converts non-linearly separable data to linearly separable data.</p>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="sklearnknn" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-5">[1]</a></td><td><a class="reference external" href="https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification">https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="mlinaction" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><em>(<a class="fn-backref" href="#footnote-reference-1">1</a>, <a class="fn-backref" href="#footnote-reference-2">2</a>)</em> <a class="reference external" href="https://www.manning.com/books/machine-learning-in-action">Machine Learning in Action by Peter Harrington</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="sklearntree" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-4">[3]</a></td><td><a class="reference external" href="https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart">Scikit-learn Documentations: Tree algorithms: ID3, C4.5, C5.0 and CART</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="sklearnensemble" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td><a class="reference external" href="https://scikit-learn.org/stable/modules/ensemble.html#">Scikit-learn Documentations: Ensemble Method</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="boostingiteration" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[5]</td><td><a class="reference external" href="https://medium.com/analytics-vidhya/what-is-gradient-boosting-how-is-it-different-from-ada-boost-2d5ff5767cb2#">Medium-article: what is Gradient Boosting</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="decisiontrees" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-3">[6]</a></td><td><a class="reference external" href="https://www.cs.cmu.edu/~bhiksha/courses/10-601/decisiontrees/">Decision Trees</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="svm" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-6">[7]</a></td><td><a class="reference external" href="https://www.javatpoint.com/machine-learning-support-vector-machine-algorithm">Support Vector Machine</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="svm2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[8]</td><td><em>(<a class="fn-backref" href="#footnote-reference-7">1</a>, <a class="fn-backref" href="#footnote-reference-8">2</a>, <a class="fn-backref" href="#footnote-reference-9">3</a>, <a class="fn-backref" href="#footnote-reference-10">4</a>)</em> <a class="reference external" href="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/">Support Vector Machine</a></td></tr>
</tbody>
</table>
</div>
</div>
<span id="document-clustering_algos"></span><div class="section" id="clustering-algorithms">
<span id="clustering-algos"></span><h2>Clustering Algorithms<a class="headerlink" href="#clustering-algorithms" title="Permalink to this headline">¶</a></h2>
<div class="section" id="centroid">
<h3>Centroid<a class="headerlink" href="#centroid" title="Permalink to this headline">¶</a></h3>
<p>Be the first to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet">contribute!</a></p>
</div>
<div class="section" id="density">
<h3>Density<a class="headerlink" href="#density" title="Permalink to this headline">¶</a></h3>
<p>Be the first to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet">contribute!</a></p>
</div>
<div class="section" id="distribution">
<h3>Distribution<a class="headerlink" href="#distribution" title="Permalink to this headline">¶</a></h3>
<p>Be the first to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet">contribute!</a></p>
</div>
<div class="section" id="hierarchical">
<h3>Hierarchical<a class="headerlink" href="#hierarchical" title="Permalink to this headline">¶</a></h3>
<p>Be the first to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet">contribute!</a></p>
</div>
<div class="section" id="k-means">
<h3>K-Means<a class="headerlink" href="#k-means" title="Permalink to this headline">¶</a></h3>
<p>Be the first to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet">contribute!</a></p>
</div>
<div class="section" id="mean-shift">
<h3>Mean shift<a class="headerlink" href="#mean-shift" title="Permalink to this headline">¶</a></h3>
<p>Be the first to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet">contribute!</a></p>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="https://en.wikipedia.org/wiki/Cluster_analysis">https://en.wikipedia.org/wiki/Cluster_analysis</a></td></tr>
</tbody>
</table>
</div>
</div>
<span id="document-regression_algos"></span><div class="section" id="regression-algorithms">
<span id="regression-algos"></span><h2>Regression Algorithms<a class="headerlink" href="#regression-algorithms" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#ordinary-least-squares" id="toc-entry-1">Ordinary Least Squares</a></li>
<li><a class="reference internal" href="#polynomial" id="toc-entry-2">Polynomial</a></li>
<li><a class="reference internal" href="#lasso" id="toc-entry-3">Lasso</a></li>
<li><a class="reference internal" href="#ridge" id="toc-entry-4">Ridge</a></li>
<li><a class="reference internal" href="#stepwise" id="toc-entry-5">Stepwise</a></li>
</ul>
</div>
<div class="section" id="ordinary-least-squares">
<h3><a class="toc-backref" href="#toc-entry-1">Ordinary Least Squares</a><a class="headerlink" href="#ordinary-least-squares" title="Permalink to this headline">¶</a></h3>
<p>OLS is the method with which linear regression is performed. The square of the difference from the mean is taken for every data point, and the summed loss function is to be minimized.</p>
<div class="math notranslate nohighlight">
\[l = \sum_{i=1}^n (y_i - \bar{y})^2\]</div>
</div>
<div class="section" id="polynomial">
<h3><a class="toc-backref" href="#toc-entry-2">Polynomial</a><a class="headerlink" href="#polynomial" title="Permalink to this headline">¶</a></h3>
<p>Polynomial regression is a modification of linear regression where the existing features are mapped to a polynomial form. The problem is still a linear regression problem, but the input vector is now mapped to a higher dimensional vector which serves as a pseudo-input vector of sorts.</p>
<div class="math notranslate nohighlight">
\[\textbf{x} = (x_0, x_1) \rightarrow \textbf{x'} = (x_0, x^2_0, x_1, x^2_1, x_0x_1)\]</div>
</div>
<div class="section" id="lasso">
<h3><a class="toc-backref" href="#toc-entry-3">Lasso</a><a class="headerlink" href="#lasso" title="Permalink to this headline">¶</a></h3>
<p>Lasso Regression tries to reduce the ordinary least squares error similar to vanilla regression, but adds an extra term. The sum of the <span class="math notranslate nohighlight">\(L_1\)</span> norm for every data point multiplied by a hyperparameter <span class="math notranslate nohighlight">\(\alpha\)</span> is used. This reduces model complexity and prevents overfitting.</p>
<div class="math notranslate nohighlight">
\[l = \sum_{i=1}^n (y_i - \tilde{y})^2 + \alpha \sum_{j=1}^p |w_j|\]</div>
</div>
<div class="section" id="ridge">
<h3><a class="toc-backref" href="#toc-entry-4">Ridge</a><a class="headerlink" href="#ridge" title="Permalink to this headline">¶</a></h3>
<p>Ridge regression is similar to lasso regression, but the regularization term uses the <span class="math notranslate nohighlight">\(L_2\)</span> norm instead.</p>
<div class="math notranslate nohighlight">
\[l = \sum_{i=1}^n (y_i - \tilde{y})^2 + \alpha \sum_{j=1}^p w^2_j\]</div>
</div>
<div class="section" id="stepwise">
<h3><a class="toc-backref" href="#toc-entry-5">Stepwise</a><a class="headerlink" href="#stepwise" title="Permalink to this headline">¶</a></h3>
<p>Stepwise regression or spline regression helps us fit a piece wise function to the data. It is usually used with linear models, but it can be generalized to higher degrees as well. The regression equation takes the form of</p>
<div class="math notranslate nohighlight">
\[y = ax + b(x-\bar{x})H_{\alpha}+c\]</div>
<p>where <span class="math notranslate nohighlight">\(H_{\alpha}\)</span> is the shifted Heaviside step function, having its discontinuity at <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/">https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/">http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/</a></td></tr>
</tbody>
</table>
</div>
</div>
<span id="document-reinforcement_learning"></span><div class="section" id="reinforcement-learning-1">
<span id="reinforcement-learning"></span><h2>Reinforcement Learning<a class="headerlink" href="#reinforcement-learning-1" title="Permalink to this headline">¶</a></h2>
<p>In machine learning, supervised is sometimes contrasted with unsupervised learning. This is a useful distinction, but there are some problem domains that have share characteristics with each without fitting exactly in either category. In cases where the algorithm does not have explicit labels but does receive a form of feedback, we are dealing with a third and distinct paradigm of machine learning - reinforcement learning.</p>
<p>Programmatic and a theoretical introduction to reinforcement learning:https://spinningup.openai.com/</p>
<p>There are different problem types and algorithms, but all reinforcement learning problems have the following aspects in common:</p>
<blockquote>
<div><ul class="simple">
<li>an <strong>agent</strong> - the algorithm or “AI” responsible for making decisions</li>
<li>an <strong>environment</strong>, consisting of different <strong>states</strong> in which the agent may find itself</li>
<li>a <strong>reward</strong> signal which is returned by the environment as a function of the current state</li>
<li><strong>actions</strong>, each of which takes the agent from one state to another</li>
<li>a <strong>policy</strong>, i.e. a mapping from states to actions that defines the agent’s behavior</li>
</ul>
</div></blockquote>
<p>The goal of reinforcement learning is to learn the optimal policy, that is the policy that maximizes expected (discounted) cumulative reward.</p>
<p>Many RL algorithms will include a value function or a Q-function. A value function gives the expected cumulative reward for each state under the current policy In other words, it answers the question, “If I begin in state <span class="math notranslate nohighlight">\(i\)</span> and follow my policy, what will be my expected reward?”</p>
<p>In most algorithms, expected cumulative reward is discounted by some factor <span class="math notranslate nohighlight">\(\gamma \in (0, 1)\)</span>; a typical value for <span class="math notranslate nohighlight">\(\gamma\)</span> is 0.9. In addition to more accurately modeling the behavior of humans and other animals, <span class="math notranslate nohighlight">\(\gamma &lt; 1\)</span> helps to ensure that algorithms converge even when there is no terminal state or when the terminal state is never found (because otherwise expected cumulative reward may also become infinite).</p>
<div class="section" id="note-on-terminology">
<h3>Note on Terminology<a class="headerlink" href="#note-on-terminology" title="Permalink to this headline">¶</a></h3>
<p>For mostly historical reasons, engineering and operations research use different words to talk about the same concepts. For example, the general field of reinforcement learning itself is sometimes referred to as optimal control, approximate dynamic programming, or neuro-dynamic programming.<sup>1</sup></p>
</div>
<div class="section" id="eploration-vs-exploitation">
<h3>Eploration vs. Exploitation<a class="headerlink" href="#eploration-vs-exploitation" title="Permalink to this headline">¶</a></h3>
<p>One dilemma inherent to the RL problem setting is the tension between the desire to choose the best known option and the need to try something new in order to discover other options that may be even better. Choosing the best known action is known as exploitation, while choosing a different action is known as exploration.</p>
<p>Typically, this is solved by adding to the policy a small probability of exploration. For example, the policy could be to choose the optimal action (optimal with regard to what is known) with probability 0.95, and exploring by randomly choosing some other action with probability 0.5 (if uniform across all remaining actions: probability 0.5/(n-1) where n is the number of states).</p>
</div>
<div class="section" id="mdps-and-tabular-methods">
<h3>MDPs and Tabular methods<a class="headerlink" href="#mdps-and-tabular-methods" title="Permalink to this headline">¶</a></h3>
<p>Many problems can be effectively modeled as Markov Decision Processes (MDPs), and usually as <cite>Partially Observable Markov Decision Processes (POMDPs) &lt;https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process&gt;</cite>. That is, we have</p>
<blockquote>
<div><ul class="simple">
<li>a set of states <span class="math notranslate nohighlight">\(S\)</span></li>
<li>a set of actions <span class="math notranslate nohighlight">\(A\)</span></li>
<li>a set of conditional state transition probabilities <span class="math notranslate nohighlight">\(T\)</span></li>
<li>a reward function <span class="math notranslate nohighlight">\(R: S \times A \rightarrow \mathbb{R}\)</span></li>
<li>a set of observations <span class="math notranslate nohighlight">\(\Omega\)</span></li>
<li>a set of condition observation probabilities <span class="math notranslate nohighlight">\(O\)</span></li>
<li>a discount factor <span class="math notranslate nohighlight">\(\gamma \in [0]\)</span></li>
</ul>
</div></blockquote>
<p>Given these things, the goal is to choose the action at each time step which will maximize <span class="math notranslate nohighlight">\(E \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right]\)</span>, the expected discounted reward.</p>
</div>
<div class="section" id="monte-carlo-methods">
<h3>Monte Carlo methods<a class="headerlink" href="#monte-carlo-methods" title="Permalink to this headline">¶</a></h3>
<p>One possible approach is to run a large number of simulations to learn <span class="math notranslate nohighlight">\(p^*\)</span>. This is good for cases where we know the environment and can run many simulations reasonably quickly. For example, it is fairly trivial to compute an optimal policy for the card game <cite>21 (blackjack) &lt;https://en.wikipedia.org/wiki/Twenty-One_(card_game)&gt;</cite> by running many simulations, and the same is true for most simple games.</p>
</div>
<div class="section" id="temporal-difference-learning">
<h3>Temporal-Difference Learning<a class="headerlink" href="#temporal-difference-learning" title="Permalink to this headline">¶</a></h3>
<p>TODO</p>
</div>
<div class="section" id="planning">
<h3>Planning<a class="headerlink" href="#planning" title="Permalink to this headline">¶</a></h3>
<p>TODO</p>
</div>
<div class="section" id="on-policy-vs-off-policy-learning">
<h3>On-Policy vs. Off-Policy Learning<a class="headerlink" href="#on-policy-vs-off-policy-learning" title="Permalink to this headline">¶</a></h3>
<p>TODO</p>
</div>
<div class="section" id="model-free-vs-model-based-approaches">
<h3>Model-Free vs. Model-Based Approaches<a class="headerlink" href="#model-free-vs-model-based-approaches" title="Permalink to this headline">¶</a></h3>
<p>TODO</p>
</div>
<div class="section" id="imitation-learning">
<h3>Imitation Learning<a class="headerlink" href="#imitation-learning" title="Permalink to this headline">¶</a></h3>
<p>TODO</p>
</div>
<div class="section" id="q-learning">
<h3>Q-Learning<a class="headerlink" href="#q-learning" title="Permalink to this headline">¶</a></h3>
<p>Q Learning, a model-free RL algorithm, is to update Q values to the optimal by iteration. It is an off-policy method that select the optimal action based on the current estimated Q* and does not follow the current policy.</p>
<p>The algorithm of Q Learning is:</p>
<blockquote>
<div><ol class="arabic simple">
<li>Initialize t = 0.</li>
<li>Start at initial state s<sub>t</sub> = 0.</li>
<li>The agent chooses a<sub>t</sub> = ɛ-greedy
action.</li>
<li>For given a<sub>t</sub>, the agent retrieves
the reward r<sub>t+1</sub> as well as the next
state s<sub>t+1</sub>.</li>
<li>Get (but do not perform) the next action
a<sub>t+1</sub> =
argmax<sub>a∈A</sub>Q(s<sub>t+1</sub>, a).</li>
<li>Compute the TD target y<sub>t</sub> =
r<sub>t+1</sub> + γ · Q(s<sub>t+1</sub>,
a<sub>t+1</sub>), where γ is the discounted
factor.</li>
<li>Calculate the TD error δ = y<sub>t</sub> −
Q(s<sub>t</sub>, a<sub>t</sub>).</li>
<li>Update Q(s<sub>t</sub>, a<sub>t</sub>) ←
Q(s<sub>t</sub>, a<sub>t</sub>) + α<sub>t</sub> ·
δ, where α<sub>t</sub> is the step size
(learning rate) at t.</li>
<li>Update t ← t + 1 and repeat step 3-9 until
Q(s, a) converge.</li>
</ol>
</div></blockquote>
<p>Epsilon-Greedy Algorithm</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
a_{t} = \begin{cases}
argmax_{a∈A} &amp; \text{if } p = 1 - e \\
random\, action\ &amp;\text{otherwise}
\end{cases}
\end{equation}\end{split}\]</div>
<p>The agent performs optimal action for exploitation or random action for exploration during training. It acts randomly in the beginning with the ɛ = 1 and chooses the best action based on the Q function with a decreasing ɛ capped at some small constant not equal to zero.</p>
<p>Q-Table / Q-Matrix</p>
<blockquote>
<div><table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="24%" />
<col width="24%" />
<col width="8%" />
<col width="24%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>&#160;</td>
<td>a<sub>1</sub></td>
<td>a<sub>2</sub></td>
<td>…</td>
<td>a<sub>n</sub></td>
</tr>
<tr class="row-even"><td>s<sub>1</sub></td>
<td>Q
(s<sub>1</sub>,
a<sub>1</sub>)</td>
<td>Q
(s<sub>1</sub>,
a<sub>2</sub>)</td>
<td>…</td>
<td>Q
(s<sub>1</sub>,
a<sub>3</sub>)</td>
</tr>
<tr class="row-odd"><td>s<sub>2</sub></td>
<td>Q
(s<sub>2</sub>,
a<sub>1</sub>)</td>
<td>Q
(s<sub>2</sub>,
a<sub>2</sub>)</td>
<td>…</td>
<td>Q
(s<sub>2</sub>,
a<sub>3</sub>)</td>
</tr>
<tr class="row-even"><td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="row-odd"><td>s<sub>m</sub></td>
<td>Q
(s<sub>m</sub>,
a<sub>1</sub>)</td>
<td>Q
(s<sub>m</sub>,
a<sub>2</sub>)</td>
<td>…</td>
<td>Q
(s<sub>m</sub>,
a<sub>3</sub>)</td>
</tr>
</tbody>
</table>
</div></blockquote>
<p>It’s a lookup table storing the action-value function Q(s, a) for state-action pairs where there are M states and n actions. We can initialize the Q(s, a) arbitrarily except s = terminal state. For s = final state, we set it equal to the reward on that state.</p>
<p>Reasons of using Q Learning are:</p>
<blockquote>
<div><ul class="simple">
<li>It’s applicable for the discrete action space of our environment.</li>
<li>When we don’t have the true MDP model: transitional probability matrix and rewards (Model-Free Setting).</li>
<li>It’s able to learn from incomplete episodes because of TD learning.</li>
</ul>
</div></blockquote>
<p>Drawbacks of Q Learning are:</p>
<blockquote>
<div><ul class="simple">
<li>When the state space and action space are continuous and extremely large, due to the curse of dimensionality, it’s nearly impossible to maintain a Q-matrix when the data is large.</li>
<li>Using a Q-table is unable to infer optimal action for unseen states.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="deep-q-learning">
<h3>Deep Q-Learning<a class="headerlink" href="#deep-q-learning" title="Permalink to this headline">¶</a></h3>
<p>Deep Q-learning pursues the same general methods as Q-learning. Its innovation is to add a neural network, which makes it possible to learn a very complex Q-function. This makes it very powerful, especially because it makes a large body of well-developed theory and tools for deep learning useful to reinforcement learning problems.</p>
</div>
<div class="section" id="examples-of-applications">
<h3>Examples of Applications<a class="headerlink" href="#examples-of-applications" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="https://blog.paperspace.com/creating-custom-environments-openai-gym/">Getting Started With OpenAI Gym: Creating Custom Gym Environments</a></li>
<li><a class="reference external" href="https://www.simplilearn.com/tutorials/machine-learning-tutorial/what-is-q-learning">What Is Q-Learning: The Best Guide To Understand Q-Learning (Simplilearn)</a></li>
<li><a class="reference external" href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html">REINFORCEMENT LEARNING (DQN) TUTORIAL (PyTorch)</a></li>
<li><a class="reference external" href="https://github.com/yatshunlee/qwop_RL">QWOP Game AI (DQN/DDQN)</a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="links">
<h3>Links<a class="headerlink" href="#links" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="https://towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12">Practical Applications of Reinforcement Learning (tTowards Data Science)</a></li>
<li><a class="reference external" href="https://www.geeksforgeeks.org/what-is-reinforcement-learning/">Reinforcement learning (GeeksforGeeks)</a></li>
<li><a class="reference external" href="https://medium.com/&#64;SmartLabAI/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc">Reinforcement Learning Algorithms: An Intuitive Overview (SmartLabAI)</a></li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Q-learning">Q-learning(Wikipedia)</a></li>
<li><a class="reference external" href="https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/">Epsilon-Greedy Algorithm in Reinforcement Learning (GeeksforGeeks)</a></li>
<li><a class="reference external" href="https://www.gymlibrary.ml/">OpenAI Gym Documentation</a></li>
<li><a class="reference external" href="https://stable-baselines3.readthedocs.io/en/master/#">Stable-Baselines3 Documentation</a></li>
<li><a class="reference external" href="https://www.davidsilver.uk/teaching/">David Silver Teaching Material</a></li>
</ul>
</div></blockquote>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="https://en.wikipedia.org/wiki/Reinforcement_learning#Introduction">https://en.wikipedia.org/wiki/Reinforcement_learning#Introduction</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Reinforcement Learning: An Introduction (Sutton and Barto, 2018)</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>Silver, David. “Lecture 5: Model-Free Control.” UCL, Computer Sci. Dep. Reinf Learn. Lect. (2015): 101-140.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>En.wikipedia.org. 2022. Q-learning - Wikipedia. [online] Available at: &lt;<a class="reference external" href="https://en.wikipedia.org/wiki/Q-learning">https://en.wikipedia.org/wiki/Q-learning</a>&gt; [Accessed 15 June 2022].</td></tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-datasets"></span><div class="section" id="datasets-1">
<span id="datasets"></span><h2>Datasets<a class="headerlink" href="#datasets-1" title="Permalink to this headline">¶</a></h2>
<p>Public datasets in vision, nlp and more forked from caesar0301’s <a class="reference external" href="https://github.com/caesar0301/awesome-public-datasets">awesome datasets</a> wiki.</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#agriculture" id="toc-entry-1">Agriculture</a></li>
<li><a class="reference internal" href="#art" id="toc-entry-2">Art</a></li>
<li><a class="reference internal" href="#biology" id="toc-entry-3">Biology</a></li>
<li><a class="reference internal" href="#chemistry-materials-science" id="toc-entry-4">Chemistry/Materials Science</a></li>
<li><a class="reference internal" href="#climate-weather" id="toc-entry-5">Climate/Weather</a></li>
<li><a class="reference internal" href="#complex-networks" id="toc-entry-6">Complex Networks</a></li>
<li><a class="reference internal" href="#computer-networks" id="toc-entry-7">Computer Networks</a></li>
<li><a class="reference internal" href="#data-challenges" id="toc-entry-8">Data Challenges</a></li>
<li><a class="reference internal" href="#earth-science" id="toc-entry-9">Earth Science</a></li>
<li><a class="reference internal" href="#economics" id="toc-entry-10">Economics</a></li>
<li><a class="reference internal" href="#education" id="toc-entry-11">Education</a></li>
<li><a class="reference internal" href="#energy" id="toc-entry-12">Energy</a></li>
<li><a class="reference internal" href="#finance" id="toc-entry-13">Finance</a></li>
<li><a class="reference internal" href="#gis" id="toc-entry-14">GIS</a></li>
<li><a class="reference internal" href="#government" id="toc-entry-15">Government</a></li>
<li><a class="reference internal" href="#healthcare" id="toc-entry-16">Healthcare</a></li>
<li><a class="reference internal" href="#image-processing" id="toc-entry-17">Image Processing</a></li>
<li><a class="reference internal" href="#machine-learning" id="toc-entry-18">Machine Learning</a></li>
<li><a class="reference internal" href="#museums" id="toc-entry-19">Museums</a></li>
<li><a class="reference internal" href="#music" id="toc-entry-20">Music</a></li>
<li><a class="reference internal" href="#natural-language" id="toc-entry-21">Natural Language</a></li>
<li><a class="reference internal" href="#neuroscience" id="toc-entry-22">Neuroscience</a></li>
<li><a class="reference internal" href="#physics" id="toc-entry-23">Physics</a></li>
<li><a class="reference internal" href="#psychology-cognition" id="toc-entry-24">Psychology/Cognition</a></li>
<li><a class="reference internal" href="#public-domains" id="toc-entry-25">Public Domains</a></li>
<li><a class="reference internal" href="#search-engines" id="toc-entry-26">Search Engines</a></li>
<li><a class="reference internal" href="#social-networks" id="toc-entry-27">Social Networks</a></li>
<li><a class="reference internal" href="#social-sciences" id="toc-entry-28">Social Sciences</a></li>
<li><a class="reference internal" href="#software" id="toc-entry-29">Software</a></li>
<li><a class="reference internal" href="#sports" id="toc-entry-30">Sports</a></li>
<li><a class="reference internal" href="#time-series" id="toc-entry-31">Time Series</a></li>
<li><a class="reference internal" href="#transportation" id="toc-entry-32">Transportation</a></li>
</ul>
</div>
<div class="section" id="agriculture">
<h3><a class="toc-backref" href="#toc-entry-1">Agriculture</a><a class="headerlink" href="#agriculture" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://www.plants.usda.gov/dl_all.html">U.S. Department of Agriculture’s PLANTS Database</a></li>
<li><a class="reference external" href="https://www.ars.usda.gov/northeast-area/beltsville-md/beltsville-human-nutrition-research-center/nutrient-data-laboratory/docs/sr28-download-files/">U.S. Department of Agriculture’s Nutrient Database</a></li>
</ul>
</div>
<div class="section" id="art">
<h3><a class="toc-backref" href="#toc-entry-2">Art</a><a class="headerlink" href="#art" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://quickdraw.withgoogle.com/data">Google’s Quick Draw Sketch Dataset</a></li>
</ul>
</div>
<div class="section" id="biology">
<h3><a class="toc-backref" href="#toc-entry-3">Biology</a><a class="headerlink" href="#biology" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://www.1000genomes.org/data">1000 Genomes</a></li>
<li><a class="reference external" href="https://github.com/biocore/American-Gut">American Gut (Microbiome Project)</a></li>
<li><a class="reference external" href="https://www.broadinstitute.org/bbbc">Broad Bioimage Benchmark Collection (BBBC)</a></li>
<li><a class="reference external" href="http://www.broadinstitute.org/ccle/home">Broad Cancer Cell Line Encyclopedia (CCLE)</a></li>
<li><a class="reference external" href="http://www.cellimagelibrary.org">Cell Image Library</a></li>
<li><a class="reference external" href="http://www.completegenomics.com/public-data/69-genomes/">Complete Genomics Public Data</a></li>
<li><a class="reference external" href="http://www.ebi.ac.uk/arrayexpress/">EBI ArrayExpress</a></li>
<li><a class="reference external" href="http://www.ebi.ac.uk/pdbe/emdb/index.html/">EBI Protein Data Bank in Europe</a></li>
<li><a class="reference external" href="http://www.ebi.ac.uk/pdbe/emdb/empiar/">Electron Microscopy Pilot Image Archive (EMPIAR)</a></li>
<li><a class="reference external" href="https://www.encodeproject.org">ENCODE project</a></li>
<li><a class="reference external" href="http://ensemblgenomes.org/info/genomes">Ensembl Genomes</a></li>
<li><a class="reference external" href="http://www.ncbi.nlm.nih.gov/geo/">Gene Expression Omnibus (GEO)</a></li>
<li><a class="reference external" href="http://geneontology.org/page/download-annotations">Gene Ontology (GO)</a></li>
<li><a class="reference external" href="https://github.com/jhpoelen/eol-globi-data/wiki#accessing-species-interaction-data">Global Biotic Interactions (GloBI)</a></li>
<li><a class="reference external" href="http://lincs.hms.harvard.edu">Harvard Medical School (HMS) LINCS Project</a></li>
<li><a class="reference external" href="http://www.hagsc.org/hgdp/files.html">Human Genome Diversity Project</a></li>
<li><a class="reference external" href="http://www.hmpdacc.org/reference_genomes/reference_genomes.php">Human Microbiome Project (HMP)</a></li>
<li><a class="reference external" href="http://ico2s.org/datasets/psp_benchmark.html">ICOS PSP Benchmark</a></li>
<li><a class="reference external" href="http://hapmap.ncbi.nlm.nih.gov/downloads/index.html.en">International HapMap Project</a></li>
<li><a class="reference external" href="http://jcb-dataviewer.rupress.org">Journal of Cell Biology DataViewer</a></li>
<li><a class="reference external" href="http://www.broadinstitute.org/cgi-bin/cancer/datasets.cgi">MIT Cancer Genomics Data</a></li>
<li><a class="reference external" href="http://www.ncbi.nlm.nih.gov/guide/proteins/#databases">NCBI Proteins</a></li>
<li><a class="reference external" href="http://www.ncbi.nlm.nih.gov/taxonomy">NCBI Taxonomy</a></li>
<li><a class="reference external" href="https://gdc-portal.nci.nih.gov">NCI Genomic Data Commons</a></li>
<li><a class="reference external" href="http://bit.do/VVW6">NIH Microarray data</a> or <a class="reference external" href="ftp://ftp.ncbi.nih.gov/pub/geo/DATA/supplementary/series/GSE6532/">FTP</a> (see FTP link on <a class="reference external" href="https://raw.githubusercontent.com/caesar0301/awesome-public-datasets/master/README.rst">RAW</a>)</li>
<li><a class="reference external" href="https://opensnp.org/">OpenSNP genotypes data</a></li>
<li><a class="reference external" href="http://www.pathguide.org/">Pathguid - Protein-Protein Interactions Catalog</a></li>
<li><a class="reference external" href="http://www.rcsb.org/">Protein Data Bank</a></li>
<li><a class="reference external" href="https://www.med.unc.edu/pgc/downloads">Psychiatric Genomics Consortium</a></li>
<li><a class="reference external" href="https://pubchem.ncbi.nlm.nih.gov/">PubChem Project</a></li>
<li><a class="reference external" href="http://www.pubgene.org/">PubGene (now Coremine Medical)</a></li>
<li><a class="reference external" href="http://cancer.sanger.ac.uk/cosmic">Sanger Catalogue of Somatic Mutations in Cancer (COSMIC)</a></li>
<li><a class="reference external" href="http://www.cancerrxgene.org/">Sanger Genomics of Drug Sensitivity in Cancer Project (GDSC)</a></li>
<li><a class="reference external" href="http://www.ncbi.nlm.nih.gov/Traces/sra/">Sequence Read Archive(SRA)</a></li>
<li><a class="reference external" href="http://smd.stanford.edu/">Stanford Microarray Data</a></li>
<li><a class="reference external" href="http://www.stowers.org/research/publications/odr">Stowers Institute Original Data Repository</a></li>
<li><a class="reference external" href="http://ssbd.qbic.riken.jp">Systems Science of Biological Dynamics (SSBD) Database</a></li>
<li><a class="reference external" href="https://gdac.broadinstitute.org/">The Cancer Genome Atlas (TCGA), available via Broad GDAC</a></li>
<li><a class="reference external" href="http://www.catalogueoflife.org/content/annual-checklist-archive">The Catalogue of Life</a></li>
<li><a class="reference external" href="http://www.personalgenomes.org/">The Personal Genome Project</a> or <a class="reference external" href="https://my.pgp-hms.org/public_genetic_data">PGP</a></li>
<li><a class="reference external" href="http://hgdownload.soe.ucsc.edu/downloads.html">UCSC Public Data</a></li>
<li><a class="reference external" href="http://www.ncbi.nlm.nih.gov/unigene">UniGene</a></li>
<li><a class="reference external" href="http://www.uniprot.org/downloads">Universal Protein Resource (UnitProt)</a></li>
</ul>
</div>
<div class="section" id="chemistry-materials-science">
<h3><a class="toc-backref" href="#toc-entry-4">Chemistry/Materials Science</a><a class="headerlink" href="#chemistry-materials-science" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://catalog.data.gov/dataset/nist-computational-chemistry-comparison-and-benchmark-database-srd-101">NIST Computational Chemistry Comparison and Benchmark Database - SRD 101</a></li>
<li><a class="reference external" href="http://oqmd.org/download/">Open Quantum Materials Database</a></li>
<li><a class="reference external" href="https://citrination.com/datasets#public">Citrination Public Datasets</a></li>
<li><a class="reference external" href="https://khazana.gatech.edu/module_search/search.php?m=2">Khazana Project</a></li>
</ul>
</div>
<div class="section" id="climate-weather">
<h3><a class="toc-backref" href="#toc-entry-5">Climate/Weather</a><a class="headerlink" href="#climate-weather" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://actuariesclimateindex.org/data/">Actuaries Climate Index</a></li>
<li><a class="reference external" href="http://www.bom.gov.au/climate/dwo/">Australian Weather</a></li>
<li><a class="reference external" href="https://aviationweather.gov/adds/dataserver">Aviation Weather Center - Consistent, timely and accurate weather information for the world airspace system</a></li>
<li><a class="reference external" href="http://sinda.crn2.inpe.br/PCD/SITE/novo/site/">Brazilian Weather - Historical data (In Portuguese)</a></li>
<li><a class="reference external" href="http://weather.gc.ca/grib/index_e.html">Canadian Meteorological Centre</a></li>
<li><a class="reference external" href="https://crudata.uea.ac.uk/cru/data/temperature/#datterandftp://ftp.cmdl.noaa.gov/">Climate Data from UEA (updated monthly)</a></li>
<li><a class="reference external" href="http://eca.knmi.nl/">European Climate Assessment &amp; Dataset</a></li>
<li><a class="reference external" href="http://en.tutiempo.net/climate">Global Climate Data Since 1929</a></li>
<li><a class="reference external" href="https://wiki.earthdata.nasa.gov/display/GIBS">NASA Global Imagery Browse Services</a></li>
<li><a class="reference external" href="http://www.beringclimate.noaa.gov/">NOAA Bering Sea Climate</a></li>
<li><a class="reference external" href="http://www.ncdc.noaa.gov/data-access/quick-links">NOAA Climate Datasets</a></li>
<li><a class="reference external" href="http://www.ncdc.noaa.gov/data-access/model-data/model-datasets/numerical-weather-prediction">NOAA Realtime Weather Models</a></li>
<li><a class="reference external" href="https://www.esrl.noaa.gov/gmd/grad/stardata.html">NOAA SURFRAD Meteorology and Radiation Datasets</a></li>
<li><a class="reference external" href="http://data.worldbank.org/developers/climate-data-api">The World Bank Open Data Resources for Climate Change</a></li>
<li><a class="reference external" href="http://www.cru.uea.ac.uk/data">UEA Climatic Research Unit</a></li>
<li><a class="reference external" href="http://www.worldclim.org">WorldClim - Global Climate Data</a></li>
<li><a class="reference external" href="https://www.wunderground.com/history/index.html">WU Historical Weather Worldwide</a></li>
</ul>
</div>
<div class="section" id="complex-networks">
<h3><a class="toc-backref" href="#toc-entry-6">Complex Networks</a><a class="headerlink" href="#complex-networks" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://aminer.org/citation">AMiner Citation Network Dataset</a></li>
<li><a class="reference external" href="https://archive.org/details/doi-urls">CrossRef DOI URLs</a></li>
<li><a class="reference external" href="https://kdl.cs.umass.edu/display/public/DBLP">DBLP Citation dataset</a></li>
<li><a class="reference external" href="http://www.dis.uniroma1.it/challenge9/download.shtml">DIMACS Road Networks Collection</a></li>
<li><a class="reference external" href="http://nber.org/patents/">NBER Patent Citations</a></li>
<li><a class="reference external" href="http://networkrepository.com/">Network Repository with Interactive Exploratory Analysis Tools</a></li>
<li><a class="reference external" href="http://math.nist.gov/~RPozo/complex_datasets.html">NIST complex networks data collection</a></li>
<li><a class="reference external" href="http://vlado.fmf.uni-lj.si/pub/networks/data/bio/Yeast/Yeast.htm">Protein-protein interaction network</a></li>
<li><a class="reference external" href="https://ogirardot.wordpress.com/2013/01/31/sharing-pypimaven-dependency-data/">PyPI and Maven Dependency Network</a></li>
<li><a class="reference external" href="https://www.elsevier.com/solutions/scopus">Scopus Citation Database</a></li>
<li><a class="reference external" href="http://www-personal.umich.edu/~mejn/netdata/">Small Network Data</a></li>
<li><a class="reference external" href="http://www3.cs.stonybrook.edu/~algorith/implement/graphbase/implement.shtml">Stanford GraphBase (Steven Skiena)</a></li>
<li><a class="reference external" href="http://snap.stanford.edu/data/">Stanford Large Network Dataset Collection</a></li>
<li><a class="reference external" href="http://stanford.edu/group/sonia/dataSources/index.html">Stanford Longitudinal Network Data Sources</a></li>
<li><a class="reference external" href="http://konect.uni-koblenz.de/">The Koblenz Network Collection</a></li>
<li><a class="reference external" href="http://law.di.unimi.it/datasets.php">The Laboratory for Web Algorithmics (UNIMI)</a></li>
<li><a class="reference external" href="http://nexus.igraph.org/">The Nexus Network Repository</a></li>
<li><a class="reference external" href="https://networkdata.ics.uci.edu/resources.php">UCI Network Data Repository</a></li>
<li><a class="reference external" href="http://www.cise.ufl.edu/research/sparse/matrices/">UFL sparse matrix collection</a></li>
<li><a class="reference external" href="http://www.eecs.wsu.edu/mgd/gdb.html">WSU Graph Database</a></li>
</ul>
</div>
<div class="section" id="computer-networks">
<h3><a class="toc-backref" href="#toc-entry-7">Computer Networks</a><a class="headerlink" href="#computer-networks" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://www.bigdatanews.com/profiles/blogs/big-data-set-3-5-billion-web-pages-made-available-for-all-of-us">3.5B Web Pages from CommonCrawl 2012</a></li>
<li><a class="reference external" href="http://cnets.indiana.edu/groups/nan/webtraffic/click-dataset/">53.5B Web clicks of 100K users in Indiana Univ.</a></li>
<li><a class="reference external" href="http://www.caida.org/data/overview/">CAIDA Internet Datasets</a></li>
<li><a class="reference external" href="http://lemurproject.org/clueweb09/">ClueWeb09 - 1B web pages</a></li>
<li><a class="reference external" href="http://lemurproject.org/clueweb12/">ClueWeb12 - 733M web pages</a></li>
<li><a class="reference external" href="http://commoncrawl.org/the-data/get-started/">CommonCrawl Web Data over 7 years</a></li>
<li><a class="reference external" href="https://crawdad.cs.dartmouth.edu/">CRAWDAD Wireless datasets from Dartmouth Univ.</a></li>
<li><a class="reference external" href="http://labs.criteo.com/2015/03/criteo-releases-its-new-dataset/">Criteo click-through data</a></li>
<li><a class="reference external" href="https://ooni.torproject.org/data/">OONI: Open Observatory of Network Interference - Internet censorship data</a></li>
<li><a class="reference external" href="https://console.developers.google.com/storage/openmobiledata_public/">Open Mobile Data by MobiPerf</a></li>
<li><a class="reference external" href="https://sonar.labs.rapid7.com/">Rapid7 Sonar Internet Scans</a></li>
<li><a class="reference external" href="http://www.caida.org/projects/network_telescope/">UCSD Network Telescope, IPv4 /8 net</a></li>
</ul>
</div>
<div class="section" id="data-challenges">
<h3><a class="toc-backref" href="#toc-entry-8">Data Challenges</a><a class="headerlink" href="#data-challenges" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://github.com/duyetdev/bruteforce-database">Bruteforce Database</a></li>
<li><a class="reference external" href="http://www.chalearn.org/">Challenges in Machine Learning</a></li>
<li><a class="reference external" href="http://data.crowdanalytix.com">CrowdANALYTIX dataX</a></li>
<li><a class="reference external" href="http://www.d4d.orange.com/en/home">D4D Challenge of Orange</a></li>
<li><a class="reference external" href="http://www.drivendata.org/">DrivenData Competitions for Social Good</a></li>
<li><a class="reference external" href="http://icwsm.cs.umbc.edu/">ICWSM Data Challenge (since 2009)</a></li>
<li><a class="reference external" href="https://www.kaggle.com/">Kaggle Competition Data</a></li>
<li><a class="reference external" href="http://www.kddcup2012.org/">KDD Cup by Tencent 2012</a></li>
<li><a class="reference external" href="https://github.com/localytics/data-viz-challenge">Localytics Data Visualization Challenge</a></li>
<li><a class="reference external" href="http://netflixprize.com/leaderboard.html">Netflix Prize</a></li>
<li><a class="reference external" href="https://2015.spaceappschallenge.org">Space Apps Challenge</a></li>
<li><a class="reference external" href="https://dandelion.eu/datamine/open-big-data/">Telecom Italia Big Data Challenge</a></li>
<li><a class="reference external" href="https://travistorrent.testroots.org/">TravisTorrent Dataset - MSR’2017 Mining Challenge</a></li>
<li><a class="reference external" href="http://www.yelp.com/dataset_challenge">Yelp Dataset Challenge</a></li>
</ul>
</div>
<div class="section" id="earth-science">
<h3><a class="toc-backref" href="#toc-entry-9">Earth Science</a><a class="headerlink" href="#earth-science" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://www.fao.org/nr/water/aquastat/data/query/index.html?lang=en">AQUASTAT - Global water resources and uses</a></li>
<li><a class="reference external" href="https://www.bodc.ac.uk/data/">BODC - marine data of ~22K vars</a></li>
<li><a class="reference external" href="http://www.earthmodels.org/">Earth Models</a></li>
<li><a class="reference external" href="http://sedac.ciesin.columbia.edu/data/sets/browse">EOSDIS - NASA’s earth observing system data</a></li>
<li><a class="reference external" href="https://imos.aodn.org.au">Integrated Marine Observing System (IMOS) - roughly 30TB of ocean measurements</a> or <a class="reference external" href="http://imos-data.s3-website-ap-southeast-2.amazonaws.com/">on S3</a></li>
<li><a class="reference external" href="http://marinexplore.org/">Marinexplore - Open Oceanographic Data</a></li>
<li><a class="reference external" href="http://volcano.si.edu/">Smithsonian Institution Global Volcano and Eruption Database</a></li>
<li><a class="reference external" href="http://earthquake.usgs.gov/earthquakes/search/">USGS Earthquake Archives</a></li>
</ul>
</div>
<div class="section" id="economics">
<h3><a class="toc-backref" href="#toc-entry-10">Economics</a><a class="headerlink" href="#economics" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://www.aeaweb.org/resources/data">American Economic Association (AEA)</a></li>
<li><a class="reference external" href="http://inforumweb.umd.edu/econdata/econdata.html">EconData from UMD</a></li>
<li><a class="reference external" href="http://www.freetheworld.com/datasets_efw.html">Economic Freedom of the World Data</a></li>
<li><a class="reference external" href="http://www.historicalstatistics.org/">Historical MacroEconomc Statistics</a></li>
<li><a class="reference external" href="http://widukind.cepremap.org/">International Economics Database</a> and <a class="reference external" href="https://github.com/Widukind">various data tools</a></li>
<li><a class="reference external" href="http://www.econostatistics.co.za/">International Trade Statistics</a></li>
<li><a class="reference external" href="http://www.upcdatabase.com/">Internet Product Code Database</a></li>
<li><a class="reference external" href="http://www.jedh.org/">Joint External Debt Data Hub</a></li>
<li><a class="reference external" href="http://www.macalester.edu/research/economics/PAGE/HAVEMAN/Trade.Resources/TradeData.html">Jon Haveman International Trade Data Links</a></li>
<li><a class="reference external" href="https://opencorporates.com/">OpenCorporates Database of Companies in the World</a></li>
<li><a class="reference external" href="http://ourworldindata.org/">Our World in Data</a></li>
<li><a class="reference external" href="http://econ.sciences-po.fr/thierry-mayer/data">SciencesPo World Trade Gravity Datasets</a></li>
<li><a class="reference external" href="http://atlas.cid.harvard.edu">The Atlas of Economic Complexity</a></li>
<li><a class="reference external" href="http://cid.econ.ucdavis.edu">The Center for International Data</a></li>
<li><a class="reference external" href="http://atlas.media.mit.edu/en/">The Observatory of Economic Complexity</a></li>
<li><a class="reference external" href="http://comtrade.un.org/db/">UN Commodity Trade Statistics</a></li>
<li><a class="reference external" href="http://hdr.undp.org/en">UN Human Development Reports</a></li>
</ul>
</div>
<div class="section" id="education">
<h3><a class="toc-backref" href="#toc-entry-11">Education</a><a class="headerlink" href="#education" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://collegescorecard.ed.gov/data/">College Scorecard Data</a></li>
<li><a class="reference external" href="http://academictorrents.com/details/030b10dad0846b5aecc3905692890fb02404adbf">Student Data from Free Code Camp</a></li>
</ul>
</div>
<div class="section" id="energy">
<h3><a class="toc-backref" href="#toc-entry-12">Energy</a><a class="headerlink" href="#energy" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://ampds.org/">AMPds</a></li>
<li><a class="reference external" href="http://nilm.cmubi.org/">BLUEd</a></li>
<li><a class="reference external" href="http://combed.github.io/">COMBED</a></li>
<li><a class="reference external" href="https://dataport.pecanstreet.org/">Dataport</a></li>
<li><a class="reference external" href="http://www.st.ewi.tudelft.nl/~akshay/dred/">DRED</a></li>
<li><a class="reference external" href="http://www.vs.inf.ethz.ch/res/show.html?what=eco-data">ECO</a></li>
<li><a class="reference external" href="http://www.eia.gov/electricity/data/eia923/">EIA</a></li>
<li><a class="reference external" href="http://randd.defra.gov.uk/Default.aspx?Menu=Menu&amp;Module=More&amp;Location=None&amp;ProjectID=17359&amp;FromSearch=Y&amp;Publisher=1&amp;SearchText=EV0702&amp;SortString=ProjectCode&amp;SortOrder=Asc&amp;Paging=10#Description">HES</a> - Household Electricity Study, UK</li>
<li><a class="reference external" href="http://hfed.github.io/">HFED</a></li>
<li><a class="reference external" href="http://iawe.github.io/">iAWE</a></li>
<li><a class="reference external" href="http://plaidplug.com/">PLAID</a> - the Plug Load Appliance Identification Dataset</li>
<li><a class="reference external" href="http://redd.csail.mit.edu/">REDD</a></li>
<li><a class="reference external" href="https://www.tracebase.org">Tracebase</a></li>
<li><a class="reference external" href="http://www.doc.ic.ac.uk/~dk3810/data/">UK-DALE</a> - UK Domestic Appliance-Level Electricity</li>
<li><a class="reference external" href="http://nilmworkshop.org/2016/proceedings/Poster_ID18.pdf">WHITED</a></li>
</ul>
</div>
<div class="section" id="finance">
<h3><a class="toc-backref" href="#toc-entry-13">Finance</a><a class="headerlink" href="#finance" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://cfe.cboe.com/Data/">CBOE Futures Exchange</a></li>
<li><a class="reference external" href="https://www.google.com/finance">Google Finance</a></li>
<li><a class="reference external" href="http://www.google.com/trends?q=google&amp;ctab=0&amp;geo=all&amp;date=all&amp;sort=0">Google Trends</a></li>
<li><a class="reference external" href="https://data.nasdaq.com/">NASDAQ</a></li>
<li><a class="reference external" href="ftp://ftp.nyxdata.com">NYSE Market Data</a> (see FTP link on <a class="reference external" href="https://raw.githubusercontent.com/caesar0301/awesome-public-datasets/master/README.rst">RAW</a>)</li>
<li><a class="reference external" href="http://www.oanda.com/">OANDA</a></li>
<li><a class="reference external" href="http://fisher.osu.edu/fin/fdf/osudata.htm">OSU Financial data</a></li>
<li><a class="reference external" href="https://www.quandl.com/">Quandl</a></li>
<li><a class="reference external" href="https://research.stlouisfed.org/fred2/">St Louis Federal</a></li>
<li><a class="reference external" href="http://finance.yahoo.com/">Yahoo Finance</a></li>
</ul>
</div>
<div class="section" id="gis">
<h3><a class="toc-backref" href="#toc-entry-14">GIS</a><a class="headerlink" href="#gis" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://opendata.arcgis.com/">ArcGIS Open Data portal</a></li>
<li><a class="reference external" href="http://cambridgegis.github.io/gisdata.html">Cambridge, MA, US, GIS data on GitHub</a></li>
<li><a class="reference external" href="https://www.factual.com/">Factual Global Location Data</a></li>
<li><a class="reference external" href="http://geodacenter.asu.edu/datalist/">Geo Spatial Data from ASU</a></li>
<li><a class="reference external" href="http://geo-wiki.org/">Geo Wiki Project - Citizen-driven Environmental Monitoring</a></li>
<li><a class="reference external" href="http://download.geofabrik.de/">GeoFabrik - OSM data extracted to a variety of formats and areas</a></li>
<li><a class="reference external" href="http://www.geonames.org/">GeoNames Worldwide</a></li>
<li><a class="reference external" href="http://www.gadm.org/">Global Administrative Areas Database (GADM)</a></li>
<li><a class="reference external" href="https://hifld-dhs-gii.opendata.arcgis.com/">Homeland Infrastructure Foundation-Level Data</a></li>
<li><a class="reference external" href="https://aws.amazon.com/public-data-sets/landsat/">Landsat 8 on AWS</a></li>
<li><a class="reference external" href="https://github.com/umpirsky/country-list">List of all countries in all languages</a></li>
<li><a class="reference external" href="http://www.nws.noaa.gov/gis/">National Weather Service GIS Data Portal</a></li>
<li><a class="reference external" href="http://www.naturalearthdata.com/">Natural Earth - vectors and rasters of the world</a></li>
<li><a class="reference external" href="http://openaddresses.io/">OpenAddresses</a></li>
<li><a class="reference external" href="http://wiki.openstreetmap.org/wiki/Downloading_data">OpenStreetMap (OSM)</a></li>
<li><a class="reference external" href="http://pleiades.stoa.org/">Pleiades - Gazetteer and graph of ancient places</a></li>
<li><a class="reference external" href="https://github.com/kno10/reversegeocode">Reverse Geocoder using OSM data</a> &amp; <a class="reference external" href="http://data.ub.uni-muenchen.de/61/">additional high-resolution data files</a></li>
<li><a class="reference external" href="http://www.census.gov/geo/maps-data/data/tiger-line.html">TIGER/Line - U.S. boundaries and roads</a></li>
<li><a class="reference external" href="https://github.com/foursquare/twofishes">TwoFishes - Foursquare’s coarse geocoder</a></li>
<li><a class="reference external" href="http://efele.net/maps/tz/world/">TZ Timezones shapfiles</a></li>
<li><a class="reference external" href="http://geodata.grid.unep.ch/">UN Environmental Data</a></li>
<li><a class="reference external" href="https://hiu.state.gov/data/data.aspx">World boundaries from  the U.S. Department of State</a></li>
<li><a class="reference external" href="https://github.com/mledoze/countries">World countries in multiple formats</a></li>
</ul>
</div>
<div class="section" id="government">
<h3><a class="toc-backref" href="#toc-entry-15">Government</a><a class="headerlink" href="#government" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://github.com/caesar0301/awesome-public-datasets/blob/master/Government.rst">A list of cities and countries contributed by community</a></li>
<li><a class="reference external" href="http://opendataforafrica.org/">Open Data for Africa</a></li>
<li><a class="reference external" href="https://www.opendatasoft.com/a-comprehensive-list-of-all-open-data-portals-around-the-world/">OpenDataSoft’s list of 1,600 open data</a></li>
</ul>
</div>
<div class="section" id="healthcare">
<h3><a class="toc-backref" href="#toc-entry-16">Healthcare</a><a class="headerlink" href="#healthcare" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://www.ehdp.com/vitalnet/datasets.htm">EHDP Large Health Data Sets</a></li>
<li><a class="reference external" href="http://www.gapminder.org/data/">Gapminder World demographic databases</a></li>
<li><a class="reference external" href="https://www.cms.gov/medicare-coverage-database/">Medicare Coverage Database (MCD), U.S.</a></li>
<li><a class="reference external" href="https://data.medicare.gov/">Medicare Data Engine of medicare.gov Data</a></li>
<li><a class="reference external" href="http://go.cms.gov/19xxPN4">Medicare Data File</a></li>
<li><a class="reference external" href="https://www.nlm.nih.gov/mesh/filelist.html">MeSH, the vocabulary thesaurus used for indexing articles for PubMed</a></li>
<li><a class="reference external" href="https://data.hdx.rwlabs.org/dataset/ebola-cases-2014">Number of Ebola Cases and Deaths in Affected Countries (2014)</a></li>
<li><a class="reference external" href="http://www.openods.co.uk">Open-ODS (structure of the UK NHS)</a></li>
<li><a class="reference external" href="https://openpaymentsdata.cms.gov">OpenPaymentsData, Healthcare financial relationship data</a></li>
<li><a class="reference external" href="https://tcga-data.nci.nih.gov/tcga/tcgaDownload.jsp">The Cancer Genome Atlas project (TCGA)</a> and <a class="reference external" href="http://google-genomics.readthedocs.org/en/latest/use_cases/discover_public_data/isb_cgc_data.html">BigQuery table</a></li>
<li><a class="reference external" href="http://www.who.int/gho/en/">World Health Organization Global Health Observatory</a></li>
</ul>
</div>
<div class="section" id="image-processing">
<h3><a class="toc-backref" href="#toc-entry-17">Image Processing</a><a class="headerlink" href="#image-processing" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://wilmabainbridge.com/facememorability2.html">10k US Adult Faces Database</a></li>
<li><a class="reference external" href="http://137.189.35.203/WebUI/CatDatabase/catData.html">2GB of Photos of Cats</a> or <a class="reference external" href="https://web.archive.org/web/20150520175645/http://137.189.35.203/WebUI/CatDatabase/catData.html">Archive version</a></li>
<li><a class="reference external" href="http://www.openu.ac.il/home/hassner/Adience/data.html">Adience Unfiltered faces for gender and age classification</a></li>
<li><a class="reference external" href="http://www.imageemotion.org/">Affective Image Classification</a></li>
<li><a class="reference external" href="http://attributes.kyb.tuebingen.mpg.de/">Animals with attributes</a></li>
<li><a class="reference external" href="https://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/">Caltech Pedestrian Detection Benchmark</a></li>
<li><a class="reference external" href="http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/">Chars74K dataset, Character Recognition in Natural Images (both English and Kannada are available)</a></li>
<li><a class="reference external" href="http://www.face-rec.org/databases/">Face Recognition Benchmark</a></li>
<li><a class="reference external" href="http://dmery.ing.puc.cl/index.php/material/gdxray/">GDXray: X-ray images for X-ray testing and Computer Vision</a></li>
<li><a class="reference external" href="http://www.image-net.org/">ImageNet (in WordNet hierarchy)</a></li>
<li><a class="reference external" href="http://web.mit.edu/torralba/www/indoor.html">Indoor Scene Recognition</a></li>
<li><a class="reference external" href="http://csea.phhp.ufl.edu/media/iapsmessage.html">International Affective Picture System, UFL</a></li>
<li><a class="reference external" href="http://cvcl.mit.edu/MM/stimuli.html">Massive Visual Memory Stimuli, MIT</a></li>
<li><a class="reference external" href="http://yann.lecun.com/exdb/mnist/">MNIST database of handwritten digits, near 1 million examples</a></li>
<li><a class="reference external" href="http://kaiwolf.no-ip.org/3d-model-repository.html">Several Shape-from-Silhouette Datasets</a></li>
<li><a class="reference external" href="http://vision.stanford.edu/aditya86/ImageNetDogs/">Stanford Dogs Dataset</a></li>
<li><a class="reference external" href="http://groups.csail.mit.edu/vision/SUN/hierarchy.html">SUN database, MIT</a></li>
<li><a class="reference external" href="http://www.openu.ac.il/home/hassner/data/ASLAN/ASLAN.html">The Action Similarity Labeling (ASLAN) Challenge</a></li>
<li><a class="reference external" href="http://www.robots.ox.ac.uk/~vgg/data/pets/">The Oxford-IIIT Pet Dataset</a></li>
<li><a class="reference external" href="http://www.openu.ac.il/home/hassner/data/violentflows/">Violent-Flows - Crowd Violence Non-violence Database and benchmark</a></li>
<li><a class="reference external" href="http://visualgenome.org/api/v0/api_home.html">Visual genome</a></li>
<li><a class="reference external" href="http://www.cs.tau.ac.il/~wolf/ytfaces/">YouTube Faces Database</a></li>
</ul>
</div>
<div class="section" id="machine-learning">
<h3><a class="toc-backref" href="#toc-entry-18">Machine Learning</a><a class="headerlink" href="#machine-learning" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://github.com/irecsys/CARSKit/tree/master/context-aware_data_sets">Context-aware data sets from five domains</a></li>
<li><a class="reference external" href="http://www.cs.toronto.edu/~delve/data/datasets.html">Delve Datasets for classification and regression (Univ. of Toronto)</a></li>
<li><a class="reference external" href="http://data.discogs.com/">Discogs Monthly Data</a></li>
<li><a class="reference external" href="http://www.modelingonlineauctions.com/datasets">eBay Online Auctions (2012)</a></li>
<li><a class="reference external" href="http://www.imdb.com/interfaces">IMDb Database</a></li>
<li><a class="reference external" href="http://sci2s.ugr.es/keel/datasets.php">Keel Repository for classification, regression and time series</a></li>
<li><a class="reference external" href="http://vis-www.cs.umass.edu/lfw/">Labeled Faces in the Wild (LFW)</a></li>
<li><a class="reference external" href="https://www.lendingclub.com/info/download-data.action">Lending Club Loan Data</a></li>
<li><a class="reference external" href="http://mldata.org/">Machine Learning Data Set Repository</a></li>
<li><a class="reference external" href="http://labrosa.ee.columbia.edu/millionsong/">Million Song Dataset</a></li>
<li><a class="reference external" href="http://labrosa.ee.columbia.edu/millionsong/pages/additional-datasets">More Song Datasets</a></li>
<li><a class="reference external" href="http://grouplens.org/datasets/movielens/">MovieLens Data Sets</a></li>
<li><a class="reference external" href="https://github.com/nextml/caption-contest-data">New Yorker caption contest ratings</a></li>
<li><a class="reference external" href="http://www.rdatamining.com/data">RDataMining - “R and Data Mining” ebook data</a></li>
<li><a class="reference external" href="http://healthintelligence.drupalgardens.com/content/registered-meteorites-has-impacted-earth-visualized">Registered Meteorites on Earth</a></li>
<li><a class="reference external" href="http://missionlocal.org/san-francisco-restaurant-health-inspections/">Restaurants Health Score Data in San Francisco</a></li>
<li><a class="reference external" href="http://archive.ics.uci.edu/ml/">UCI Machine Learning Repository</a></li>
<li><a class="reference external" href="http://webscope.sandbox.yahoo.com/catalog.php?datatype=r">Yahoo! Ratings and Classification Data</a></li>
<li><a class="reference external" href="https://research.google.com/youtube8m/download.html">Youtube 8m</a></li>
</ul>
</div>
<div class="section" id="museums">
<h3><a class="toc-backref" href="#toc-entry-19">Museums</a><a class="headerlink" href="#museums" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://techno-science.ca/en/data.php">Canada Science and Technology Museums Corporation’s Open Data</a></li>
<li><a class="reference external" href="https://github.com/cooperhewitt/collection">Cooper-Hewitt’s Collection Database</a></li>
<li><a class="reference external" href="https://github.com/artsmia/collection">Minneapolis Institute of Arts metadata</a></li>
<li><a class="reference external" href="http://data.nhm.ac.uk/">Natural History Museum (London) Data Portal</a></li>
<li><a class="reference external" href="https://www.rijksmuseum.nl/en/api">Rijksmuseum Historical Art Collection</a></li>
<li><a class="reference external" href="https://github.com/tategallery/collection">Tate Collection metadata</a></li>
<li><a class="reference external" href="http://vocab.getty.edu">The Getty vocabularies</a></li>
</ul>
</div>
<div class="section" id="music">
<h3><a class="toc-backref" href="#toc-entry-20">Music</a><a class="headerlink" href="#music" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://github.com/jukedeck/nottingham-dataset">Nottingham Folk Songs</a></li>
<li><a class="reference external" href="http://music.cs.northwestern.edu/data/Bach10_Dataset_Description.pdf">Bach 10</a></li>
</ul>
</div>
<div class="section" id="natural-language">
<h3><a class="toc-backref" href="#toc-entry-21">Natural Language</a><a class="headerlink" href="#natural-language" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://github.com/snkim/AutomaticKeyphraseExtraction/">Automatic Keyphrase Extracttion</a></li>
<li><a class="reference external" href="http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm">Blogger Corpus</a></li>
<li><a class="reference external" href="http://www.clips.uantwerpen.be/datasets/csi-corpus">CLiPS Stylometry Investigation Corpus</a></li>
<li><a class="reference external" href="http://lemurproject.org/clueweb09/FACC1/">ClueWeb09 FACC</a></li>
<li><a class="reference external" href="http://lemurproject.org/clueweb12/FACC1/">ClueWeb12 FACC</a></li>
<li><a class="reference external" href="http://wiki.dbpedia.org/Datasets">DBpedia - 4.58M things with 583M facts</a></li>
<li><a class="reference external" href="http://www.isi.edu/~lerman/downloads/flickr/flickr_taxonomies.html">Flickr Personal Taxonomies</a></li>
<li><a class="reference external" href="http://www.freebase.com/">Freebase.com of people, places, and things</a></li>
<li><a class="reference external" href="https://aws.amazon.com/datasets/google-books-ngrams/">Google Books Ngrams (2.2TB)</a></li>
<li><a class="reference external" href="https://github.com/google/mcafp">Google MC-AFP, generated based on the public available Gigaword dataset using Paragraph Vectors</a></li>
<li><a class="reference external" href="https://catalog.ldc.upenn.edu/LDC2006T13">Google Web 5gram (1TB, 2006)</a></li>
<li><a class="reference external" href="http://www.gutenberg.org/wiki/Gutenberg:Offline_Catalogs">Gutenberg eBooks List</a></li>
<li><a class="reference external" href="http://www.isi.edu/natural-language/download/hansard/">Hansards text chunks of Canadian Parliament</a></li>
<li><a class="reference external" href="http://research.microsoft.com/en-us/um/redmond/projects/mctest/index.html">Machine Comprehension Test (MCTest) of text from Microsoft Research</a></li>
<li><a class="reference external" href="http://statmt.org/wmt11/translation-task.html#download">Machine Translation of European languages</a></li>
<li><a class="reference external" href="http://www.msmarco.org/dataset.aspx">Microsoft MAchine Reading COmprehension Dataset (or MS MARCO)</a></li>
<li><a class="reference external" href="http://www.cs.jhu.edu/~mdredze/datasets/sentiment/">Multi-Domain Sentiment Dataset (version 2.0)</a></li>
<li><a class="reference external" href="http://compling.hss.ntu.edu.sg/omw/">Open Multilingual Wordnet</a></li>
<li><a class="reference external" href="http://www.clips.uantwerpen.be/datasets/personae-corpus">Personae Corpus</a></li>
<li><a class="reference external" href="https://github.com/ParallelMazen/SaudiNewsNet">SaudiNewsNet Collection of Saudi Newspaper Articles (Arabic, 30K articles)</a></li>
<li><a class="reference external" href="http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/">SMS Spam Collection in English</a></li>
<li><a class="reference external" href="http://universaldependencies.org">Universal Dependencies</a></li>
<li><a class="reference external" href="http://www.psych.ualberta.ca/~westburylab/downloads/usenetcorpus.download.html">USENET postings corpus of 2005~2011</a></li>
<li><a class="reference external" href="https://webhose.io/datasets">Webhose - News/Blogs in multiple languages</a></li>
<li><a class="reference external" href="https://www.wikidata.org/wiki/Wikidata:Database_download">Wikidata - Wikipedia databases</a></li>
<li><a class="reference external" href="https://code.google.com/p/wiki-links/downloads/list">Wikipedia Links data - 40 Million Entities in Context</a></li>
<li><a class="reference external" href="http://wordnet.princeton.edu/wordnet/download/">WordNet databases and tools</a></li>
</ul>
</div>
<div class="section" id="neuroscience">
<h3><a class="toc-backref" href="#toc-entry-22">Neuroscience</a><a class="headerlink" href="#neuroscience" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://www.brain-map.org/">Allen Institute Datasets</a></li>
<li><a class="reference external" href="http://braincatalogue.org/">Brain Catalogue</a></li>
<li><a class="reference external" href="http://brainomics.cea.fr/localizer">Brainomics</a></li>
<li><a class="reference external" href="http://datasets.codeneuro.org/">CodeNeuro Datasets</a></li>
<li><a class="reference external" href="http://crcns.org/data-sets">Collaborative Research in Computational Neuroscience (CRCNS)</a></li>
<li><a class="reference external" href="http://fcon_1000.projects.nitrc.org/index.html">FCP-INDI</a></li>
<li><a class="reference external" href="http://www.humanconnectome.org/data/">Human Connectome Project</a></li>
<li><a class="reference external" href="https://ndar.nih.gov/">NDAR</a></li>
<li><a class="reference external" href="http://neurodata.io">NeuroData</a></li>
<li><a class="reference external" href="http://neuroelectro.org/">Neuroelectro</a></li>
<li><a class="reference external" href="http://data-archive.nimh.nih.gov/">NIMH Data Archive</a></li>
<li><a class="reference external" href="http://www.oasis-brains.org/">OASIS</a></li>
<li><a class="reference external" href="https://openfmri.org/">OpenfMRI</a></li>
<li><a class="reference external" href="http://studyforrest.org">Study Forrest</a></li>
</ul>
</div>
<div class="section" id="physics">
<h3><a class="toc-backref" href="#toc-entry-23">Physics</a><a class="headerlink" href="#physics" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://opendata.cern.ch/">CERN Open Data Portal</a></li>
<li><a class="reference external" href="http://www.crystallography.net/">Crystallography Open Database</a></li>
<li><a class="reference external" href="http://exoplanetarchive.ipac.caltech.edu/">NASA Exoplanet Archive</a></li>
<li><a class="reference external" href="http://nssdc.gsfc.nasa.gov/nssdc/obtaining_data.html">NSSDC (NASA) data of 550 space spacecraft</a></li>
<li><a class="reference external" href="http://www.sdss.org/">Sloan Digital Sky Survey (SDSS) - Mapping the Universe</a></li>
</ul>
</div>
<div class="section" id="psychology-cognition">
<h3><a class="toc-backref" href="#toc-entry-24">Psychology/Cognition</a><a class="headerlink" href="#psychology-cognition" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://www.cmr.osu.edu/browse/datasets">OSU Cognitive Modeling Repository Datasets</a></li>
</ul>
</div>
<div class="section" id="public-domains">
<h3><a class="toc-backref" href="#toc-entry-25">Public Domains</a><a class="headerlink" href="#public-domains" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://aws.amazon.com/datasets/">Amazon</a></li>
<li><a class="reference external" href="https://www.archive-it.org/explore?show=Collections">Archive-it from Internet Archive</a></li>
<li><a class="reference external" href="https://archive.org/details/datasets">Archive.org Datasets</a></li>
<li><a class="reference external" href="http://lib.stat.cmu.edu/jasadata/">CMU JASA data archive</a></li>
<li><a class="reference external" href="http://lib.stat.cmu.edu/datasets/">CMU StatLab collections</a></li>
<li><a class="reference external" href="https://data.world">Data.World</a></li>
<li><a class="reference external" href="http://www.data360.org/index.aspx">Data360</a></li>
<li><a class="reference external" href="http://datamob.org/datasets">Datamob.org</a></li>
<li><a class="reference external" href="http://www.google.com/publicdata/directory">Google</a></li>
<li><a class="reference external" href="http://www.infochimps.com/">Infochimps</a></li>
<li><a class="reference external" href="http://www.kdnuggets.com/datasets/index.html">KDNuggets Data Collections</a></li>
<li><a class="reference external" href="http://datamarket.azure.com/browse/data?price=free">Microsoft Azure Data Market Free DataSets</a></li>
<li><a class="reference external" href="http://aka.ms/Data-Science">Microsoft Data Science for Research</a></li>
<li><a class="reference external" href="http://numbrary.com/">Numbray</a></li>
<li><a class="reference external" href="https://openlibrary.org/developers/dumps">Open Library Data Dumps</a></li>
<li><a class="reference external" href="https://www.reddit.com/r/datasets">Reddit Datasets</a></li>
<li><a class="reference external" href="http://packages.revolutionanalytics.com/datasets/">RevolutionAnalytics Collection</a></li>
<li><a class="reference external" href="http://stat.ethz.ch/R-manual/R-patched/library/datasets/html/00Index.html">Sample R data sets</a></li>
<li><a class="reference external" href="http://www.stats4stem.org/data-sets.html">Stats4Stem R data sets</a></li>
<li><a class="reference external" href="http://www.statsci.org/datasets.html">StatSci.org</a></li>
<li><a class="reference external" href="http://www.washingtonpost.com/wp-srv/metro/data/datapost.html">The Washington Post List</a></li>
<li><a class="reference external" href="http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data">UCLA SOCR data collection</a></li>
<li><a class="reference external" href="http://www.nuforc.org/webreports.html">UFO Reports</a></li>
<li><a class="reference external" href="https://911.wikileaks.org/files/index.html">Wikileaks 911 pager intercepts</a></li>
<li><a class="reference external" href="http://webscope.sandbox.yahoo.com/catalog.php">Yahoo Webscope</a></li>
</ul>
</div>
<div class="section" id="search-engines">
<h3><a class="toc-backref" href="#toc-entry-26">Search Engines</a><a class="headerlink" href="#search-engines" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://academictorrents.com/">Academic Torrents of data sharing from UMB</a></li>
<li><a class="reference external" href="https://datahub.io/dataset">Datahub.io</a></li>
<li><a class="reference external" href="https://datamarket.com/data/list/?q=all">DataMarket (Qlik)</a></li>
<li><a class="reference external" href="https://dataverse.harvard.edu/">Harvard Dataverse Network of scientific data</a></li>
<li><a class="reference external" href="http://www.icpsr.umich.edu/icpsrweb/ICPSR/index.jsp">ICPSR (UMICH)</a></li>
<li><a class="reference external" href="http://eric.ed.gov">Institute of Education Sciences</a></li>
<li><a class="reference external" href="http://www.ntis.gov/products/ntrl/">National Technical Reports Library</a></li>
<li><a class="reference external" href="https://certificates.theodi.org/en/datasets">Open Data Certificates (beta)</a></li>
<li><a class="reference external" href="http://www.opendatanetwork.com/">OpenDataNetwork - A search engine of all Socrata powered data portals</a></li>
<li><a class="reference external" href="http://www.statista.com/">Statista.com - statistics and Studies</a></li>
<li><a class="reference external" href="https://zenodo.org/collection/datasets">Zenodo - An open dependable home for the long-tail of science</a></li>
</ul>
</div>
<div class="section" id="social-networks">
<h3><a class="toc-backref" href="#toc-entry-27">Social Networks</a><a class="headerlink" href="#social-networks" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://waxy.org/random/misc/gamergate_tweets.csv">72 hours #gamergate Twitter Scrape</a></li>
<li><a class="reference external" href="http://www.cs.cmu.edu/~jelsas/data/ancestry.com/">Ancestry.com Forum Dataset over 10 years</a></li>
<li><a class="reference external" href="https://archive.org/details/twitter_cikm_2010">Cheng-Caverlee-Lee September 2009 - January 2010 Twitter Scrape</a></li>
<li><a class="reference external" href="http://www.cs.cmu.edu/~enron/">CMU Enron Email of 150 users</a></li>
<li><a class="reference external" href="https://aws.amazon.com/datasets/enron-email-data/">EDRM Enron EMail of 151 users, hosted on S3</a></li>
<li><a class="reference external" href="https://archive.org/details/oxford-2005-facebook-matrix">Facebook Data Scrape (2005)</a></li>
<li><a class="reference external" href="http://law.di.unimi.it/datasets.php">Facebook Social Networks from LAW (since 2007)</a></li>
<li><a class="reference external" href="https://archive.org/details/201309_foursquare_dataset_umn">Foursquare from UMN/Sarwat (2013)</a></li>
<li><a class="reference external" href="https://www.githubarchive.org/">GitHub Collaboration Archive</a></li>
<li><a class="reference external" href="http://www3.cs.stonybrook.edu/~leman/data/gscholar.db">Google Scholar citation relations</a></li>
<li><a class="reference external" href="http://www.sociopatterns.org/datasets/">High-Resolution Contact Networks from Wearable Sensors</a></li>
<li><a class="reference external" href="https://kdl.cs.umass.edu/display/public/Mobile+Social+Networks">Mobile Social Networks from UMASS</a></li>
<li><a class="reference external" href="http://snap.stanford.edu/data/higgs-twitter.html">Network Twitter Data</a></li>
<li><a class="reference external" href="https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/">Reddit Comments</a></li>
<li><a class="reference external" href="https://github.com/quankiquanki/skytrax-reviews-dataset">Skytrax’ Air Travel Reviews Dataset</a></li>
<li><a class="reference external" href="http://snap.stanford.edu/data/egonets-Twitter.html">Social Twitter Data</a></li>
<li><a class="reference external" href="http://www3.nd.edu/~oss/Data/data.html">SourceForge.net Research Data</a></li>
<li><a class="reference external" href="http://nlp.uned.es/replab2013/">Twitter Data for Online Reputation Management</a></li>
<li><a class="reference external" href="http://help.sentiment140.com/for-students/">Twitter Data for Sentiment Analysis</a></li>
<li><a class="reference external" href="http://an.kaist.ac.kr/traces/WWW2010.html">Twitter Graph of entire Twitter site</a></li>
<li><a class="reference external" href="http://archive.org/details/2011-05-calufa-twitter-sql">Twitter Scrape Calufa May 2011</a></li>
<li><a class="reference external" href="http://law.di.unimi.it/datasets.php">UNIMI/LAW Social Network Datasets</a></li>
<li><a class="reference external" href="http://webscope.sandbox.yahoo.com/catalog.php?datatype=g">Yahoo! Graph and Social Data</a></li>
<li><a class="reference external" href="http://netsg.cs.sfu.ca/youtubedata/">Youtube Video Social Graph in 2007,2008</a></li>
</ul>
</div>
<div class="section" id="social-sciences">
<h3><a class="toc-backref" href="#toc-entry-28">Social Sciences</a><a class="headerlink" href="#social-sciences" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://www.acleddata.com/">ACLED (Armed Conflict Location &amp; Event Data Project)</a></li>
<li><a class="reference external" href="https://www.canlii.org/en/index.php">Canadian Legal Information Institute</a></li>
<li><a class="reference external" href="http://www.systemicpeace.org/">Center for Systemic Peace Datasets - Conflict Trends, Polities, State Fragility, etc</a></li>
<li><a class="reference external" href="http://www.correlatesofwar.org/">Correlates of War Project</a></li>
<li><a class="reference external" href="http://cryptome.org">Cryptome Conspiracy Theory Items</a></li>
<li><a class="reference external" href="http://datacards.org">Datacards</a></li>
<li><a class="reference external" href="http://www.europeansocialsurvey.org/data/">European Social Survey</a></li>
<li><a class="reference external" href="https://github.com/emorisse/FBI-Hate-Crime-Statistics/tree/master/2013">FBI Hate Crime 2013 - aggregated data</a></li>
<li><a class="reference external" href="http://fsi.fundforpeace.org/data">Fragile States Index</a></li>
<li><a class="reference external" href="http://gdeltproject.org/data.html">GDELT Global Events Database</a></li>
<li><a class="reference external" href="http://gss.norc.org">General Social Survey (GSS) since 1972</a></li>
<li><a class="reference external" href="http://www.gesis.org/en/home/">German Social Survey</a></li>
<li><a class="reference external" href="http://www.globalreligiousfutures.org/">Global Religious Futures Project</a></li>
<li><a class="reference external" href="https://data.hdx.rwlabs.org/">Humanitarian Data Exchange</a></li>
<li><a class="reference external" href="http://www.inform-index.org/Results/Global">INFORM Index for Risk Management</a></li>
<li><a class="reference external" href="http://www.ined.fr/en/">Institute for Demographic Studies</a></li>
<li><a class="reference external" href="http://www.princeton.edu/~ina/">International Networks Archive</a></li>
<li><a class="reference external" href="http://www.issp.org">International Social Survey Program ISSP</a></li>
<li><a class="reference external" href="http://www.isacompendium.com/public/">International Studies Compendium Project</a></li>
<li><a class="reference external" href="http://jmcguire.faculty.wesleyan.edu/welcome/cross-national-data/">James McGuire Cross National Data</a></li>
<li><a class="reference external" href="http://nsd.uib.no">MacroData Guide by Norsk samfunnsvitenskapelig datatjeneste</a></li>
<li><a class="reference external" href="https://www.ipums.org/">Minnesota Population Center</a></li>
<li><a class="reference external" href="http://realitycommons.media.mit.edu/realitymining.html">MIT Reality Mining Dataset</a></li>
<li><a class="reference external" href="http://index.gain.org/about/download">Notre Dame Global Adaptation Index (NG-DAIN)</a></li>
<li><a class="reference external" href="https://data.police.uk/data/">Open Crime and Policing Data in England, Wales and Northern Ireland</a></li>
<li><a class="reference external" href="http://www.paulhensel.org/dataintl.html">Paul Hensel General International Data Page</a></li>
<li><a class="reference external" href="http://www.pewinternet.org/datasets/pages/2/">PewResearch Internet Survey Project</a></li>
<li><a class="reference external" href="http://www.pewresearch.org/data/download-datasets/">PewResearch Society Data Collection</a></li>
<li><a class="reference external" href="http://www3.cs.stonybrook.edu/~leman/data/14-icwsm-political-polarity-data.zip">Political Polarity Data</a></li>
<li><a class="reference external" href="http://data.stackexchange.com/help">StackExchange Data Explorer</a></li>
<li><a class="reference external" href="http://www.trackingterrorism.org/">Terrorism Research and Analysis Consortium</a></li>
<li><a class="reference external" href="http://www.tdcj.state.tx.us/death_row/dr_executed_offenders.html">Texas Inmates Executed Since 1984</a></li>
<li><a class="reference external" href="https://github.com/caesar0301/awesome-public-datasets/tree/master/Datasets">Titanic Survival Data Set</a> or <a class="reference external" href="https://www.kaggle.com/c/titanic/data">on Kaggle</a></li>
<li><a class="reference external" href="http://ucdata.berkeley.edu/">UCB’s Archive of Social Science Data (D-Lab)</a></li>
<li><a class="reference external" href="http://dataarchives.ss.ucla.edu/Home.DataPortals.htm">UCLA Social Sciences Data Archive</a></li>
<li><a class="reference external" href="http://esango.un.org/civilsociety/">UN Civil Society Database</a></li>
<li><a class="reference external" href="http://univ.cc/">Universities Worldwide</a></li>
<li><a class="reference external" href="http://www.upjohn.org/services/resources/employment-research-data-center">UPJOHN for Labor Employment Research</a></li>
<li><a class="reference external" href="http://ucdp.uu.se/">Uppsala Conflict Data Program</a></li>
<li><a class="reference external" href="http://data.worldbank.org/">World Bank Open Data</a></li>
<li><a class="reference external" href="http://www.worldpop.org.uk/data/get_data/">WorldPop project - Worldwide human population distributions</a></li>
</ul>
</div>
<div class="section" id="software">
<h3><a class="toc-backref" href="#toc-entry-29">Software</a><a class="headerlink" href="#software" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://flossdata.syr.edu/data/">FLOSSmole data about free, libre, and open source software development</a></li>
</ul>
</div>
<div class="section" id="sports">
<h3><a class="toc-backref" href="#toc-entry-30">Sports</a><a class="headerlink" href="#sports" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://www.draftexpress.com/stats.php">Basketball (NBA/NCAA/Euro) Player Database and Statistics</a></li>
<li><a class="reference external" href="http://data.betfair.com/">Betfair Historical Exchange Data</a></li>
<li><a class="reference external" href="http://cricsheet.org/">Cricsheet Matches (cricket)</a></li>
<li><a class="reference external" href="http://ergast.com/mrd/db">Ergast Formula 1, from 1950 up to date (API)</a></li>
<li><a class="reference external" href="http://www.jokecamp.com/blog/guide-to-football-and-soccer-data-and-apis/">Football/Soccer resources (data and APIs)</a></li>
<li><a class="reference external" href="http://www.seanlahman.com/baseball-archive/statistics/">Lahman’s Baseball Database</a></li>
<li><a class="reference external" href="https://github.com/phillc73/pinhooker">Pinhooker: Thoroughbred Bloodstock Sale Data</a></li>
<li><a class="reference external" href="http://www.retrosheet.org/game.htm">Retrosheet Baseball Statistics</a></li>
<li><a class="reference external" href="https://github.com/JeffSackmann/tennis_atp">Tennis database of rankings, results, and stats for ATP</a>, <a class="reference external" href="https://github.com/JeffSackmann/tennis_wta">WTA</a>, <a class="reference external" href="https://github.com/JeffSackmann/tennis_slam_pointbypoint">Grand Slams</a> and <a class="reference external" href="https://github.com/JeffSackmann/tennis_MatchChartingProject">Match Charting Project</a></li>
</ul>
</div>
<div class="section" id="time-series">
<h3><a class="toc-backref" href="#toc-entry-31">Time Series</a><a class="headerlink" href="#time-series" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://www.cntsdata.com">Databanks International Cross National Time Series Data Archive</a></li>
<li><a class="reference external" href="https://www.backblaze.com/hard-drive-test-data.html">Hard Drive Failure Rates</a></li>
<li><a class="reference external" href="http://ecg.mit.edu/time-series/">Heart Rate Time Series from MIT</a></li>
<li><a class="reference external" href="https://datamarket.com/data/list/?q=provider:tsdl">Time Series Data Library (TSDL) from MU</a></li>
<li><a class="reference external" href="http://www.cs.ucr.edu/~eamonn/time_series_data/">UC Riverside Time Series Dataset</a></li>
</ul>
</div>
<div class="section" id="transportation">
<h3><a class="toc-backref" href="#toc-entry-32">Transportation</a><a class="headerlink" href="#transportation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://stat-computing.org/dataexpo/2009/the-data.html">Airlines OD Data 1987-2008</a></li>
<li><a class="reference external" href="http://www.bayareabikeshare.com/open-data">Bay Area Bike Share Data</a></li>
<li><a class="reference external" href="https://github.com/BetaNYC/Bike-Share-Data-Best-Practices/wiki/Bike-Share-Data-Systems">Bike Share Systems (BSS) collection</a></li>
<li><a class="reference external" href="http://research.microsoft.com/en-us/downloads/b16d359d-d164-469e-9fd4-daa38f2b2e13/">GeoLife GPS Trajectory from Microsoft Research</a></li>
<li><a class="reference external" href="http://data.deutschebahn.com/datasets/">German train system by Deutsche Bahn</a></li>
<li><a class="reference external" href="http://hubwaydatachallenge.org/trip-history-data/">Hubway Million Rides in MA</a></li>
<li><a class="reference external" href="http://www.marinetraffic.com/de/ais-api-services">Marine Traffic - ship tracks, port calls and more</a></li>
<li><a class="reference external" href="https://montreal.bixi.com/en/open-data">Montreal BIXI Bike Share</a></li>
<li><a class="reference external" href="http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml">NYC Taxi Trip Data 2009-</a></li>
<li><a class="reference external" href="https://archive.org/details/nycTaxiTripData2013">NYC Taxi Trip Data 2013 (FOIA/FOILed)</a></li>
<li><a class="reference external" href="https://github.com/fivethirtyeight/uber-tlc-foil-response">NYC Uber trip data April 2014 to September 2014</a></li>
<li><a class="reference external" href="https://github.com/graphhopper/open-traffic-collection">Open Traffic collection</a></li>
<li><a class="reference external" href="http://openflights.org/data.html">OpenFlights - airport, airline and route data</a></li>
<li><a class="reference external" href="https://www.rideindego.com/stations/json/">Philadelphia Bike Share Stations (JSON)</a></li>
<li><a class="reference external" href="http://www.planecrashinfo.com/database.htm">Plane Crash Database, since 1920</a></li>
<li><a class="reference external" href="http://www.transtats.bts.gov/Tables.asp?DB_ID=120">RITA Airline On-Time Performance data</a></li>
<li><a class="reference external" href="http://www.transtats.bts.gov/DataIndex.asp">RITA/BTS transport data collection (TranStat)</a></li>
<li><a class="reference external" href="http://www.bikesharetoronto.com/data/stations/bikeStations.xml">Toronto Bike Share Stations (XML file)</a></li>
<li><a class="reference external" href="https://tfl.gov.uk/info-for/open-data-users/our-open-data">Transport for London (TFL)</a></li>
<li><a class="reference external" href="http://www.cmap.illinois.gov/data/transportation/travel-tracker-survey">Travel Tracker Survey (TTS) for Chicago</a></li>
<li><a class="reference external" href="http://www.rita.dot.gov/bts/">U.S. Bureau of Transportation Statistics (BTS)</a></li>
<li><a class="reference external" href="http://academictorrents.com/details/a2ccf94bbb4af222bf8e69dad60a68a29f310d9a">U.S. Domestic Flights 1990 to 2009</a></li>
<li><a class="reference external" href="http://ops.fhwa.dot.gov/freight/freight_analysis/faf/index.htm">U.S. Freight Analysis Framework since 2007</a></li>
</ul>
</div>
</div>
<span id="document-libraries"></span><div class="section" id="libraries-1">
<span id="libraries"></span><h2>Libraries<a class="headerlink" href="#libraries-1" title="Permalink to this headline">¶</a></h2>
<p>Machine learning libraries and frameworks forked from josephmisti’s <a class="reference external" href="https://github.com/josephmisiti/awesome-machine-learning">awesome machine learning</a>.</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#apl" id="toc-entry-1">APL</a></li>
<li><a class="reference internal" href="#c" id="toc-entry-2">C</a></li>
<li><a class="reference internal" href="#c-1" id="toc-entry-3">C++</a></li>
<li><a class="reference internal" href="#common-lisp" id="toc-entry-4">Common Lisp</a></li>
<li><a class="reference internal" href="#clojure" id="toc-entry-5">Clojure</a></li>
<li><a class="reference internal" href="#elixir" id="toc-entry-6">Elixir</a></li>
<li><a class="reference internal" href="#erlang" id="toc-entry-7">Erlang</a></li>
<li><a class="reference internal" href="#go" id="toc-entry-8">Go</a></li>
<li><a class="reference internal" href="#haskell" id="toc-entry-9">Haskell</a></li>
<li><a class="reference internal" href="#java" id="toc-entry-10">Java</a></li>
<li><a class="reference internal" href="#javascript" id="toc-entry-11">Javascript</a></li>
<li><a class="reference internal" href="#julia" id="toc-entry-12">Julia</a></li>
<li><a class="reference internal" href="#lua" id="toc-entry-13">Lua</a></li>
<li><a class="reference internal" href="#matlab" id="toc-entry-14">Matlab</a></li>
<li><a class="reference internal" href="#net" id="toc-entry-15">.NET</a></li>
<li><a class="reference internal" href="#objective-c" id="toc-entry-16">Objective C</a></li>
<li><a class="reference internal" href="#ocaml" id="toc-entry-17">OCaml</a></li>
<li><a class="reference internal" href="#php" id="toc-entry-18">PHP</a></li>
<li><a class="reference internal" href="#python" id="toc-entry-19">Python</a></li>
<li><a class="reference internal" href="#ruby" id="toc-entry-20">Ruby</a></li>
<li><a class="reference internal" href="#rust" id="toc-entry-21">Rust</a></li>
<li><a class="reference internal" href="#r" id="toc-entry-22">R</a></li>
<li><a class="reference internal" href="#sas" id="toc-entry-23">SAS</a></li>
<li><a class="reference internal" href="#scala" id="toc-entry-24">Scala</a></li>
<li><a class="reference internal" href="#swift" id="toc-entry-25">Swift</a></li>
</ul>
</div>
<div class="section" id="apl">
<h3><a class="toc-backref" href="#toc-entry-1">APL</a><a class="headerlink" href="#apl" title="Permalink to this headline">¶</a></h3>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/mattcunningham/naive-apl">naive-apl</a> - Naive Bayesian Classifier implementation in APL</li>
</ul>
</div>
<div class="section" id="c">
<h3><a class="toc-backref" href="#toc-entry-2">C</a><a class="headerlink" href="#c" title="Permalink to this headline">¶</a></h3>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/pjreddie/darknet">Darknet</a> - Darknet is an open source neural network framework written in C and CUDA. It is fast, easy to install, and supports CPU and GPU computation.</li>
<li><a class="reference external" href="https://github.com/GHamrouni/Recommender">Recommender</a> - A C library for product recommendations/suggestions using collaborative filtering (CF).</li>
<li><a class="reference external" href="https://github.com/SeniorSA/hybrid-rs-trainner">Hybrid Recommender System</a> - A hybrid recomender system based upon scikit-learn algorithms.</li>
</ul>
<p class="rubric">Computer Vision</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/liuliu/ccv">CCV</a> - C-based/Cached/Core Computer Vision Library, A Modern Computer Vision Library</li>
<li><a class="reference external" href="http://www.vlfeat.org/">VLFeat</a> - VLFeat is an open and portable library of computer vision algorithms, which has Matlab toolbox</li>
</ul>
<p class="rubric">Speech Recognition</p>
<ul class="simple">
<li><a class="reference external" href="http://htk.eng.cam.ac.uk/">HTK</a> -The Hidden Markov Model Toolkit. HTK is a portable toolkit for building and manipulating hidden Markov models.</li>
</ul>
</div>
<div class="section" id="c-1">
<h3><a class="toc-backref" href="#toc-entry-3">C++</a><a class="headerlink" href="#c-1" title="Permalink to this headline">¶</a></h3>
<p class="rubric">Computer Vision</p>
<ul class="simple">
<li><a class="reference external" href="http://dlib.net/imaging.html">DLib</a> - DLib has C++ and Python interfaces for face detection and training general object detectors.</li>
<li><a class="reference external" href="http://eblearn.sourceforge.net/">EBLearn</a> - Eblearn is an object-oriented C++ library that implements various machine learning models</li>
<li><a class="reference external" href="http://opencv.org">OpenCV</a> - OpenCV has C++, C, Python, Java and MATLAB interfaces and supports Windows, Linux, Android and Mac OS.</li>
<li><a class="reference external" href="https://github.com/ukoethe/vigra">VIGRA</a> - VIGRA is a generic cross-platform C++ computer vision and machine learning library for volumes of arbitrary dimensionality with Python bindings.</li>
</ul>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/jkomiyama/banditlib">BanditLib</a> - A simple Multi-armed Bandit library.</li>
<li><a class="reference external" href="http://caffe.berkeleyvision.org">Caffe</a>  - A deep learning framework developed with cleanliness, readability, and speed in mind. [DEEP LEARNING]</li>
<li><a class="reference external" href="https://github.com/Microsoft/CNTK)-TheComputationalNetworkToolkit(CNTK">CNTK</a> by Microsoft Research, is a unified deep-learning toolkit that describes neural networks as a series of computational steps via a directed graph.</li>
<li><a class="reference external" href="https://code.google.com/p/cuda-convnet/">CUDA</a> - This is a fast C++/CUDA implementation of convolutional [DEEP LEARNING]</li>
<li><a class="reference external" href="https://github.com/antinucleon/cxxnet">CXXNET</a> - Yet another deep learning framework with less than 1000 lines core code [DEEP LEARNING]</li>
<li><a class="reference external" href="https://github.com/beniz/deepdetect">DeepDetect</a> - A machine learning API and server written in C++11. It makes state of the art machine learning easy to work with and integrate into existing applications.</li>
<li><a class="reference external" href="http://www.dmtk.io/)-Adistributedmachinelearning(parameterserver)frameworkbyMicrosoft.Enablestrainingmodelsonlargedatasetsacrossmultiplemachines.Currenttoolsbundledwithitinclude:LightLDAandDistributed(Multisense">Disrtibuted Machine learning Tool Kit (DMTK)</a> Word Embedding.</li>
<li><a class="reference external" href="http://dlib.net/ml.html">DLib</a> - A suite of ML tools designed to be easy to imbed in other applications</li>
<li><a class="reference external" href="https://github.com/amznlabs/amazon-dsstne">DSSTNE</a> - A software library created by Amazon for training and deploying deep neural networks using GPUs which emphasizes speed and scale over experimental flexibility.</li>
<li><a class="reference external" href="https://github.com/clab/dynet">DyNet</a> - A dynamic neural network library working well with networks that have dynamic structures that change for every training instance. Written in C++ with bindings in Python.</li>
<li><a class="reference external" href="https://code.google.com/archive/p/encog-cpp">encog-cpp</a></li>
<li><a class="reference external" href="https://github.com/FidoProject/Fido">Fido</a> - A highly-modular C++ machine learning library for embedded electronics and robotics.</li>
<li><a class="reference external" href="http://igraph.org/c/">igraph</a> - General purpose graph library</li>
<li><a class="reference external" href="https://github.com/01org/daal">Intel(R) DAAL</a> - A high performance software library developed by Intel and optimized for Intel’s architectures. Library provides algorithmic building blocks for all stages of data analytics and allows to process data in batch, online and distributed modes.</li>
<li><a class="reference external" href="https://github.com/Microsoft/LightGBM)-Microsoft'sfast,distributed,highperformancegradientboosting(GBDT,GBRT,GBMorMART">LightGBM</a> framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.</li>
<li><a class="reference external" href="https://mldb.ai">MLDB</a> - The Machine Learning Database is a database designed for machine learning. Send it commands over a RESTful API to store data, explore it using SQL, then train machine learning models and expose them as APIs.</li>
<li><a class="reference external" href="http://www.mlpack.org/">mlpack</a> - A scalable C++ machine learning library</li>
<li><a class="reference external" href="https://root.cern.ch">ROOT</a> - A modular scientific software framework. It provides all the functionalities needed to deal with big data processing, statistical analysis, visualization and storage.</li>
<li><a class="reference external" href="http://image.diku.dk/shark/sphinx_pages/build/html/index.html">shark</a> - A fast, modular, feature-rich open-source C++ machine learning library.</li>
<li><a class="reference external" href="https://github.com/shogun-toolbox/shogun">Shogun</a> - The Shogun Machine Learning Toolbox</li>
<li><a class="reference external" href="https://code.google.com/archive/p/sofia-ml">sofia-ml</a> - Suite of fast incremental algorithms.</li>
<li><a class="reference external" href="http://mc-stan.org/">Stan</a> - A probabilistic programming language implementing full Bayesian statistical inference with Hamiltonian Monte Carlo sampling</li>
<li><a class="reference external" href="https://languagemachines.github.io/timbl/">Timbl</a> - A software package/C++ library implementing several memory-based learning algorithms, among which IB1-IG, an implementation of k-nearest neighbor classification, and IGTree, a decision-tree approximation of IB1-IG. Commonly used for NLP.</li>
<li><a class="reference external" href="https://github.com/JohnLangford/vowpal_wabbit/wiki">Vowpal Wabbit (VW)</a> - A fast out-of-core learning system.</li>
<li><a class="reference external" href="https://github.com/baidu-research/warp-ctc)-AfastparallelimplementationofConnectionistTemporalClassification(CTC">Warp-CTC</a>, on both CPU and GPU.</li>
<li><a class="reference external" href="https://github.com/dmlc/xgboost">XGBoost</a> - A parallelized optimized general purpose gradient boosting library.</li>
</ul>
<p class="rubric">Natural Language Processing</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/BLLIP/bllip-parser)-BLLIPNaturalLanguageParser(alsoknownastheCharniak-Johnsonparser">BLLIP Parser</a></li>
<li><a class="reference external" href="https://github.com/proycon/colibri-core">colibri-core</a> - C++ library, command line tools, and Python binding for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.</li>
<li><a class="reference external" href="https://taku910.github.io/crfpp/)-OpensourceimplementationofConditionalRandomFields(CRFs">CRF++</a> for segmenting/labeling sequential data &amp; other Natural Language Processing tasks.</li>
<li><a class="reference external" href="http://www.chokkan.org/software/crfsuite/)-CRFsuiteisanimplementationofConditionalRandomFields(CRFs">CRFsuite</a> for labeling sequential data.</li>
<li><a class="reference external" href="https://github.com/LanguageMachines/frog">frog</a> - Memory-based NLP suite developed for Dutch: PoS tagger, lemmatiser, dependency parser, NER, shallow parser, morphological analyzer.</li>
<li><a class="reference external" href="http://proycon.github.io/folia/">libfolia](https://github.com/LanguageMachines/libfolia) - C++ library for the [FoLiA format</a></li>
<li><a class="reference external" href="https://meta-toolkit.org/">MeTA](https://github.com/meta-toolkit/meta) - [MeTA : ModErn Text Analysis</a> is a C++ Data Sciences Toolkit that facilitates mining big text data.</li>
<li><a class="reference external" href="https://github.com/mit-nlp/MITIE">MIT Information Extraction Toolkit</a> - C, C++, and Python tools for named entity recognition and relation extraction</li>
<li><a class="reference external" href="https://github.com/LanguageMachines/ucto">ucto</a> - Unicode-aware regular-expression based tokenizer for various languages. Tool and C++ library. Supports FoLiA format.</li>
</ul>
<p class="rubric">Speech Recognition</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/kaldi-asr/kaldi">Kaldi</a> - Kaldi is a toolkit for speech recognition written in C++ and licensed under the Apache License v2.0. Kaldi is intended for use by speech recognition researchers.</li>
</ul>
<p class="rubric">Sequence Analysis</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/ayoshiaki/tops">ToPS</a> - This is an objected-oriented framework that facilitates the integration of probabilistic models for sequences over a user defined alphabet.</li>
</ul>
<p class="rubric">Gesture Detection</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/nickgillian/grt">grt</a> - The Gesture Recognition Toolkit. GRT is a cross-platform, open-source, C++ machine learning library designed for real-time gesture recognition.</li>
</ul>
</div>
<div class="section" id="common-lisp">
<h3><a class="toc-backref" href="#toc-entry-4">Common Lisp</a><a class="headerlink" href="#common-lisp" title="Permalink to this headline">¶</a></h3>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/melisgl/mgl/)-Neuralnetworks(boltzmannmachines,feed-forwardandrecurrentnets">mgl</a>, Gaussian Processes</li>
<li><a class="reference external" href="https://github.com/melisgl/mgl-gpr/">mgl-gpr</a> - Evolutionary algorithms</li>
<li><a class="reference external" href="https://github.com/melisgl/cl-libsvm/">cl-libsvm</a> - Wrapper for the libsvm support vector machine library</li>
</ul>
</div>
<div class="section" id="clojure">
<h3><a class="toc-backref" href="#toc-entry-5">Clojure</a><a class="headerlink" href="#clojure" title="Permalink to this headline">¶</a></h3>
<p class="rubric">Natural Language Processing</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/dakrone/clojure-opennlp">Clojure-openNLP</a> - Natural Language Processing in Clojure (opennlp)</li>
<li><a class="reference external" href="https://github.com/r0man/inflections-clj">Infections-clj</a> - Rails-like inflection library for Clojure and ClojureScript</li>
</ul>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/ptaoussanis/touchstone">Touchstone</a> - Clojure A/B testing library</li>
<li><a class="reference external" href="https://github.com/lspector/Clojush">Clojush</a> -  The Push programming language and the PushGP genetic programming system implemented in Clojure</li>
<li><a class="reference external" href="https://github.com/aria42/infer">Infer</a> - Inference and machine learning in clojure</li>
<li><a class="reference external" href="https://github.com/antoniogarrote/clj-ml">Clj-ML</a> - A machine learning library for Clojure built on top of Weka and friends</li>
<li><a class="reference external" href="https://github.com/engagor/dl4clj/">DL4CLJ</a> - Clojure wrapper for Deeplearning4j</li>
<li><a class="reference external" href="https://github.com/jimpil/enclog)-ClojurewrapperforEncog(v3)(Machine-Learningframeworkthatspecializesinneural-nets">Encog</a></li>
<li><a class="reference external" href="https://github.com/vollmerm/fungp">Fungp</a> - A genetic programming library for Clojure</li>
<li><a class="reference external" href="https://github.com/clojurewerkz/statistiker">Statistiker</a> - Basic Machine Learning algorithms in Clojure.</li>
<li><a class="reference external" href="https://github.com/htm-community/clortex">clortex</a> - General Machine Learning library using Numenta’s Cortical Learning Algorithm</li>
<li><a class="reference external" href="https://github.com/htm-community/comportex">comportex</a> - Functionally composable Machine Learning library using Numenta’s Cortical Learning Algorithm</li>
<li><a class="reference external" href="https://github.com/thinktopic/cortex">cortex</a> - Neural networks, regression and feature learning in Clojure.</li>
<li><a class="reference external" href="https://github.com/cloudkj/lambda-ml">lambda-ml</a> - Simple, concise implementations of machine learning techniques and utilities in Clojure.</li>
</ul>
<p class="rubric">Data Analysis / Data Visualization</p>
<ul class="simple">
<li><a class="reference external" href="http://incanter.org/">Incanter</a> - Incanter is a Clojure-based, R-like platform for statistical computing and graphics.</li>
<li><a class="reference external" href="https://github.com/Netflix/PigPen">PigPen</a> - Map-Reduce for Clojure.</li>
<li><a class="reference external" href="https://github.com/clojurewerkz/envision">Envision</a> - Clojure Data Visualisation library, based on Statistiker and D3</li>
</ul>
</div>
<div class="section" id="elixir">
<h3><a class="toc-backref" href="#toc-entry-6">Elixir</a><a class="headerlink" href="#elixir" title="Permalink to this headline">¶</a></h3>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/fredwu/simple_bayes">Simple Bayes</a> - A Simple Bayes / Naive Bayes implementation in Elixir.</li>
</ul>
<p class="rubric">Natural Language Processing</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/fredwu/stemmer)-AnEnglish(Porter2">Stemmer</a> stemming implementation in Elixir.</li>
</ul>
</div>
<div class="section" id="erlang">
<h3><a class="toc-backref" href="#toc-entry-7">Erlang</a><a class="headerlink" href="#erlang" title="Permalink to this headline">¶</a></h3>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/discoproject/disco/">Disco</a> - Map Reduce in Erlang</li>
</ul>
</div>
<div class="section" id="go">
<h3><a class="toc-backref" href="#toc-entry-8">Go</a><a class="headerlink" href="#go" title="Permalink to this headline">¶</a></h3>
<p class="rubric">Natural Language Processing</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/reiver/go-porterstemmer">go-porterstemmer</a> - A native Go clean room implementation of the Porter Stemming algorithm.</li>
<li><a class="reference external" href="https://github.com/Rookii/paicehusk">paicehusk</a> - Golang implementation of the Paice/Husk Stemming Algorithm.</li>
<li><a class="reference external" href="https://github.com/tebeka/snowball">snowball</a> - Snowball Stemmer for Go.</li>
<li><a class="reference external" href="https://github.com/Lazin/go-ngram">go-ngram</a> - In-memory n-gram index with compression.</li>
</ul>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/MaxHalford/gago">gago</a> - Multi-population, flexible, parallel genetic algorithm.</li>
<li><a class="reference external" href="https://github.com/sjwhitworth/golearn">Go Learn</a> - Machine Learning for Go</li>
<li><a class="reference external" href="https://github.com/daviddengcn/go-pr">go-pr</a> - Pattern recognition package in Go lang.</li>
<li><a class="reference external" href="https://github.com/alonsovidales/go_ml">go-ml</a> - Linear / Logistic regression, Neural Networks, Collaborative Filtering and Gaussian Multivariate Distribution</li>
<li><a class="reference external" href="https://github.com/jbrukh/bayesian">bayesian</a> - Naive Bayesian Classification for Golang.</li>
<li><a class="reference external" href="https://github.com/thoj/go-galib">go-galib</a> - Genetic Algorithms library written in Go / golang</li>
<li><a class="reference external" href="https://github.com/ryanbressler/CloudForest">Cloudforest</a> - Ensembles of decision trees in go/golang.</li>
<li><a class="reference external" href="https://github.com/goml/gobrain">gobrain</a> - Neural Networks written in go</li>
<li><a class="reference external" href="https://github.com/fxsjy/gonn">GoNN</a> - GoNN is an implementation of Neural Network in Go Language, which includes BPNN, RBF, PCN</li>
<li><a class="reference external" href="https://github.com/dmlc/mxnet">MXNet</a> - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.</li>
<li><a class="reference external" href="https://github.com/songtianyi/go-mxnet-predictor">go-mxnet-predictor</a> - Go binding for MXNet c_predict_api to do inference with pre-trained model</li>
</ul>
<p class="rubric">Data Analysis / Data Visualization</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/StepLg/go-graph">go-graph</a> - Graph library for Go/golang language.</li>
<li><a class="reference external" href="http://www.svgopen.org/2011/papers/34-SVGo_a_Go_Library_for_SVG_generation/">SVGo</a> - The Go Language library for SVG generation</li>
<li><a class="reference external" href="https://github.com/fxsjy/RF.go">RF</a> - Random forests implementation in Go</li>
</ul>
</div>
<div class="section" id="haskell">
<h3><a class="toc-backref" href="#toc-entry-9">Haskell</a><a class="headerlink" href="#haskell" title="Permalink to this headline">¶</a></h3>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/ajtulloch/haskell-ml">haskell-ml</a> - Haskell implementations of various ML algorithms.</li>
<li><a class="reference external" href="https://github.com/mikeizbicki/HLearn">HLearn</a> - a suite of libraries for interpreting machine learning models according to their algebraic structure.</li>
<li><a class="reference external" href="https://wiki.haskell.org/HNN">hnn</a> - Haskell Neural Network library.</li>
<li><a class="reference external" href="https://github.com/ajtulloch/hopfield-networks">hopfield-networks</a> - Hopfield Networks for unsupervised learning in Haskell.</li>
<li><a class="reference external" href="https://github.com/ajtulloch/dnngraph">caffegraph</a> - A DSL for deep neural networks</li>
<li><a class="reference external" href="https://github.com/jbarrow/LambdaNet">LambdaNet</a> - Configurable Neural Networks in Haskell</li>
</ul>
</div>
<div class="section" id="java">
<h3><a class="toc-backref" href="#toc-entry-10">Java</a><a class="headerlink" href="#java" title="Permalink to this headline">¶</a></h3>
<p class="rubric">Natural Language Processing</p>
<ul class="simple">
<li><a class="reference external" href="http://www.cortical.io/)-Retina:anAPIperformingcomplexNLPoperations(disambiguation,classification,streamingtextfiltering,etc...">Cortical.io</a> as quickly and intuitively as the brain.</li>
<li><a class="reference external" href="http://nlp.stanford.edu/software/corenlp.shtml">CoreNLP</a> - Stanford CoreNLP provides a set of natural language analysis tools which can take raw English language text input and give the base forms of words</li>
<li><a class="reference external" href="http://nlp.stanford.edu/software/lex-parser.shtml">Stanford Parser</a> - A natural language parser is a program that works out the grammatical structure of sentences</li>
<li><a class="reference external" href="http://nlp.stanford.edu/software/tagger.shtml">Stanford POS Tagger</a> - A Part-Of-Speech Tagger (POS Tagger</li>
<li><a class="reference external" href="http://nlp.stanford.edu/software/CRF-NER.shtml">Stanford Name Entity Recognizer</a> - Stanford NER is a Java implementation of a Named Entity Recognizer.</li>
<li><a class="reference external" href="http://nlp.stanford.edu/software/segmenter.shtml">Stanford Word Segmenter</a> - Tokenization of raw text is a standard pre-processing step for many NLP tasks.</li>
<li><a class="reference external" href="http://nlp.stanford.edu/software/tregex.shtml)-Tregexisautilityformatchingpatternsintrees,basedontreerelationshipsandregularexpressionmatchesonnodes(thenameisshortfor&quot;treeregularexpressions&quot;">Tregex, Tsurgeon and Semgrex</a>.</li>
<li><a class="reference external" href="http://nlp.stanford.edu/phrasal/">Stanford Phrasal: A Phrase-Based Translation System</a></li>
<li><a class="reference external" href="http://nlp.stanford.edu/software/tokenizer.shtml">Stanford English Tokenizer</a> - Stanford Phrasal is a state-of-the-art statistical phrase-based machine translation system, written in Java.</li>
<li><a class="reference external" href="http://nlp.stanford.edu/software/tokensregex.shtml">Stanford Tokens Regex</a> - A tokenizer divides text into a sequence of tokens, which roughly correspond to “words”</li>
<li><a class="reference external" href="http://nlp.stanford.edu/software/sutime.shtml">Stanford Temporal Tagger</a> - SUTime is a library for recognizing and normalizing time expressions.</li>
<li><a class="reference external" href="http://nlp.stanford.edu/software/patternslearning.shtml">Stanford SPIED</a> - Learning entities from unlabeled text starting with seed sets using patterns in an iterative fashion</li>
<li><a class="reference external" href="http://nlp.stanford.edu/software/tmt/tmt-0.4/">Stanford Topic Modeling Toolbox</a> - Topic modeling tools to social scientists and others who wish to perform analysis on datasets</li>
<li><a class="reference external" href="https://github.com/twitter/twitter-text-java">Twitter Text Java</a> - A Java implementation of Twitter’s text processing library</li>
<li><a class="reference external" href="http://mallet.cs.umass.edu/">MALLET</a> - A Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.</li>
<li><a class="reference external" href="https://opennlp.apache.org/">OpenNLP</a> - a machine learning based toolkit for the processing of natural language text.</li>
<li><a class="reference external" href="http://alias-i.com/lingpipe/index.html">LingPipe</a> - A tool kit for processing text using computational linguistics.</li>
<li><a class="reference external" href="https://code.google.com/archive/p/cleartk)-ClearTKprovidesaframeworkfordevelopingstatisticalnaturallanguageprocessing(NLP">ClearTK</a> components in Java and is built on top of Apache UIMA.</li>
<li><a class="reference external" href="http://ctakes.apache.org/)-ApacheclinicalTextAnalysisandKnowledgeExtractionSystem(cTAKES">Apache cTAKES</a> is an open-source natural language processing system for information extraction from electronic medical record clinical free-text.</li>
<li><a class="reference external" href="https://github.com/clir/clearnlp">ClearNLP</a> - The ClearNLP project provides software and resources for natural language processing. The project started at the Center for Computational Language and EducAtion Research, and is currently developed by the Center for Language and Information Research at Emory University. This project is under the Apache 2 license.</li>
<li><a class="reference external" href="https://github.com/IllinoisCogComp/illinois-cogcomp-nlp)-ThisprojectcollectsanumberofcorelibrariesforNaturalLanguageProcessing(NLP">CogcompNLP</a> developed in the University of Illinois’ Cognitive Computation Group, for example <cite>illinois-core-utilities</cite> which provides a set of NLP-friendly data structures and a number of NLP-related utilities that support writing NLP applications, running experiments, etc, <cite>illinois-edison</cite> a library for feature extraction from illinois-core-utilities data structures and many other packages.</li>
</ul>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/airbnb/aerosolve">aerosolve</a> - A machine learning library by Airbnb designed from the ground up to be human friendly.</li>
<li><a class="reference external" href="https://github.com/datumbox/datumbox-framework">Datumbox</a> - Machine Learning framework for rapid development of Machine Learning and Statistical applications</li>
<li><a class="reference external" href="https://elki-project.github.io/)-Javatoolkitfordatamining.(unsupervised:clustering,outlierdetectionetc.">ELKI</a></li>
<li><a class="reference external" href="https://github.com/encog/encog-java-core">Encog</a> - An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.</li>
<li><a class="reference external" href="https://ci.apache.org/projects/flink/flink-docs-master/apis/batch/libs/ml/index.html">FlinkML in Apache Flink</a> - Distributed machine learning library in Flink</li>
<li><a class="reference external" href="https://github.com/h2oai/h2o-3">H2O</a> - ML engine that supports distributed learning on Hadoop, Spark or your laptop via APIs in R, Python, Scala, REST/JSON.</li>
<li><a class="reference external" href="https://github.com/numenta/htm.java">htm.java</a> - General Machine Learning library using Numenta’s Cortical Learning Algorithm</li>
<li><a class="reference external" href="https://github.com/deeplearning4j/deeplearning4j">java-deeplearning</a> - Distributed Deep Learning Platform for Java, Clojure,Scala</li>
<li><a class="reference external" href="https://github.com/apache/mahout">Mahout</a> - Distributed machine learning</li>
<li><a class="reference external" href="http://meka.sourceforge.net/)-Anopensourceimplementationofmethodsformulti-labelclassificationandevaluation(extensiontoWeka">Meka</a>.</li>
<li><a class="reference external" href="http://spark.apache.org/docs/latest/mllib-guide.html">MLlib in Apache Spark</a> - Distributed machine learning library in Spark</li>
<li><a class="reference external" href="https://github.com/Hydrospheredata/mist">Hydrosphere Mist</a> - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.</li>
<li><a class="reference external" href="http://neuroph.sourceforge.net/">Neuroph</a> - Neuroph is lightweight Java neural network framework</li>
<li><a class="reference external" href="https://github.com/oryxproject/oryx">ORYX</a> - Lambda Architecture Framework using Apache Spark and Apache Kafka with a specialization for real-time large-scale machine learning.</li>
<li><a class="reference external" href="https://samoa.incubator.apache.org/">Samoa</a> SAMOA is a framework that includes distributed machine learning for data streams with an interface to plug-in different stream processing platforms.</li>
<li><a class="reference external" href="https://sourceforge.net/p/lemur/wiki/RankLib/">RankLib</a> - RankLib is a library of learning to rank algorithms</li>
<li><a class="reference external" href="https://github.com/padreati/rapaio">rapaio</a> - statistics, data mining and machine learning toolbox in Java</li>
<li><a class="reference external" href="https://rapidminer.com">RapidMiner</a> - RapidMiner integration into Java code</li>
<li><a class="reference external" href="http://nlp.stanford.edu/software/classifier.shtml">Stanford Classifier</a> - A classifier is a machine learning tool that will take data items and place them into one of k classes.</li>
<li><a class="reference external" href="https://github.com/haifengl/smile">SmileMiner</a> - Statistical Machine Intelligence &amp; Learning Engine</li>
<li><a class="reference external" href="https://github.com/apache/incubator-systemml)-flexible,scalablemachinelearning(ML">SystemML</a> language.</li>
<li><a class="reference external" href="https://github.com/WalnutiQ/wAlnut">WalnutiQ</a> - object oriented model of the human brain</li>
<li><a class="reference external" href="http://www.cs.waikato.ac.nz/ml/weka/">Weka</a> - Weka is a collection of machine learning algorithms for data mining tasks</li>
<li><a class="reference external" href="https://github.com/IllinoisCogComp/lbjava/">LBJava</a> - Learning Based Java is a modeling language for the rapid development of software systems, offers a convenient, declarative syntax for classifier and constraint definition directly in terms of the objects in the programmer’s application.</li>
</ul>
<p class="rubric">Speech Recognition</p>
<ul class="simple">
<li><a class="reference external" href="http://cmusphinx.sourceforge.net/">CMU Sphinx</a> - Open Source Toolkit For Speech Recognition purely based on Java speech recognition library.</li>
</ul>
<p class="rubric">Data Analysis / Data Visualization</p>
<ul class="simple">
<li><a class="reference external" href="http://flink.apache.org/">Flink</a> - Open source platform for distributed stream and batch data processing.</li>
<li><a class="reference external" href="https://github.com/apache/hadoop-mapreduce">Hadoop</a> - Hadoop/HDFS</li>
<li><a class="reference external" href="https://github.com/apache/spark">Spark</a> - Spark is a fast and general engine for large-scale data processing.</li>
<li><a class="reference external" href="http://storm.apache.org/">Storm</a> - Storm is a distributed realtime computation system.</li>
<li><a class="reference external" href="https://github.com/cloudera/impala">Impala</a> - Real-time Query for Hadoop</li>
<li><a class="reference external" href="http://jwork.org/dmelt/">DataMelt</a> - Mathematics software for numeric computation, statistics, symbolic calculations, data analysis and data visualization.</li>
<li><a class="reference external" href="http://www.ee.ucl.ac.uk/~mflanaga/java/">Dr. Michael Thomas Flanagan’s Java Scientific Library</a></li>
</ul>
<p class="rubric">Deep Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/deeplearning4j/deeplearning4j">Deeplearning4j</a> - Scalable deep learning for industry with parallel GPUs</li>
</ul>
</div>
<div class="section" id="javascript">
<h3><a class="toc-backref" href="#toc-entry-11">Javascript</a><a class="headerlink" href="#javascript" title="Permalink to this headline">¶</a></h3>
<p class="rubric">Natural Language Processing</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/twitter/twitter-text">Twitter-text</a> - A JavaScript implementation of Twitter’s text processing library</li>
<li><a class="reference external" href="https://github.com/nicktesla/nlpjs">NLP.js</a> - NLP utilities in javascript and coffeescript</li>
<li><a class="reference external" href="https://github.com/NaturalNode/natural">natural</a> - General natural language facilities for node</li>
<li><a class="reference external" href="https://github.com/loadfive/Knwl.js">Knwl.js</a> - A Natural Language Processor in JS</li>
<li><a class="reference external" href="https://github.com/wooorm/retext">Retext</a> - Extensible system for analyzing and manipulating natural language</li>
<li><a class="reference external" href="https://market.mashape.com/japerk/text-processing/support">TextProcessing</a> - Sentiment analysis, stemming and lemmatization, part-of-speech tagging and chunking, phrase extraction and named entity recognition.</li>
<li><a class="reference external" href="https://github.com/nlp-compromise/compromise">NLP Compromise</a> - Natural Language processing in the browser</li>
</ul>
<p class="rubric">Data Analysis / Data Visualization</p>
<ul class="simple">
<li><a class="reference external" href="https://d3js.org/">D3.js</a></li>
<li><a class="reference external" href="http://www.highcharts.com/">High Charts</a></li>
<li><a class="reference external" href="http://nvd3.org/">NVD3.js</a></li>
<li><a class="reference external" href="http://dc-js.github.io/dc.js/">dc.js</a></li>
<li><a class="reference external" href="http://www.chartjs.org/">chartjs</a></li>
<li><a class="reference external" href="http://dimplejs.org/">dimple</a></li>
<li><a class="reference external" href="https://www.amcharts.com/">amCharts</a></li>
<li><a class="reference external" href="https://github.com/NathanEpstein/D3xter">D3xter</a> - Straight forward plotting built on D3</li>
<li><a class="reference external" href="https://github.com/rigtorp/statkit">statkit</a> - Statistics kit for JavaScript</li>
<li><a class="reference external" href="https://github.com/nathanepstein/datakit">datakit</a> - A lightweight framework for data analysis in JavaScript</li>
<li><a class="reference external" href="https://github.com/jasondavies/science.js/">science.js</a> - Scientific and statistical computing in JavaScript.</li>
<li><a class="reference external" href="https://github.com/NathanEpstein/Z3d">Z3d</a> - Easily make interactive 3d plots built on Three.js</li>
<li><a class="reference external" href="http://sigmajs.org/">Sigma.js</a> - JavaScript library dedicated to graph drawing.</li>
<li><a class="reference external" href="http://c3js.org/">C3.js</a>- customizable library based on D3.js for easy chart drawing.</li>
<li><a class="reference external" href="http://datamaps.github.io/">Datamaps</a>- Customizable SVG map/geo visualizations using D3.js.</li>
<li><a class="reference external" href="https://www.zingchart.com/">ZingChart</a>- library written on Vanilla JS for big data visualization.</li>
<li><a class="reference external" href="http://www.cheminfo.org/">cheminfo</a> - Platform for data visualization and analysis, using the <a class="reference external" href="https://github.com/npellet/visualizer">visualizer</a> project.</li>
</ul>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="http://cs.stanford.edu/people/karpathy/convnetjs/">Convnet.js</a> - ConvNetJS is a Javascript library for training Deep Learning models[DEEP LEARNING]</li>
<li><a class="reference external" href="http://harthur.github.io/clusterfck/">Clusterfck</a> - Agglomerative hierarchical clustering implemented in Javascript for Node.js and the browser</li>
<li><a class="reference external" href="https://github.com/emilbayes/clustering.js">Clustering.js</a> - Clustering algorithms implemented in Javascript for Node.js and the browser</li>
<li><a class="reference external" href="https://github.com/serendipious/nodejs-decision-tree-id3">Decision Trees</a> - NodeJS Implementation of Decision Tree using ID3 Algorithm</li>
<li><a class="reference external" href="https://github.com/dn2a/dn2a-javascript">DN2A</a> - Digital Neural Networks Architecture</li>
<li><a class="reference external" href="https://code.google.com/archive/p/figue">figue</a> - K-means, fuzzy c-means and agglomerative clustering</li>
<li><a class="reference external" href="https://github.com/rlidwka/node-fann)-FANN(FastArtificialNeuralNetworkLibrary">Node-fann</a> bindings for Node.js</li>
<li><a class="reference external" href="https://github.com/emilbayes/kMeans.js">Kmeans.js</a> - Simple Javascript implementation of the k-means algorithm, for node.js and the browser</li>
<li><a class="reference external" href="https://github.com/primaryobjects/lda">LDA.js</a> - LDA topic modeling for node.js</li>
<li><a class="reference external" href="https://github.com/yandongliu/learningjs">Learning.js</a> - Javascript implementation of logistic regression/c4.5 decision tree</li>
<li><a class="reference external" href="http://joonku.com/project/machine_learning">Machine Learning</a> - Machine learning library for Node.js</li>
<li><a class="reference external" href="https://github.com/ClimbsRocks/machineJS">machineJS</a> - Automated machine learning, data formatting, ensembling, and hyperparameter optimization for competitions and exploration- just give it a .csv file!</li>
<li><a class="reference external" href="https://github.com/mil-tokyo">mil-tokyo</a> - List of several machine learning libraries</li>
<li><a class="reference external" href="https://github.com/nicolaspanel/node-svm">Node-SVM</a> - Support Vector Machine for nodejs</li>
<li><a class="reference external" href="https://github.com/harthur/brain">Brain</a> - Neural networks in JavaScript <strong>[Deprecated]</strong></li>
<li><a class="reference external" href="https://github.com/omphalos/bayesian-bandit.js">Bayesian-Bandit</a> - Bayesian bandit implementation for Node and the browser.</li>
<li><a class="reference external" href="https://github.com/cazala/synaptic">Synaptic</a> - Architecture-free neural network library for node.js and the browser</li>
<li><a class="reference external" href="https://github.com/NathanEpstein/kNear">kNear</a> - JavaScript implementation of the k nearest neighbors algorithm for supervised learning</li>
<li><a class="reference external" href="https://github.com/totemstech/neuraln">NeuralN</a> - C++ Neural Network library for Node.js. It has advantage on large dataset and multi-threaded training.</li>
<li><a class="reference external" href="https://github.com/itamarwe/kalman">kalman</a> - Kalman filter for Javascript.</li>
<li><a class="reference external" href="https://github.com/luccastera/shaman">shaman</a> - node.js library with support for both simple and multiple linear regression.</li>
<li><a class="reference external" href="https://github.com/mljs/ml">ml.js</a> - Machine learning and numerical analysis tools for Node.js and the Browser!</li>
<li><a class="reference external" href="https://github.com/NathanEpstein/Pavlov.js">Pavlov.js</a> - Reinforcement learning using Markov Decision Processes</li>
<li><a class="reference external" href="https://github.com/dmlc/mxnet">MXNet</a> - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.</li>
</ul>
<p class="rubric">Misc</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/jcoglan/sylvester">sylvester</a> - Vector and Matrix math for JavaScript.</li>
<li><a class="reference external" href="https://github.com/simple-statistics/simple-statistics)-AJavaScriptimplementationofdescriptive,regression,andinferencestatistics.ImplementedinliterateJavaScriptwithnodependencies,designedtoworkinallmodernbrowsers(includingIE">simple-statistics</a> as well as in node.js.</li>
<li><a class="reference external" href="https://github.com/Tom-Alexander/regression-js">regression-js</a> - A javascript library containing a collection of least squares fitting methods for finding a trend in a set of data.</li>
<li><a class="reference external" href="https://github.com/flurry/Lyric">Lyric</a> - Linear Regression library.</li>
<li><a class="reference external" href="https://github.com/mwgg/GreatCircle">GreatCircle</a> - Library for calculating great circle distance.</li>
</ul>
</div>
<div class="section" id="julia">
<h3><a class="toc-backref" href="#toc-entry-12">Julia</a><a class="headerlink" href="#julia" title="Permalink to this headline">¶</a></h3>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/benhamner/MachineLearning.jl">MachineLearning</a> - Julia Machine Learning library</li>
<li><a class="reference external" href="https://github.com/JuliaStats/MLBase.jl">MLBase</a> - A set of functions to support the development of machine learning algorithms</li>
<li><a class="reference external" href="https://github.com/JuliaStats/PGM.jl">PGM</a> - A Julia framework for probabilistic graphical models.</li>
<li><a class="reference external" href="https://github.com/trthatcher/DiscriminantAnalysis.jl">DA</a> - Julia package for Regularized Discriminant Analysis</li>
<li><a class="reference external" href="https://github.com/lindahua/Regression.jl)-Algorithmsforregressionanalysis(e.g.linearregressionandlogisticregression">Regression</a></li>
<li><a class="reference external" href="https://github.com/JuliaStats/Loess.jl">Local Regression</a> - Local regression, so smooooth!</li>
<li><a class="reference external" href="https://github.com/nutsiepully/NaiveBayes.jl">Naive Bayes</a> - Simple Naive Bayes implementation in Julia</li>
<li><a class="reference external" href="https://github.com/dmbates/MixedModels.jl)-AJuliapackageforfitting(statistical">Mixed Models</a> mixed-effects models</li>
<li><a class="reference external" href="https://github.com/fredo-dedup/SimpleMCMC.jl">Simple MCMC</a> - basic mcmc sampler implemented in Julia</li>
<li><a class="reference external" href="https://github.com/JuliaStats/Distance.jl">Distance</a> - Julia module for Distance evaluation</li>
<li><a class="reference external" href="https://github.com/bensadeghi/DecisionTree.jl">Decision Tree</a> - Decision Tree Classifier and Regressor</li>
<li><a class="reference external" href="https://github.com/compressed/BackpropNeuralNet.jl">Neural</a> - A neural network in Julia</li>
<li><a class="reference external" href="https://github.com/doobwa/MCMC.jl">MCMC</a> - MCMC tools for Julia</li>
<li><a class="reference external" href="https://github.com/brian-j-smith/Mamba.jl)-MarkovchainMonteCarlo(MCMC">Mamba</a> for Bayesian analysis in Julia</li>
<li><a class="reference external" href="https://github.com/JuliaStats/GLM.jl">GLM</a> - Generalized linear models in Julia</li>
<li><a class="reference external" href="https://github.com/lendle/OnlineLearning.jl">Online Learning</a></li>
<li><a class="reference external" href="https://github.com/simonster/GLMNet.jl">GLMNet</a> - Julia wrapper for fitting Lasso/ElasticNet GLM models using glmnet</li>
<li><a class="reference external" href="https://github.com/JuliaStats/Clustering.jl">Clustering</a> - Basic functions for clustering data: k-means, dp-means, etc.</li>
<li><a class="reference external" href="https://github.com/JuliaStats/SVM.jl">SVM</a> - SVM’s for Julia</li>
<li><a class="reference external" href="https://github.com/JuliaStats/KernelDensity.jl">Kernal Density</a> - Kernel density estimators for julia</li>
<li><a class="reference external" href="https://github.com/JuliaStats/DimensionalityReduction.jl">Dimensionality Reduction</a> - Methods for dimensionality reduction</li>
<li><a class="reference external" href="https://github.com/JuliaStats/NMF.jl">NMF</a> - A Julia package for non-negative matrix factorization</li>
<li><a class="reference external" href="https://github.com/EricChiang/ANN.jl">ANN</a> - Julia artificial neural networks</li>
<li><a class="reference external" href="https://github.com/pluskid/Mocha.jl">Mocha</a> - Deep Learning framework for Julia inspired by Caffe</li>
<li><a class="reference external" href="https://github.com/dmlc/XGBoost.jl">XGBoost</a> - eXtreme Gradient Boosting Package in Julia</li>
<li><a class="reference external" href="https://github.com/wildart/ManifoldLearning.jl">ManifoldLearning</a> - A Julia package for manifold learning and nonlinear dimensionality reduction</li>
<li><a class="reference external" href="https://github.com/dmlc/mxnet">MXNet</a> - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.</li>
<li><a class="reference external" href="https://github.com/hshindo/Merlin.jl">Merlin</a> - Flexible Deep Learning Framework in Julia</li>
<li><a class="reference external" href="https://github.com/davidavdav/ROCAnalysis.jl">ROCAnalysis</a> - Receiver Operating Characteristics and functions for evaluation probabilistic binary classifiers</li>
<li><a class="reference external" href="https://github.com/davidavdav/GaussianMixtures.jl">GaussianMixtures</a> - Large scale Gaussian Mixture Models</li>
<li><a class="reference external" href="https://github.com/cstjean/ScikitLearn.jl">ScikitLearn</a> - Julia implementation of the scikit-learn API</li>
<li><a class="reference external" href="https://github.com/denizyuret/Knet.jl">Knet</a> - Koç University Deep Learning Framework</li>
</ul>
<p class="rubric">Natural Language Processing</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/slycoder/TopicModels.jl">Topic Models</a> - TopicModels for Julia</li>
<li><a class="reference external" href="https://github.com/johnmyleswhite/TextAnalysis.jl">Text Analysis</a> - Julia package for text analysis</li>
</ul>
<p class="rubric">Data Analysis / Data Visualization</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/IainNZ/GraphLayout.jl">Graph Layout</a> - Graph layout algorithms in pure Julia</li>
<li><a class="reference external" href="https://github.com/JuliaStats/DataFramesMeta.jl">Data Frames Meta</a> - Metaprogramming tools for DataFrames</li>
<li><a class="reference external" href="https://github.com/nfoti/JuliaData">Julia Data</a> - library for working with tabular data in Julia</li>
<li><a class="reference external" href="https://github.com/WizardMac/ReadStat.jl">Data Read</a> - Read files from Stata, SAS, and SPSS</li>
<li><a class="reference external" href="https://github.com/JuliaStats/HypothesisTests.jl">Hypothesis Tests</a> - Hypothesis tests for Julia</li>
<li><a class="reference external" href="https://github.com/GiovineItalia/Gadfly.jl">Gadfly</a> - Crafty statistical graphics for Julia.</li>
<li><a class="reference external" href="https://github.com/JuliaStats/Stats.jl">Stats</a> - Statistical tests for Julia</li>
<li><a class="reference external" href="https://github.com/johnmyleswhite/RDatasets.jl">RDataSets</a> - Julia package for loading many of the data sets available in R</li>
<li><a class="reference external" href="https://github.com/JuliaStats/DataFrames.jl">DataFrames</a> - library for working with tabular data in Julia</li>
<li><a class="reference external" href="https://github.com/JuliaStats/Distributions.jl">Distributions</a> - A Julia package for probability distributions and associated functions.</li>
<li><a class="reference external" href="https://github.com/JuliaStats/DataArrays.jl">Data Arrays</a> - Data structures that allow missing values</li>
<li><a class="reference external" href="https://github.com/JuliaStats/TimeSeries.jl">Time Series</a> - Time series toolkit for Julia</li>
<li><a class="reference external" href="https://github.com/lindahua/Sampling.jl">Sampling</a> - Basic sampling algorithms for Julia</li>
</ul>
<p class="rubric">Misc Stuff / Presentations</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/JuliaDSP/DSP.jl)-DigitalSignalProcessing(filtering,periodograms,spectrograms,windowfunctions">DSP</a>.</li>
<li><a class="reference external" href="https://github.com/JuliaCon/presentations">JuliaCon Presentations</a> - Presentations for JuliaCon</li>
<li><a class="reference external" href="https://github.com/davidavdav/SignalProcessing.jl">SignalProcessing</a> - Signal Processing tools for Julia</li>
<li><a class="reference external" href="https://github.com/JuliaImages/Images.jl">Images</a> - An image library for Julia</li>
</ul>
</div>
<div class="section" id="lua">
<h3><a class="toc-backref" href="#toc-entry-13">Lua</a><a class="headerlink" href="#lua" title="Permalink to this headline">¶</a></h3>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="http://torch.ch/">Torch7</a></li>
<li><a class="reference external" href="https://github.com/deepmind/torch-cephes">cephes</a> - Cephes mathematical functions library, wrapped for Torch. Provides and wraps the 180+ special mathematical functions from the Cephes mathematical library, developed by Stephen L. Moshier. It is used, among many other places, at the heart of SciPy.</li>
<li><a class="reference external" href="https://github.com/twitter/torch-autograd">autograd</a> - Autograd automatically differentiates native Torch code. Inspired by the original Python version.</li>
<li><a class="reference external" href="https://github.com/torch/graph">graph</a> - Graph package for Torch</li>
<li><a class="reference external" href="https://github.com/deepmind/torch-randomkit">randomkit</a> - Numpy’s randomkit, wrapped for Torch</li>
<li><a class="reference external" href="http://soumith.ch/torch-signal/signal/">signal</a> - A signal processing toolbox for Torch-7. FFT, DCT, Hilbert, cepstrums, stft</li>
<li><a class="reference external" href="https://github.com/torch/nn">nn</a> - Neural Network package for Torch</li>
<li><a class="reference external" href="https://github.com/torchnet/torchnet">torchnet</a> - framework for torch which provides a set of abstractions aiming at encouraging code re-use as well as encouraging modular programming</li>
<li><a class="reference external" href="https://github.com/torch/nngraph">nngraph</a> - This package provides graphical computation for nn library in Torch7.</li>
<li><a class="reference external" href="https://github.com/clementfarabet/lua---nnx">nnx</a> - A completely unstable and experimental package that extends Torch’s builtin nn library</li>
<li><a class="reference external" href="https://github.com/Element-Research/rnn">rnn</a> - A Recurrent Neural Network library that extends Torch’s nn. RNNs, LSTMs, GRUs, BRNNs, BLSTMs, etc.</li>
<li><a class="reference external" href="https://github.com/Element-Research/dpnn">dpnn</a> - Many useful features that aren’t part of the main nn package.</li>
<li><a class="reference external" href="https://github.com/nicholas-leonard/dp">dp</a> - A deep learning library designed for streamlining research and development using the Torch7 distribution. It emphasizes flexibility through the elegant use of object-oriented design patterns.</li>
<li><a class="reference external" href="https://github.com/torch/optim">optim</a> - An optimization library for Torch. SGD, Adagrad, Conjugate-Gradient, LBFGS, RProp and more.</li>
<li><a class="reference external" href="https://github.com/koraykv/unsup)-ApackageforunsupervisedlearninginTorch.Providesmodulesthatarecompatiblewithnn(LinearPsd,ConvPsd,AutoEncoder,...),andself-containedalgorithms(k-means,PCA">unsup</a>.</li>
<li><a class="reference external" href="https://github.com/clementfarabet/manifold">manifold</a> - A package to manipulate manifolds</li>
<li><a class="reference external" href="https://github.com/koraykv/torch-svm">svm</a> - Torch-SVM library</li>
<li><a class="reference external" href="https://github.com/clementfarabet/lbfgs">lbfgs</a> - FFI Wrapper for liblbfgs</li>
<li><a class="reference external" href="https://github.com/clementfarabet/vowpal_wabbit">vowpalwabbit</a> - An old vowpalwabbit interface to torch.</li>
<li><a class="reference external" href="https://github.com/clementfarabet/lua---opengm">OpenGM</a> - OpenGM is a C++ library for graphical modeling, and inference. The Lua bindings provide a simple way of describing graphs, from Lua, and then optimizing them with OpenGM.</li>
<li><a class="reference external" href="https://github.com/MichaelMathieu/lua---spaghetti)-Spaghetti(sparselinear">sphagetti</a> module for torch7 by &#64;MichaelMathieu</li>
<li><a class="reference external" href="https://github.com/ocallaco/LuaSHkit">LuaSHKit</a> - A lua wrapper around the Locality sensitive hashing library SHKit</li>
<li><a class="reference external" href="https://github.com/rlowrance/kernel-smoothers">kernel smoothing</a> - KNN, kernel-weighted average, local linear regression smoothers</li>
<li><a class="reference external" href="https://github.com/torch/cutorch">cutorch</a> - Torch CUDA Implementation</li>
<li><a class="reference external" href="https://github.com/torch/cunn">cunn</a> - Torch CUDA Neural Network Implementation</li>
<li><a class="reference external" href="https://github.com/clementfarabet/lua---imgraph">imgraph</a> - An image/graph library for Torch. This package provides routines to construct graphs on images, segment them, build trees out of them, and convert them back to images.</li>
<li><a class="reference external" href="https://github.com/clementfarabet/videograph">videograph</a> - A video/graph library for Torch. This package provides routines to construct graphs on videos, segment them, build trees out of them, and convert them back to videos.</li>
<li><a class="reference external" href="https://github.com/marcoscoffier/torch-saliency">saliency</a> - code and tools around integral images. A library for finding interest points based on fast integral histograms.</li>
<li><a class="reference external" href="https://github.com/marcoscoffier/lua---stitch">stitch</a> - allows us to use hugin to stitch images and apply same stitching to a video sequence</li>
<li><a class="reference external" href="https://github.com/marcoscoffier/lua---sfm">sfm</a> - A bundle adjustment/structure from motion package</li>
<li><a class="reference external" href="https://github.com/koraykv/fex">fex</a> - A package for feature extraction in Torch. Provides SIFT and dSIFT modules.</li>
<li><a class="reference external" href="https://github.com/sermanet/OverFeat">OverFeat</a> - A state-of-the-art generic dense feature extractor</li>
<li><a class="reference external" href="http://numlua.luaforge.net/">Numeric Lua</a></li>
<li><a class="reference external" href="http://labix.org/lunatic-python">Lunatic Python</a></li>
<li><a class="reference external" href="http://scilua.org/">SciLua</a></li>
<li><a class="reference external" href="https://bitbucket.org/lucashnegri/lna">Lua - Numerical Algorithms</a></li>
<li><a class="reference external" href="https://github.com/jzrake/lunum">Lunum</a></li>
</ul>
<p class="rubric">Demos and Scripts</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/e-lab/torch7-demos">Core torch7 demos repository</a>.
* linear-regression, logistic-regression
* face detector (training and detection as separate demos)
* mst-based-segmenter
* train-a-digit-classifier
* train-autoencoder
* optical flow demo
* train-on-housenumbers
* train-on-cifar
* tracking with deep nets
* kinect demo
* filter-bank visualization
* saliency-networks</li>
<li><a class="reference external" href="https://github.com/soumith/galaxyzoo">Training a Convnet for the Galaxy-Zoo Kaggle challenge(CUDA demo)</a></li>
<li><a class="reference external" href="https://github.com/mbhenaff/MusicTagging">Music Tagging</a> - Music Tagging scripts for torch7</li>
<li><a class="reference external" href="https://github.com/rosejn/torch-datasets">torch-datasets</a> - Scripts to load several popular datasets including:
* BSR 500
* CIFAR-10
* COIL
* Street View House Numbers
* MNIST
* NORB</li>
<li><a class="reference external" href="https://github.com/fidlej/aledataset">Atari2600</a> - Scripts to generate a dataset with static frames from the Arcade Learning Environment</li>
</ul>
</div>
<div class="section" id="matlab">
<h3><a class="toc-backref" href="#toc-entry-14">Matlab</a><a class="headerlink" href="#matlab" title="Permalink to this headline">¶</a></h3>
<p class="rubric">Computer Vision</p>
<ul class="simple">
<li><a class="reference external" href="http://www.ifp.illinois.edu/~minhdo/software/contourlet_toolbox.tar">Contourlets</a> - MATLAB source code that implements the contourlet transform and its utility functions.</li>
<li><a class="reference external" href="http://www.shearlab.org/software">Shearlets</a> - MATLAB code for shearlet transform</li>
<li><a class="reference external" href="http://www.curvelet.org/software.html">Curvelets</a> - The Curvelet transform is a higher dimensional generalization of the Wavelet transform designed to represent images at different scales and different angles.</li>
<li><a class="reference external" href="http://www.cmap.polytechnique.fr/~peyre/download/">Bandlets</a> - MATLAB code for bandlet transform</li>
<li><a class="reference external" href="http://kyamagu.github.io/mexopencv/">mexopencv</a> - Collection and a development kit of MATLAB mex functions for OpenCV library</li>
</ul>
<p class="rubric">Natural Language Processing</p>
<ul class="simple">
<li><a class="reference external" href="https://amplab.cs.berkeley.edu/an-nlp-library-for-matlab/">NLP</a> - An NLP library for Matlab</li>
</ul>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html">Training a deep autoencoder or a classifier on MNIST</a></li>
<li><a class="reference external" href="http://www.socher.org/index.php/Main/Convolutional-RecursiveDeepLearningFor3DObjectClassification">Convolutional-Recursive Deep Learning for 3D Object Classification</a> - Convolutional-Recursive Deep Learning for 3D Object Classification[DEEP LEARNING]</li>
<li><a class="reference external" href="http://homepage.tudelft.nl/19j49/t-SNE.html)-t-DistributedStochasticNeighborEmbedding(t-SNE)isa(prize-winning">t-Distributed Stochastic Neighbor Embedding</a> technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets.</li>
<li><a class="reference external" href="http://people.kyb.tuebingen.mpg.de/spider/">Spider</a> - The spider is intended to be a complete object orientated environment for machine learning in Matlab.</li>
<li><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/#matlab">LibSVM</a> - A Library for Support Vector Machines</li>
<li><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/#download">LibLinear</a> - A Library for Large Linear Classification</li>
<li><a class="reference external" href="https://github.com/josephmisiti/machine-learning-module">Machine Learning Module</a> - Class on machine w/ PDF,lectures,code</li>
<li><a class="reference external" href="http://caffe.berkeleyvision.org">Caffe</a>  - A deep learning framework developed with cleanliness, readability, and speed in mind.</li>
<li><a class="reference external" href="https://github.com/covartech/PRT">Pattern Recognition Toolbox</a>  - A complete object-oriented environment for machine learning in Matlab.</li>
<li><a class="reference external" href="https://github.com/PRML/PRMLT">Pattern Recognition and Machine Learning</a> - This package contains the matlab implementation of the algorithms described in the book Pattern Recognition and Machine Learning by C. Bishop.</li>
<li><a class="reference external" href="http://optunity.readthedocs.io/en/latest/">Optunity</a> - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly with MATLAB.</li>
</ul>
<p class="rubric">Data Analysis / Data Visualization</p>
<ul class="simple">
<li><a class="reference external" href="https://www.cs.purdue.edu/homes/dgleich/packages/matlab_bgl/">matlab_gbl</a> - MatlabBGL is a Matlab package for working with graphs.</li>
<li><a class="reference external" href="http://www.mathworks.com/matlabcentral/fileexchange/24134-gaimc---graph-algorithms-in-matlab-code">gamic</a> - Efficient pure-Matlab implementations of graph algorithms to complement MatlabBGL’s mex functions.</li>
</ul>
</div>
<div class="section" id="net">
<h3><a class="toc-backref" href="#toc-entry-15">.NET</a><a class="headerlink" href="#net" title="Permalink to this headline">¶</a></h3>
<p class="rubric">Computer Vision</p>
<ul class="simple">
<li><a class="reference external" href="https://code.google.com/archive/p/opencvdotnet">OpenCVDotNet</a> - A wrapper for the OpenCV project to be used with .NET applications.</li>
<li><a class="reference external" href="http://www.emgu.com/wiki/index.php/Main_Page">Emgu CV</a> - Cross platform wrapper of OpenCV which can be compiled in Mono to e run on Windows, Linus, Mac OS X, iOS, and Android.</li>
<li><a class="reference external" href="http://www.aforgenet.com/framework/">AForge.NET</a> - Open source C# framework for developers and researchers in the fields of Computer Vision and Artificial Intelligence. Development has now shifted to GitHub.</li>
<li><a class="reference external" href="http://accord-framework.net">Accord.NET</a> - Together with AForge.NET, this library can provide image processing and computer vision algorithms to Windows, Windows RT and Windows Phone. Some components are also available for Java and Android.</li>
</ul>
<p class="rubric">Natural Language Processing</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/sergey-tihon/Stanford.NLP.NET/">Stanford.NLP for .NET</a> - A full port of Stanford NLP packages to .NET and also available precompiled as a NuGet package.</li>
</ul>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="http://accord-framework.net/">Accord-Framework</a> -The Accord.NET Framework is a complete framework for building machine learning, computer vision, computer audition, signal processing and statistical applications.</li>
<li><a class="reference external" href="http://www.nuget.org/packages/Accord.MachineLearning/">Accord.MachineLearning</a> - Support Vector Machines, Decision Trees, Naive Bayesian models, K-means, Gaussian Mixture models and general algorithms such as Ransac, Cross-validation and Grid-Search for machine-learning applications. This package is part of the Accord.NET Framework.</li>
<li><a class="reference external" href="http://diffsharp.github.io/DiffSharp/)-Anautomaticdifferentiation(AD)libraryprovidingexactandefficientderivatives(gradients,Hessians,Jacobians,directionalderivatives,andmatrix-freeHessian-andJacobian-vectorproducts">DiffSharp</a> for machine learning and optimization applications. Operations can be nested to any level, meaning that you can compute exact higher-order derivatives and differentiate functions that are internally making use of differentiation, for applications such as hyperparameter optimization.</li>
<li><a class="reference external" href="https://github.com/fsprojects/Vulpes">Vulpes</a> - Deep belief and deep learning implementation written in F# and leverages CUDA GPU execution with Alea.cuBase.</li>
<li><a class="reference external" href="http://www.nuget.org/packages/encog-dotnet-core/">Encog</a> -  An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.</li>
<li><a class="reference external" href="http://bragisoft.com/">Neural Network Designer</a> - DBMS management system and designer for neural networks. The designer application is developed using WPF, and is a user interface which allows you to design your neural network, query the network, create and configure chat bots that are capable of asking questions and learning from your feed back.  The chat bots can even scrape the internet for information to return in their output as well as to use for learning.</li>
<li><a class="reference external" href="http://infernet.azurewebsites.net/">Infer.NET</a> - Infer.NET is a framework for running Bayesian inference in graphical models. One can use Infer.NET to solve many different kinds of machine learning problems, from standard problems like classification, recommendation or clustering through to customised solutions to domain-specific problems. Infer.NET has been used in a wide variety of domains including information retrieval, bioinformatics, epidemiology, vision, and many others.</li>
</ul>
<p class="rubric">Data Analysis / Data Visualization</p>
<ul class="simple">
<li><a class="reference external" href="http://www.nuget.org/packages/numl/">numl</a> - numl is a machine learning library intended to ease the use of using standard modeling techniques for both prediction and clustering.</li>
<li><a class="reference external" href="http://www.nuget.org/packages/MathNet.Numerics/">Math.NET Numerics</a> - Numerical foundation of the Math.NET project, aiming to provide methods and algorithms for numerical computations in science, engineering and every day use. Supports .Net 4.0, .Net 3.5 and Mono on Windows, Linux and Mac; Silverlight 5, WindowsPhone/SL 8, WindowsPhone 8.1 and Windows 8 with PCL Portable Profiles 47 and 344; Android/iOS with Xamarin.</li>
<li><a class="reference external" href="https://www.microsoft.com/en-us/research/project/sho-the-net-playground-for-data/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fprojects%2Fsho%2F)-Shoisaninteractiveenvironmentfordataanalysisandscientificcomputingthatletsyouseamlesslyconnectscripts(inIronPython)withcompiledcode(in.NET">Sho</a> to enable fast and flexible prototyping. The environment includes powerful and efficient libraries for linear algebra as well as data visualization that can be used from any .NET language, as well as a feature-rich interactive shell for rapid development.</li>
</ul>
</div>
<div class="section" id="objective-c">
<h3><a class="toc-backref" href="#toc-entry-16">Objective C</a><a class="headerlink" href="#objective-c" title="Permalink to this headline">¶</a></h3>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/yconst/YCML)-AMachineLearningframeworkforObjective-CandSwift(OSX/iOS">YCML</a>.</li>
<li><a class="reference external" href="https://github.com/nikolaypavlov/MLPNeuralNet">MLPNeuralNet</a> - Fast multilayer perceptron neural network library for iOS and Mac OS X. MLPNeuralNet predicts new examples by trained neural network. It is built on top of the Apple’s Accelerate Framework, using vectorized operations and hardware acceleration if available.</li>
<li><a class="reference external" href="https://github.com/gianlucabertani/MAChineLearning">MAChineLearning</a> - An Objective-C multilayer perceptron library, with full support for training through backpropagation. Implemented using vDSP and vecLib, it’s 20 times faster than its Java equivalent. Includes sample code for use from Swift.</li>
<li><a class="reference external" href="https://github.com/Kalvar/ios-BPN-NeuralNetwork)-Itimplemented3layersneuralnetwork(InputLayer,HiddenLayerandOutputLayer)anditnamedBackPropagationNeuralNetwork(BPN">BPN-NeuralNetwork</a>. This network can be used in products recommendation, user behavior analysis, data mining and data analysis.</li>
<li><a class="reference external" href="https://github.com/Kalvar/ios-Multi-Perceptron-NeuralNetwork)-itimplementedmulti-perceptronsneuralnetwork(ニューラルネットワーク)basedonBackPropagationNeuralNetwork(BPN">Multi-Perceptron-NeuralNetwork</a> and designed unlimited-hidden-layers.</li>
<li><a class="reference external" href="https://github.com/Kalvar/ios-KRHebbian-Algorithm)-Itisanon-supervisorandself-learningalgorithm(adjusttheweights">KRHebbian-Algorithm</a> in neural network of Machine Learning.</li>
<li><a class="reference external" href="https://github.com/Kalvar/ios-KRKmeans-Algorithm">KRKmeans-Algorithm</a> - It implemented K-Means the clustering and classification algorithm. It could be used in data mining and image compression.</li>
<li><a class="reference external" href="https://github.com/Kalvar/ios-KRFuzzyCMeans-Algorithm)-ItimplementedFuzzyC-Means(FCM">KRFuzzyCMeans-Algorithm</a> the fuzzy clustering / classification algorithm on Machine Learning. It could be used in data mining and image compression.</li>
</ul>
</div>
<div class="section" id="ocaml">
<h3><a class="toc-backref" href="#toc-entry-17">OCaml</a><a class="headerlink" href="#ocaml" title="Permalink to this headline">¶</a></h3>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/hammerlab/oml/">Oml</a> - A general statistics and machine learning library.</li>
<li><a class="reference external" href="http://mmottl.github.io/gpr/">GPR</a> - Efficient Gaussian Process Regression in OCaml.</li>
<li><a class="reference external" href="http://libra.cs.uoregon.edu">Libra-Tk</a> - Algorithms for learning and inference with discrete probabilistic models.</li>
<li><a class="reference external" href="https://github.com/LaurentMazare/tensorflow-ocaml">TensorFlow</a> - OCaml bindings for TensorFlow.</li>
</ul>
</div>
<div class="section" id="php">
<h3><a class="toc-backref" href="#toc-entry-18">PHP</a><a class="headerlink" href="#php" title="Permalink to this headline">¶</a></h3>
<p class="rubric">Natural Language Processing</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/fukuball/jieba-php">jieba-php</a> - Chinese Words Segmentation Utilities.</li>
</ul>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/php-ai/php-ml">PHP-ML</a> - Machine Learning library for PHP. Algorithms, Cross Validation, Neural Network, Preprocessing, Feature Extraction and much more in one library.</li>
<li><a class="reference external" href="https://github.com/denissimon/prediction-builder">PredictionBuilder</a> - A library for machine learning that builds predictions using a linear regression.</li>
<li><a class="reference external" href="https://github.com/RubixML/RubixML">Rubix ML</a> - A high-level machine learning and deep learning library for the PHP language.</li>
</ul>
</div>
<div class="section" id="python">
<h3><a class="toc-backref" href="#toc-entry-19">Python</a><a class="headerlink" href="#python" title="Permalink to this headline">¶</a></h3>
<p class="rubric">Computer Vision</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/scikit-image/scikit-image">Scikit-Image</a> - A collection of algorithms for image processing in Python.</li>
<li><a class="reference external" href="http://simplecv.org/">SimpleCV</a> - An open source computer vision framework that gives access to several high-powered computer vision libraries, such as OpenCV. Written on Python and runs on Mac, Windows, and Ubuntu Linux.</li>
<li><a class="reference external" href="https://github.com/ukoethe/vigra">Vigranumpy</a> - Python bindings for the VIGRA C++ computer vision library.</li>
<li><a class="reference external" href="https://cmusatyalab.github.io/openface/">OpenFace</a> - Free and open source face recognition with deep neural networks.</li>
<li><a class="reference external" href="https://github.com/jesolem/PCV">PCV</a> - Open source Python module for computer vision</li>
</ul>
<p class="rubric">Natural Language Processing</p>
<ul class="simple">
<li><a class="reference external" href="http://www.nltk.org/">NLTK</a> - A leading platform for building Python programs to work with human language data.</li>
<li><a class="reference external" href="http://www.clips.ua.ac.be/pattern">Pattern</a> - A web mining module for the Python programming language. It has tools for natural language processing, machine learning, among others.</li>
<li><a class="reference external" href="https://github.com/machinalis/quepy">Quepy</a> - A python framework to transform natural language questions to queries in a database query language</li>
<li><a class="reference external" href="http://textblob.readthedocs.io/en/dev/)-ProvidingaconsistentAPIfordivingintocommonnaturallanguageprocessing(NLP">TextBlob</a> tasks. Stands on the giant shoulders of NLTK and Pattern, and plays nicely with both.</li>
<li><a class="reference external" href="https://github.com/machinalis/yalign">YAlign</a> - A sentence aligner, a friendly tool for extracting parallel sentences from comparable corpora.</li>
<li><a class="reference external" href="https://github.com/fxsjy/jieba#jieba-1">jieba</a> - Chinese Words Segmentation Utilities.</li>
<li><a class="reference external" href="https://github.com/isnowfy/snownlp">SnowNLP</a> - A library for processing Chinese text.</li>
<li><a class="reference external" href="https://github.com/prodicus/spammy">spammy</a> - A library for email Spam filtering built on top of nltk</li>
<li><a class="reference external" href="https://github.com/victorlin/loso">loso</a> - Another Chinese segmentation library.</li>
<li><a class="reference external" href="https://github.com/duanhongyi/genius">genius</a> - A Chinese segment base on Conditional Random Field.</li>
<li><a class="reference external" href="http://konlpy.org">KoNLPy</a> - A Python package for Korean natural language processing.</li>
<li><a class="reference external" href="https://github.com/pprett/nut">nut</a> - Natural language Understanding Toolkit</li>
<li><a class="reference external" href="https://github.com/columbia-applied-data-science/rosetta)-Textprocessingtoolsandwrappers(e.g.VowpalWabbit">Rosetta</a></li>
<li><a class="reference external" href="https://pypi.python.org/pypi/bllipparser/)-PythonbindingsfortheBLLIPNaturalLanguageParser(alsoknownastheCharniak-Johnsonparser">BLLIP Parser</a></li>
<li><a class="reference external" href="http://proycon.github.io/folia/">PyNLPl](https://github.com/proycon/pynlpl) - Python Natural Language Processing Library. General purpose NLP library for Python. Also contains some specific modules for parsing common NLP formats, most notably for [FoLiA</a>, but also ARPA language models, Moses phrasetables, GIZA++ alignments.</li>
<li><a class="reference external" href="https://github.com/proycon/python-ucto)-Pythonbindingtoucto(aunicode-awarerule-basedtokenizerforvariouslanguages">python-ucto</a></li>
<li><a class="reference external" href="https://github.com/proycon/python-frog)-PythonbindingtoFrog,anNLPsuiteforDutch.(postagging,lemmatisation,dependencyparsing,NER">python-frog</a></li>
<li><a class="reference external" href="https://github.com/frcchang/zpar">python-zpar](https://github.com/EducationalTestingService/python-zpar) - Python bindings for [ZPar</a>, a statistical part-of-speech-tagger, constiuency parser, and dependency parser for English.</li>
<li><a class="reference external" href="https://github.com/proycon/colibri-core">colibri-core</a> - Python binding to C++ library for extracting and working with with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.</li>
<li><a class="reference external" href="https://github.com/honnibal/spaCy/">spaCy</a> - Industrial strength NLP with Python and Cython.</li>
<li><a class="reference external" href="https://github.com/dmcc/PyStanfordDependencies">PyStanfordDependencies</a> - Python interface for converting Penn Treebank trees to Stanford Dependencies.</li>
<li><a class="reference external" href="https://github.com/doukremt/distance">Distance</a> - Levenshtein and Hamming distance computation</li>
<li><a class="reference external" href="https://github.com/seatgeek/fuzzywuzzy">Fuzzy Wuzzy</a> - Fuzzy String Matching in Python</li>
<li><a class="reference external" href="https://github.com/jamesturk/jellyfish">jellyfish</a> - a python library for doing approximate and phonetic matching of strings.</li>
<li><a class="reference external" href="https://pypi.python.org/pypi/editdistance">editdistance</a> - fast implementation of edit distance</li>
<li><a class="reference external" href="https://github.com/chartbeat-labs/textacy">textacy</a> - higher-level NLP built on Spacy</li>
<li><a class="reference external" href="https://github.com/stanfordnlp/CoreNLP">stanford-corenlp-python](https://github.com/dasmith/stanford-corenlp-python) - Python wrapper for [Stanford CoreNLP</a></li>
</ul>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/ClimbsRocks/auto_ml">auto_ml</a> - Automated machine learning for production and analytics. Lets you focus on the fun parts of ML, while outputting production-ready code, and detailed analytics of your dataset and results. Includes support for NLP, XGBoost, LightGBM, and soon, deep learning.</li>
<li><a class="reference external" href="https://github.com/jeff1evesque/machine-learning#programmatic-interface)API,forsupportvectormachines.Correspondingdataset(s)arestoredintoaSQLdatabase,thengeneratedmodel(s)usedforprediction(s">machine learning](https://github.com/jeff1evesque/machine-learning) - automated build consisting of a [web-interface](https://github.com/jeff1evesque/machine-learning#web-interface), and set of [programmatic-interface</a>, are stored into a NoSQL datastore.</li>
<li><a class="reference external" href="https://github.com/dmlc/xgboost)-PythonbindingsforeXtremeGradientBoosting(Tree">XGBoost</a> Library</li>
<li><a class="reference external" href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers">Bayesian Methods for Hackers</a> - Book/iPython notebooks on Probabilistic Programming in Python</li>
<li><a class="reference external" href="https://github.com/machinalis/featureforge">Featureforge</a> A set of tools for creating and testing machine learning features, with a scikit-learn compatible API</li>
<li><a class="reference external" href="http://spark.apache.org/docs/latest/mllib-guide.html">MLlib in Apache Spark</a> - Distributed machine learning library in Spark</li>
<li><a class="reference external" href="https://github.com/Hydrospheredata/mist">Hydrosphere Mist</a> - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.</li>
<li><a class="reference external" href="http://scikit-learn.org/">scikit-learn</a> - A Python module for machine learning built on top of SciPy.</li>
<li><a class="reference external" href="https://github.com/all-umass/metric-learn">metric-learn</a> - A Python module for metric learning.</li>
<li><a class="reference external" href="https://github.com/simpleai-team/simpleai">SimpleAI</a> Python implementation of many of the artificial intelligence algorithms described on the book “Artificial Intelligence, a Modern Approach”. It focuses on providing an easy to use, well documented and tested library.</li>
<li><a class="reference external" href="http://www.astroml.org/">astroML</a> - Machine Learning and Data Mining for Astronomy.</li>
<li><a class="reference external" href="https://turi.com/products/create/docs/)-Alibrarywithvariousmachinelearningmodels(regression,clustering,recommendersystems,graphanalytics,etc.">graphlab-create</a> implemented on top of a disk-backed DataFrame.</li>
<li><a class="reference external" href="https://bigml.com">BigML</a> - A library that contacts external servers.</li>
<li><a class="reference external" href="https://github.com/clips/pattern">pattern</a> - Web mining module for Python.</li>
<li><a class="reference external" href="https://github.com/numenta/nupic">NuPIC</a> - Numenta Platform for Intelligent Computing.</li>
<li><a class="reference external" href="https://github.com/Theano/Theano">Pylearn2](https://github.com/lisa-lab/pylearn2) - A Machine Learning library based on [Theano</a>.</li>
<li><a class="reference external" href="https://github.com/Theano/Theano">keras](https://github.com/fchollet/keras) - Modular neural network library based on [Theano</a>.</li>
<li><a class="reference external" href="https://github.com/Lasagne/Lasagne">Lasagne</a> - Lightweight library to build and train neural networks in Theano.</li>
<li><a class="reference external" href="https://github.com/hannes-brt/hebel">hebel</a> - GPU-Accelerated Deep Learning Library in Python.</li>
<li><a class="reference external" href="https://github.com/pfnet/chainer">Chainer</a> - Flexible neural network framework</li>
<li><a class="reference external" href="https://facebookincubator.github.io/prophet">prohpet</a> - Fast and automated time series forecasting framework by Facebook.</li>
<li><a class="reference external" href="https://github.com/RaRe-Technologies/gensim">gensim</a> - Topic Modelling for Humans.</li>
<li><a class="reference external" href="https://github.com/ContinuumIO/topik">topik</a> - Topic modelling toolkit</li>
<li><a class="reference external" href="https://github.com/pybrain/pybrain">PyBrain</a> - Another Python Machine Learning Library.</li>
<li><a class="reference external" href="https://github.com/IDSIA/brainstorm">Brainstorm</a> - Fast, flexible and fun neural networks. This is the successor of PyBrain.</li>
<li><a class="reference external" href="https://github.com/muricoca/crab">Crab</a> - A ﬂexible, fast recommender engine.</li>
<li><a class="reference external" href="https://github.com/ocelma/python-recsys">python-recsys</a> - A Python library for implementing a Recommender System.</li>
<li><a class="reference external" href="https://github.com/AllenDowney/ThinkBayes">thinking bayes</a> - Book on Bayesian Analysis</li>
<li><a class="reference external" href="https://arxiv.org/pdf/1611.07004.pdf">Image-to-Image Translation with Conditional Adversarial Networks](https://github.com/williamFalcon/pix2pix-keras) - Implementation of image to image (pix2pix) translation from the paper by [isola et al</a>.[DEEP LEARNING]</li>
<li><a class="reference external" href="https://github.com/echen/restricted-boltzmann-machines">Restricted Boltzmann Machines</a> -Restricted Boltzmann Machines in Python. [DEEP LEARNING]</li>
<li><a class="reference external" href="https://github.com/pprett/bolt">Bolt</a> - Bolt Online Learning Toolbox</li>
<li><a class="reference external" href="https://github.com/patvarilly/CoverTree">CoverTree</a> - Python implementation of cover trees, near-drop-in replacement for scipy.spatial.kdtree</li>
<li><a class="reference external" href="https://github.com/nilearn/nilearn">nilearn</a> - Machine learning for NeuroImaging in Python</li>
<li><a class="reference external" href="http://contrib.scikit-learn.org/imbalanced-learn/">imbalanced-learn</a> - Python module to perform under sampling and over sampling with various techniques.</li>
<li><a class="reference external" href="https://github.com/shogun-toolbox/shogun">Shogun</a> - The Shogun Machine Learning Toolbox</li>
<li><a class="reference external" href="https://github.com/perone/Pyevolve">Pyevolve</a> - Genetic algorithm framework.</li>
<li><a class="reference external" href="http://caffe.berkeleyvision.org">Caffe</a>  - A deep learning framework developed with cleanliness, readability, and speed in mind.</li>
<li><a class="reference external" href="https://github.com/breze-no-salt/breze">breze</a> - Theano based library for deep and recurrent neural networks</li>
<li><a class="reference external" href="https://github.com/mattjj/pyhsmm)-libraryforapproximateunsupervisedinferenceinBayesianHiddenMarkovModels(HMMs)andexplicit-durationHiddensemi-MarkovModels(HSMMs">pyhsmm</a>, focusing on the Bayesian Nonparametric extensions, the HDP-HMM and HDP-HSMM, mostly with weak-limit approximations.</li>
<li><a class="reference external" href="https://pythonhosted.org/mrjob/">mrjob</a> - A library to let Python program run on Hadoop.</li>
<li><a class="reference external" href="https://github.com/EducationalTestingService/skll">SKLL</a> - A wrapper around scikit-learn that makes it simpler to conduct experiments.</li>
<li><a class="reference external" href="https://github.com/zueve/neurolab">neurolab</a> - <a class="reference external" href="https://github.com/zueve/neurolab">https://github.com/zueve/neurolab</a></li>
<li><a class="reference external" href="https://github.com/JasperSnoek/spearmint">Spearmint</a> - Spearmint is a package to perform Bayesian optimization according to the algorithms outlined in the paper: Practical Bayesian Optimization of Machine Learning Algorithms. Jasper Snoek, Hugo Larochelle and Ryan P. Adams. Advances in Neural Information Processing Systems, 2012.</li>
<li><a class="reference external" href="https://github.com/abhik/pebl/">Pebl</a> - Python Environment for Bayesian Learning</li>
<li><a class="reference external" href="https://github.com/Theano/Theano/">Theano</a> - Optimizing GPU-meta-programming code generating array oriented optimizing math compiler in Python</li>
<li><a class="reference external" href="https://github.com/tensorflow/tensorflow/">TensorFlow</a> - Open source software library for numerical computation using data flow graphs</li>
<li><a class="reference external" href="https://github.com/jmschrei/yahmm/">yahmm</a> - Hidden Markov Models for Python, implemented in Cython for speed and efficiency.</li>
<li><a class="reference external" href="https://github.com/proycon/python-timbl">python-timbl</a> - A Python extension module wrapping the full TiMBL C++ programming interface. Timbl is an elaborate k-Nearest Neighbours machine learning toolkit.</li>
<li><a class="reference external" href="https://github.com/deap/deap">deap</a> - Evolutionary algorithm framework.</li>
<li><a class="reference external" href="https://github.com/andersbll/deeppy">pydeep</a> - Deep Learning In Python</li>
<li><a class="reference external" href="https://github.com/rasbt/mlxtend">mlxtend</a> - A library consisting of useful tools for data science and machine learning tasks.</li>
<li><a class="reference external" href="https://github.com/soumith/convnet-benchmarks">neon](https://github.com/NervanaSystems/neon) - Nervana’s [high-performance</a> Python-based Deep Learning framework [DEEP LEARNING]</li>
<li><a class="reference external" href="http://optunity.readthedocs.io/en/latest/">Optunity</a> - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search.</li>
<li><a class="reference external" href="https://github.com/mnielsen/neural-networks-and-deep-learning">Neural Networks and Deep Learning</a> - Code samples for my book “Neural Networks and Deep Learning” [DEEP LEARNING]</li>
<li><a class="reference external" href="https://github.com/spotify/annoy">Annoy</a> - Approximate nearest neighbours implementation</li>
<li><a class="reference external" href="https://github.com/tensorflow/skflow">skflow</a> - Simplified interface for TensorFlow, mimicking Scikit Learn.</li>
<li><a class="reference external" href="https://github.com/rhiever/tpot">TPOT</a> - Tool that automatically creates and optimizes machine learning pipelines using genetic programming. Consider it your personal data science assistant, automating a tedious part of machine learning.</li>
<li><a class="reference external" href="https://github.com/pgmpy/pgmpy">pgmpy</a> A python library for working with Probabilistic Graphical Models.</li>
<li><a class="reference external" href="https://github.com/NVIDIA/DIGITS)-TheDeepLearningGPUTrainingSystem(DIGITS">DIGITS</a> is a web application for training deep learning models.</li>
<li><a class="reference external" href="http://orange.biolab.si/">Orange</a> - Open source data visualization and data analysis for novices and experts.</li>
<li><a class="reference external" href="https://github.com/dmlc/mxnet">MXNet</a> - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.</li>
<li><a class="reference external" href="https://github.com/luispedro/milk">milk</a> - Machine learning toolkit focused on supervised classification.</li>
<li><a class="reference external" href="https://github.com/tflearn/tflearn">TFLearn</a> - Deep learning library featuring a higher-level API for TensorFlow.</li>
<li><a class="reference external" href="https://github.com/yandex/rep">REP</a> - an IPython-based environment for conducting data-driven research in a consistent and reproducible way. REP is not trying to substitute scikit-learn, but extends it and provides better user experience.</li>
<li><a class="reference external" href="https://github.com/fukatani/rgf_python)-PythonbindingsforRegularizedGreedyForest(Tree">rgf_python</a> Library.</li>
<li><a class="reference external" href="https://github.com/openai/gym">gym</a> - OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms.</li>
<li><a class="reference external" href="https://github.com/AmazaspShumik/sklearn-bayes">skbayes</a> - Python package for Bayesian Machine Learning with scikit-learn API</li>
<li><a class="reference external" href="https://github.com/fukuball/fuku-ml">fuku-ml</a> - Simple machine learning library, including Perceptron, Regression, Support Vector Machine, Decision Tree and more, it’s easy to use and easy to learn for beginners.</li>
</ul>
<p class="rubric">Data Analysis / Data Visualization</p>
<ul class="simple">
<li><a class="reference external" href="http://www.scipy.org/">SciPy</a> - A Python-based ecosystem of open-source software for mathematics, science, and engineering.</li>
<li><a class="reference external" href="http://www.numpy.org/">NumPy</a> - A fundamental package for scientific computing with Python.</li>
<li><a class="reference external" href="http://numba.pydata.org/)-PythonJIT(justintime">Numba</a> complier to LLVM aimed at scientific Python by the developers of Cython and NumPy.</li>
<li><a class="reference external" href="https://networkx.github.io/">NetworkX</a> - A high-productivity software for complex networks.</li>
<li><a class="reference external" href="http://igraph.org/python/">igraph</a> - binding to igraph library - General purpose graph library</li>
<li><a class="reference external" href="http://pandas.pydata.org/">Pandas</a> - A library providing high-performance, easy-to-use data structures and data analysis tools.</li>
<li><a class="reference external" href="https://github.com/mining/mining)-BusinessIntelligence(BI)inPython(Pandaswebinterface">Open Mining</a></li>
<li><a class="reference external" href="https://github.com/pymc-devs/pymc">PyMC</a> - Markov Chain Monte Carlo sampling toolkit.</li>
<li><a class="reference external" href="https://github.com/quantopian/zipline">zipline</a> - A Pythonic algorithmic trading library.</li>
<li><a class="reference external" href="http://www.pydy.org/">PyDy</a> - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion based around NumPy, SciPy, IPython, and matplotlib.</li>
<li><a class="reference external" href="https://github.com/sympy/sympy">SymPy</a> - A Python library for symbolic mathematics.</li>
<li><a class="reference external" href="https://github.com/statsmodels/statsmodels">statsmodels</a> - Statistical modeling and econometrics in Python.</li>
<li><a class="reference external" href="http://www.astropy.org/">astropy</a> - A community Python library for Astronomy.</li>
<li><a class="reference external" href="http://matplotlib.org/">matplotlib</a> - A Python 2D plotting library.</li>
<li><a class="reference external" href="https://github.com/bokeh/bokeh">bokeh</a> - Interactive Web Plotting for Python.</li>
<li><a class="reference external" href="https://plot.ly/python/">plotly</a> - Collaborative web plotting for Python and matplotlib.</li>
<li><a class="reference external" href="https://github.com/wrobstory/vincent">vincent</a> - A Python to Vega translator.</li>
<li><a class="reference external" href="https://d3js.org/">d3py](https://github.com/mikedewar/d3py) - A plotting library for Python, based on [D3.js</a>.</li>
<li><a class="reference external" href="https://github.com/D3xterjs/pydexter">PyDexter</a> - Simple plotting for Python. Wrapper for D3xterjs; easily render charts in-browser.</li>
<li><a class="reference external" href="https://github.com/yhat/ggpy">ggplot</a> - Same API as ggplot2 for R.</li>
<li><a class="reference external" href="https://github.com/sinhrks/ggfortify">ggfortify</a> - Unified interface to ggplot2 popular R packages.</li>
<li><a class="reference external" href="https://github.com/kartograph/kartograph.py">Kartograph.py</a> - Rendering beautiful SVG maps in Python.</li>
<li><a class="reference external" href="http://pygal.org/en/stable/">pygal</a> - A Python SVG Charts Creator.</li>
<li><a class="reference external" href="https://github.com/pyqtgraph/pyqtgraph">PyQtGraph</a> - A pure-python graphics and GUI library built on PyQt4 / PySide and NumPy.</li>
<li><a class="reference external" href="https://github.com/twitter/pycascading">pycascading</a></li>
<li><a class="reference external" href="https://github.com/AirSage/Petrel">Petrel</a> - Tools for writing, submitting, debugging, and monitoring Storm topologies in pure Python.</li>
<li><a class="reference external" href="https://github.com/blaze/blaze">Blaze</a> - NumPy and Pandas interface to Big Data.</li>
<li><a class="reference external" href="https://github.com/dfm/emcee">emcee</a> - The Python ensemble sampling toolkit for affine-invariant MCMC.</li>
<li><a class="reference external" href="http://www.windml.org">windML</a> - A Python Framework for Wind Energy Analysis and Prediction</li>
<li><a class="reference external" href="https://github.com/vispy/vispy">vispy</a> - GPU-based high-performance interactive OpenGL 2D/3D data visualization library</li>
<li><a class="reference external" href="https://github.com/numenta/nupic.cerebro2">cerebro2</a> A web-based visualization and debugging platform for NuPIC.</li>
<li><a class="reference external" href="https://github.com/htm-community/nupic.studio">NuPIC Studio</a> An all-in-one NuPIC Hierarchical Temporal Memory visualization and debugging super-tool!</li>
<li><a class="reference external" href="https://github.com/sparklingpandas/sparklingpandas)PandasonPySpark(POPS">SparklingPandas</a></li>
<li><a class="reference external" href="http://seaborn.pydata.org/">Seaborn</a> - A python visualization library based on matplotlib</li>
<li><a class="reference external" href="https://github.com/bloomberg/bqplot)-AnAPIforplottinginJupyter(IPython">bqplot</a></li>
<li><a class="reference external" href="https://github.com/rewonc/pastalog">pastalog</a> - Simple, realtime visualization of neural network training performance.</li>
<li><a class="reference external" href="https://github.com/airbnb/superset">caravel</a> - A data exploration platform designed to be visual, intuitive, and interactive.</li>
<li><a class="reference external" href="https://github.com/nathanepstein/dora">Dora</a> - Tools for exploratory data analysis in Python.</li>
<li><a class="reference external" href="http://www.ruffus.org.uk">Ruffus</a> - Computation Pipeline library for python.</li>
<li><a class="reference external" href="https://github.com/sevamoo/SOMPY)-SelfOrganizingMapwritteninPython(Usesneuralnetworksfordataanalysis">SOMPY</a>.</li>
<li><a class="reference external" href="https://github.com/peterwittek/somoclu">somoclu</a> Massively parallel self-organizing maps: accelerate training on multicore CPUs, GPUs, and clusters, has python API.</li>
<li><a class="reference external" href="https://github.com/lmcinnes/hdbscan">HDBScan</a> - implementation of the hdbscan algorithm in Python - used for clustering</li>
<li><a class="reference external" href="https://github.com/ayush1997/visualize_ML">visualize_ML</a> - A python package for data exploration and data analysis.</li>
<li><a class="reference external" href="https://github.com/reiinakano/scikit-plot">scikit-plot</a> - A visualization library for quick and easy generation of common plots in data analysis and machine learning.</li>
</ul>
<p class="rubric">Neural networks</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/karpathy/neuraltalk">Neural networks</a> - NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences.</li>
<li><a class="reference external" href="https://github.com/molcik/python-neuron)-Neuronissimpleclassfortimeseriespredictions.It'sutilizeLNU(LinearNeuralUnit),QNU(QuadraticNeuralUnit),RBF(RadialBasisFunction),MLP(MultiLayerPerceptron),MLP-ELM(MultiLayerPerceptron-ExtremeLearningMachine">Neuron</a> neural networks learned with Gradient descent or LeLevenberg–Marquardt algorithm.</li>
<li><a class="reference external" href="https://github.com/atmb4u/data-driven-code">Data Driven Code</a> - Very simple implementation of neural networks for dummies in python without using any libraries, with detailed comments.</li>
</ul>
</div>
<div class="section" id="ruby">
<h3><a class="toc-backref" href="#toc-entry-20">Ruby</a><a class="headerlink" href="#ruby" title="Permalink to this headline">¶</a></h3>
<p class="rubric">Natural Language Processing</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/louismullie/treat">Treat</a> -  Text REtrieval and Annotation Toolkit, definitely the most comprehensive toolkit I’ve encountered so far for Ruby</li>
<li><a class="reference external" href="https://deveiate.org/projects/Linguistics">Ruby Linguistics</a> -  Linguistics is a framework for building linguistic utilities for Ruby objects in any language. It includes a generic language-independent front end, a module for mapping language codes into language names, and a module which contains various English-language utilities.</li>
<li><a class="reference external" href="https://github.com/aurelian/ruby-stemmer">Stemmer</a> - Expose libstemmer_c to Ruby</li>
<li><a class="reference external" href="https://deveiate.org/projects/Ruby-WordNet/">Ruby Wordnet</a> - This library is a Ruby interface to WordNet</li>
<li><a class="reference external" href="https://sourceforge.net/projects/raspell/">Raspel</a> - raspell is an interface binding for ruby</li>
<li><a class="reference external" href="https://github.com/ealdent/uea-stemmer">UEA Stemmer</a> - Ruby port of UEALite Stemmer - a conservative stemmer for search and indexing</li>
<li><a class="reference external" href="https://github.com/twitter/twitter-text-rb">Twitter-text-rb</a> - A library that does auto linking and extraction of usernames, lists and hashtags in tweets</li>
</ul>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/tsycho/ruby-machine-learning">Ruby Machine Learning</a> - Some Machine Learning algorithms, implemented in Ruby</li>
<li><a class="reference external" href="https://github.com/mizoR/machine-learning-ruby">Machine Learning Ruby</a></li>
<li><a class="reference external" href="https://github.com/vasinov/jruby_mahout">jRuby Mahout</a> - JRuby Mahout is a gem that unleashes the power of Apache Mahout in the world of JRuby.</li>
<li><a class="reference external" href="https://github.com/cardmagic/classifier">CardMagic-Classifier</a> - A general classifier module to allow Bayesian and other types of classifications.</li>
<li><a class="reference external" href="https://github.com/febeling/rb-libsvm">rb-libsvm</a> - Ruby language bindings for LIBSVM which is a Library for Support Vector Machines</li>
<li><a class="reference external" href="https://github.com/asafschers/random_forester">Random Forester</a> - Creates Random Forest classifiers from PMML files</li>
</ul>
<p class="rubric">Data Analysis / Data Visualization</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/alexgutteridge/rsruby">rsruby</a> - Ruby - R bridge</li>
<li><a class="reference external" href="https://github.com/chrislo/data_visualisation_ruby">data-visualization-ruby</a> - Source code and supporting content for my Ruby Manor presentation on Data Visualisation with Ruby</li>
<li><a class="reference external" href="https://www.ruby-toolbox.com/projects/ruby-plot">ruby-plot</a> - gnuplot wrapper for ruby, especially for plotting roc curves into svg files</li>
<li><a class="reference external" href="https://github.com/zuhao/plotrb">plot-rb</a> - A plotting library in Ruby built on top of Vega and D3.</li>
<li><a class="reference external" href="http://www.rubyinside.com/scruffy-a-beautiful-graphing-toolkit-for-ruby-194.html">scruffy</a> - A beautiful graphing toolkit for Ruby</li>
<li><a class="reference external" href="http://sciruby.com/">SciRuby</a></li>
<li><a class="reference external" href="https://github.com/glean/glean">Glean</a> - A data management tool for humans</li>
<li><a class="reference external" href="https://github.com/bioruby/bioruby">Bioruby</a></li>
<li><a class="reference external" href="https://github.com/nkallen/arel">Arel</a></li>
</ul>
<p class="rubric">Misc</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/infochimps-labs/big_data_for_chimps">Big Data For Chimps</a></li>
<li><a class="reference external" href="http://kevincobain2000.github.io/listof/">Listof](https://github.com/kevincobain2000/listof) - Community based data collection, packed in gem. Get list of pretty much anything (stop words, countries, non words) in txt, json or hash. [Demo/Search for a list</a></li>
</ul>
</div>
<div class="section" id="rust">
<h3><a class="toc-backref" href="#toc-entry-21">Rust</a><a class="headerlink" href="#rust" title="Permalink to this headline">¶</a></h3>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/tedsta/deeplearn-rs">deeplearn-rs</a> - deeplearn-rs provides simple networks that use matrix multiplication, addition, and ReLU under the MIT license.</li>
<li><a class="reference external" href="https://github.com/maciejkula/rustlearn">rustlearn</a> - a machine learning framework featuring logistic regression, support vector machines, decision trees and random forests.</li>
<li><a class="reference external" href="https://github.com/AtheMathmo/rusty-machine">rusty-machine</a> - a pure-rust machine learning library.</li>
<li><a class="reference external" href="https://medium.com/&#64;mjhirn/tensorflow-wins-89b78b29aafb#.s0a3uy4cc">leaf](https://github.com/autumnai/leaf) - open source framework for machine intelligence, sharing concepts from TensorFlow and Caffe.  Available under the MIT license. [**[Deprecated]**</a></li>
<li><a class="reference external" href="https://github.com/jackm321/RustNN">RustNN</a> - RustNN is a feedforward neural network library.</li>
</ul>
</div>
<div class="section" id="r">
<h3><a class="toc-backref" href="#toc-entry-22">R</a><a class="headerlink" href="#r" title="Permalink to this headline">¶</a></h3>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="http://cran.r-project.org/web/packages/ahaz/index.html">ahaz</a> - ahaz: Regularization for semiparametric additive hazards regression</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/arules/index.html">arules</a> - arules: Mining Association Rules and Frequent Itemsets</li>
<li><a class="reference external" href="https://cran.r-project.org/web/packages/biglasso/index.html">biglasso</a> - biglasso: Extending Lasso Model Fitting to Big Data in R</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/bigrf/index.html">bigrf</a> - bigrf: Big Random Forests: Classification and Regression Forests for Large Data Sets</li>
<li><a href="#system-message-1"><span class="problematic" id="problematic-1">`bigRR &lt;http://cran.r-project.org/web/packages/bigRR/index.html) - bigRR: Generalized Ridge Regression (with special advantage for p &gt;&gt; n cases&gt;`__</span></a></li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/bmrm/index.html">bmrm</a> - bmrm: Bundle Methods for Regularized Risk Minimization Package</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/Boruta/index.html">Boruta</a> - Boruta: A wrapper algorithm for all-relevant feature selection</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/bst/index.html">bst</a> - bst: Gradient Boosting</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/C50/index.html">C50</a> - C50: C5.0 Decision Trees and Rule-Based Models</li>
<li><a class="reference external" href="http://caret.r-forge.r-project.org/">caret</a> - Classification and Regression Training: Unified interface to ~150 ML algorithms in R.</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/caretEnsemble/index.html">caretEnsemble</a> - caretEnsemble: Framework for fitting multiple caret models as well as creating ensembles of such models.</li>
<li><a class="reference external" href="https://github.com/jbrownlee/CleverAlgorithmsMachineLearning">Clever Algorithms For Machine Learning</a></li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/CORElearn/index.html">CORElearn</a> - CORElearn: Classification, regression, feature evaluation and ordinal evaluation</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/CoxBoost/index.html">CoxBoost</a> - CoxBoost: Cox models by likelihood based boosting for a single survival endpoint or competing risks</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/Cubist/index.html">Cubist</a> - Cubist: Rule- and Instance-Based Regression Modeling</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/e1071/index.html)-e1071:MiscFunctionsoftheDepartmentofStatistics(e1071">e1071</a>, TU Wien</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/earth/index.html">earth</a> - earth: Multivariate Adaptive Regression Spline Models</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/elasticnet/index.html">elasticnet</a> - elasticnet: Elastic-Net for Sparse Estimation and Sparse PCA</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/ElemStatLearn/index.html">ElemStatLearn</a> - ElemStatLearn: Data sets, functions and examples from the book: “The Elements of Statistical Learning, Data Mining, Inference, and Prediction” by Trevor Hastie, Robert Tibshirani and Jerome Friedman Prediction” by Trevor Hastie, Robert Tibshirani and Jerome Friedman</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/evtree/index.html">evtree</a> - evtree: Evolutionary Learning of Globally Optimal Trees</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/forecast/index.html">forecast</a> - forecast: Timeseries forecasting using ARIMA, ETS, STLM, TBATS, and neural network models</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/forecastHybrid/index.html">forecastHybrid</a> - forecastHybrid: Automatic ensemble and cross validation of ARIMA, ETS, STLM, TBATS, and neural network models from the “forecast” package</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/fpc/index.html">fpc</a> - fpc: Flexible procedures for clustering</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/frbs/index.html">frbs</a> - frbs: Fuzzy Rule-based Systems for Classification and Regression Tasks</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/GAMBoost/index.html">GAMBoost</a> - GAMBoost: Generalized linear and additive models by likelihood based boosting</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/gamboostLSS/index.html">gamboostLSS</a> - gamboostLSS: Boosting Methods for GAMLSS</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/gbm/index.html">gbm</a> - gbm: Generalized Boosted Regression Models</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/glmnet/index.html">glmnet</a> - glmnet: Lasso and elastic-net regularized generalized linear models</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/glmpath/index.html">glmpath</a> - glmpath: L1 Regularization Path for Generalized Linear Models and Cox Proportional Hazards Model</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/GMMBoost/index.html">GMMBoost</a> - GMMBoost: Likelihood-based Boosting for Generalized mixed models</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/grplasso/index.html">grplasso</a> - grplasso: Fitting user specified models with Group Lasso penalty</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/grpreg/index.html">grpreg</a> - grpreg: Regularization paths for regression models with grouped covariates</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/h2o/index.html">h2o</a> - A framework for fast, parallel, and distributed machine learning algorithms at scale – Deeplearning, Random forests, GBM, KMeans, PCA, GLM</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/hda/index.html">hda</a> - hda: Heteroscedastic Discriminant Analysis</li>
<li><a class="reference external" href="http://www-bcf.usc.edu/~gareth/ISL/">Introduction to Statistical Learning</a></li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/ipred/index.html">ipred</a> - ipred: Improved Predictors</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/kernlab/index.html">kernlab</a> - kernlab: Kernel-based Machine Learning Lab</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/klaR/index.html">klaR</a> - klaR: Classification and visualization</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/lars/index.html">lars</a> - lars: Least Angle Regression, Lasso and Forward Stagewise</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/lasso2/index.html">lasso2</a> - lasso2: L1 constrained estimation aka ‘lasso’</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/LiblineaR/index.html">LiblineaR</a> - LiblineaR: Linear Predictive Models Based On The Liblinear C/C++ Library</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/LogicReg/index.html">LogicReg</a> - LogicReg: Logic Regression</li>
<li><a class="reference external" href="https://github.com/johnmyleswhite/ML_for_Hackers">Machine Learning For Hackers</a></li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/maptree/index.html">maptree</a> - maptree: Mapping, pruning, and graphing tree models</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/mboost/index.html">mboost</a> - mboost: Model-Based Boosting</li>
<li><a class="reference external" href="https://www.kaggle.com/forums/f/15/kaggle-forum/t/3661/medley-a-new-r-package-for-blending-regression-models?forumMessageId=21278">medley</a> - medley: Blending regression models, using a greedy stepwise approach</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/mlr/index.html">mlr</a> - mlr: Machine Learning in R</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/mvpart/index.html">mvpart</a> - mvpart: Multivariate partitioning</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/ncvreg/index.html">ncvreg</a> - ncvreg: Regularization paths for SCAD- and MCP-penalized regression models</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/nnet/index.html">nnet</a> - nnet: Feed-forward Neural Networks and Multinomial Log-Linear Models</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/oblique.tree/index.html">oblique.tree</a> - oblique.tree: Oblique Trees for Classification Data</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/pamr/index.html">pamr</a> - pamr: Pam: prediction analysis for microarrays</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/party/index.html">party</a> - party: A Laboratory for Recursive Partytioning</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/partykit/index.html">partykit</a> - partykit: A Toolkit for Recursive Partytioning</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/penalized/index.html)-penalized:L1(lassoandfusedlasso)andL2(ridge">penalized</a> penalized estimation in GLMs and in the Cox model</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/penalizedLDA/index.html">penalizedLDA</a> - penalizedLDA: Penalized classification using Fisher’s linear discriminant</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/penalizedSVM/index.html">penalizedSVM</a> - penalizedSVM: Feature Selection SVM using penalty functions</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/quantregForest/index.html">quantregForest</a> - quantregForest: Quantile Regression Forests</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/randomForest/index.html">randomForest</a> - randomForest: Breiman and Cutler’s random forests for classification and regression</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/randomForestSRC/index.html)-randomForestSRC:RandomForestsforSurvival,RegressionandClassification(RF-SRC">randomForestSRC</a></li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/rattle/index.html">rattle</a> - rattle: Graphical user interface for data mining in R</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/rda/index.html">rda</a> - rda: Shrunken Centroids Regularized Discriminant Analysis</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/rdetools/index.html)-rdetools:RelevantDimensionEstimation(RDE">rdetools</a> in Feature Spaces</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/REEMtree/index.html)-REEMtree:RegressionTreeswithRandomEffectsforLongitudinal(Panel">REEMtree</a> Data</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/relaxo/index.html">relaxo</a> - relaxo: Relaxed Lasso</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/rgenoud/index.html">rgenoud</a> - rgenoud: R version of GENetic Optimization Using Derivatives</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/rgp/index.html">rgp</a> - rgp: R genetic programming framework</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/Rmalschains/index.html)-Rmalschains:ContinuousOptimizationusingMemeticAlgorithmswithLocalSearchChains(MA-LS-Chains">Rmalschains</a> in R</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/rminer/index.html)-rminer:Simpleruseofdataminingmethods(e.g.NNandSVM">rminer</a> in classification and regression</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/ROCR/index.html">ROCR</a> - ROCR: Visualizing the performance of scoring classifiers</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/RoughSets/index.html">RoughSets</a> - RoughSets: Data Analysis Using Rough Set and Fuzzy Rough Set Theories</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/rpart/index.html">rpart</a> - rpart: Recursive Partitioning and Regression Trees</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/RPMM/index.html">RPMM</a> - RPMM: Recursively Partitioned Mixture Model</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/RSNNS/index.html)-RSNNS:NeuralNetworksinRusingtheStuttgartNeuralNetworkSimulator(SNNS">RSNNS</a></li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/RWeka/index.html">RWeka</a> - RWeka: R/Weka interface</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/RXshrink/index.html">RXshrink</a> - RXshrink: Maximum Likelihood Shrinkage via Generalized Ridge or Least Angle Regression</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/sda/index.html">sda</a> - sda: Shrinkage Discriminant Analysis and CAT Score Variable Selection</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/SDDA/index.html">SDDA</a> - SDDA: Stepwise Diagonal Discriminant Analysis</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/subsemble/index.html">SuperLearner](https://github.com/ecpolley/SuperLearner) and [subsemble</a> - Multi-algorithm ensemble learning packages.</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/svmpath/index.html">svmpath</a> - svmpath: svmpath: the SVM Path algorithm</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/tgp/index.html">tgp</a> - tgp: Bayesian treed Gaussian process models</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/tree/index.html">tree</a> - tree: Classification and regression trees</li>
<li><a class="reference external" href="http://cran.r-project.org/web/packages/varSelRF/index.html">varSelRF</a> - varSelRF: Variable selection using random forests</li>
<li><a class="reference external" href="https://github.com/tqchen/xgboost/tree/master/R-package)-RbindingforeXtremeGradientBoosting(Tree">XGBoost.R</a> Library</li>
<li><a class="reference external" href="http://optunity.readthedocs.io/en/latest/">Optunity</a> - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly to R.</li>
<li><a class="reference external" href="http://igraph.org/r/">igraph</a> - binding to igraph library - General purpose graph library</li>
<li><a class="reference external" href="https://github.com/dmlc/mxnet">MXNet</a> - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.</li>
<li><a class="reference external" href="https://github.com/Azure/Azure-TDSP-Utilities)-TwodatascienceutilitiesinRfromMicrosoft:1)InteractiveDataExploration,Analysis,andReporting(IDEAR);2)AutomatedModelingandReporting(AMR">TDSP-Utilities</a>.</li>
</ul>
<p class="rubric">Data Analysis / Data Visualization</p>
<ul class="simple">
<li><a class="reference external" href="http://ggplot2.org/">ggplot2</a> - A data visualization package based on the grammar of graphics.</li>
</ul>
</div>
<div class="section" id="sas">
<h3><a class="toc-backref" href="#toc-entry-23">SAS</a><a class="headerlink" href="#sas" title="Permalink to this headline">¶</a></h3>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://www.sas.com/en_us/software/enterprise-miner.html">Enterprise Miner</a> - Data mining and machine learning that creates deployable models using a GUI or code.</li>
<li><a class="reference external" href="https://www.sas.com/en_us/software/factory-miner.html">Factory Miner</a> - Automatically creates deployable machine learning models across numerous market or customer segments using a GUI.</li>
</ul>
<p class="rubric">Data Analysis / Data Visualization</p>
<ul class="simple">
<li><a class="reference external" href="https://www.sas.com/en_us/software/analytics/stat.html">SAS/STAT</a> - For conducting advanced statistical analysis.</li>
<li><a class="reference external" href="https://www.sas.com/en_us/software/university-edition.html">University Edition</a> - FREE! Includes all SAS packages necessary for data analysis and visualization, and includes online SAS courses.</li>
</ul>
<p class="rubric">High Performance Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://www.sas.com/en_us/software/analytics/high-performance-data-mining.html">High Performance Data Mining</a> - Data mining and machine learning that creates deployable models using a GUI or code in an MPP environment, including Hadoop.</li>
<li><a class="reference external" href="https://www.sas.com/en_us/software/analytics/high-performance-text-mining.html">High Performance Text Mining</a> - Text mining using a GUI or code in an MPP environment, including Hadoop.</li>
</ul>
<p class="rubric">Natural Language Processing</p>
<ul class="simple">
<li><a class="reference external" href="https://www.sas.com/en_us/software/analytics/contextual-analysis.html">Contextual Analysis</a> - Add structure to unstructured text using a GUI.</li>
<li><a class="reference external" href="https://www.sas.com/en_us/software/analytics/sentiment-analysis.html">Sentiment Analysis</a> - Extract sentiment from text using a GUI.</li>
<li><a class="reference external" href="https://www.sas.com/en_us/software/analytics/text-miner.html">Text Miner</a> - Text mining using a GUI or code.</li>
</ul>
<p class="rubric">Demos and Scripts</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/sassoftware/enlighten-apply/tree/master/ML_tables">ML_Tables</a> - Concise cheat sheets containing machine learning best practices.</li>
<li><a class="reference external" href="https://github.com/sassoftware/enlighten-apply">enlighten-apply</a> - Example code and materials that illustrate applications of SAS machine learning techniques.</li>
<li><a class="reference external" href="https://github.com/sassoftware/enlighten-integration">enlighten-integration</a> - Example code and materials that illustrate techniques for integrating SAS with other analytics technologies in Java, PMML, Python and R.</li>
<li><a class="reference external" href="https://github.com/sassoftware/enlighten-deep">enlighten-deep</a> - Example code and materials that illustrate using neural networks with several hidden layers in SAS.</li>
<li><a class="reference external" href="https://github.com/sassoftware/dm-flow">dm-flow</a> - Library of SAS Enterprise Miner process flow diagrams to help you learn by example about specific data mining topics.</li>
</ul>
</div>
<div class="section" id="scala">
<h3><a class="toc-backref" href="#toc-entry-24">Scala</a><a class="headerlink" href="#scala" title="Permalink to this headline">¶</a></h3>
<p class="rubric">Natural Language Processing</p>
<ul class="simple">
<li><a class="reference external" href="http://www.scalanlp.org/">ScalaNLP</a> - ScalaNLP is a suite of machine learning and numerical computing libraries.</li>
<li><a class="reference external" href="https://github.com/scalanlp/breeze">Breeze</a> - Breeze is a numerical processing library for Scala.</li>
<li><a class="reference external" href="https://github.com/scalanlp/chalk">Chalk</a> - Chalk is a natural language processing library.</li>
<li><a class="reference external" href="https://github.com/factorie/factorie">FACTORIE</a> - FACTORIE is a toolkit for deployable probabilistic modeling, implemented as a software library in Scala. It provides its users with a succinct language for creating relational factor graphs, estimating parameters and performing inference.</li>
</ul>
<p class="rubric">Data Analysis / Data Visualization</p>
<ul class="simple">
<li><a class="reference external" href="http://spark.apache.org/docs/latest/mllib-guide.html">MLlib in Apache Spark</a> - Distributed machine learning library in Spark</li>
<li><a class="reference external" href="https://github.com/Hydrospheredata/mist">Hydrosphere Mist</a> - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.</li>
<li><a class="reference external" href="https://github.com/twitter/scalding">Scalding</a> - A Scala API for Cascading</li>
<li><a class="reference external" href="https://github.com/twitter/summingbird">Summing Bird</a> - Streaming MapReduce with Scalding and Storm</li>
<li><a class="reference external" href="https://github.com/twitter/algebird">Algebird</a> - Abstract Algebra for Scala</li>
<li><a class="reference external" href="https://github.com/xerial/xerial">xerial</a> - Data management utilities for Scala</li>
<li><a class="reference external" href="https://github.com/avibryant/simmer">simmer</a> - Reduce your data. A unix filter for algebird-powered aggregation.</li>
<li><a class="reference external" href="https://github.com/apache/incubator-predictionio">PredictionIO</a> - PredictionIO, a machine learning server for software developers and data engineers.</li>
<li><a class="reference external" href="https://github.com/BIDData/BIDMat">BIDMat</a> - CPU and GPU-accelerated matrix library intended to support large-scale exploratory data analysis.</li>
<li><a class="reference external" href="http://www.wolfe.ml/">Wolfe</a> Declarative Machine Learning</li>
<li><a class="reference external" href="http://flink.apache.org/">Flink</a> - Open source platform for distributed stream and batch data processing.</li>
<li><a class="reference external" href="http://spark-notebook.io">Spark Notebook</a> - Interactive and Reactive Data Science using Scala and Spark.</li>
</ul>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/etsy/Conjecture">Conjecture</a> - Scalable Machine Learning in Scalding</li>
<li><a class="reference external" href="https://github.com/stripe/brushfire">brushfire</a> - Distributed decision tree ensemble learning in Scala</li>
<li><a class="reference external" href="https://github.com/tresata/ganitha">ganitha</a> - scalding powered machine learning</li>
<li><a class="reference external" href="https://github.com/bigdatagenomics/adam">adam</a> - A genomics processing engine and specialized file format built using Apache Avro, Apache Spark and Parquet. Apache 2 licensed.</li>
<li><a class="reference external" href="https://github.com/bioscala/bioscala">bioscala</a> - Bioinformatics for the Scala programming language</li>
<li><a class="reference external" href="https://github.com/BIDData/BIDMach">BIDMach</a> - CPU and GPU-accelerated Machine Learning Library.</li>
<li><a class="reference external" href="https://github.com/p2t2/figaro">Figaro</a> - a Scala library for constructing probabilistic models.</li>
<li><a class="reference external" href="https://github.com/h2oai/sparkling-water">H2O Sparkling Water</a> - H2O and Spark interoperability.</li>
<li><a class="reference external" href="https://ci.apache.org/projects/flink/flink-docs-master/apis/batch/libs/ml/index.html">FlinkML in Apache Flink</a> - Distributed machine learning library in Flink</li>
<li><a class="reference external" href="https://github.com/transcendent-ai-labs/DynaML">DynaML</a> - Scala Library/REPL for Machine Learning Research</li>
<li><a class="reference external" href="https://github.com/IllinoisCogComp/saul/">Saul</a> - Flexible Declarative Learning-Based Programming.</li>
<li><a class="reference external" href="https://github.com/valdanylchuk/swiftlearner/">SwiftLearner</a> - Simply written algorithms to help study ML or write your own implementations.</li>
</ul>
</div>
<div class="section" id="swift">
<h3><a class="toc-backref" href="#toc-entry-25">Swift</a><a class="headerlink" href="#swift" title="Permalink to this headline">¶</a></h3>
<p class="rubric">General-Purpose Machine Learning</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/collinhundley/Swift-AI">Swift AI</a> - Highly optimized artificial intelligence and machine learning library written in Swift.</li>
<li><a class="reference external" href="https://github.com/aleph7/BrainCore">BrainCore</a> - The iOS and OS X neural network framework</li>
<li><a class="reference external" href="https://github.com/stsievert/swix">swix</a> - A bare bones library that
includes a general matrix language and wraps some OpenCV for iOS development.</li>
<li><a class="reference external" href="http://deeplearningkit.org/">DeepLearningKit</a> an Open Source Deep Learning Framework for Apple’s iOS, OS X and tvOS.
It currently allows using deep convolutional neural network models trained in Caffe on Apple operating systems.</li>
<li><a class="reference external" href="https://github.com/KevinCoble/AIToolbox">AIToolbox</a> - A toolbox framework of AI modules written in Swift:  Graphs/Trees, Linear Regression, Support Vector Machines, Neural Networks, PCA, KMeans, Genetic Algorithms, MDP, Mixture of Gaussians.</li>
<li><a class="reference external" href="https://github.com/Somnibyte/MLKit">MLKit</a> - A simple Machine Learning Framework written in Swift. Currently features Simple Linear Regression, Polynomial Regression, and Ridge Regression.</li>
<li><a class="reference external" href="https://github.com/vlall/Swift-Brain">Swift Brain</a> - The first neural network / machine learning library written in Swift. This is a project for AI algorithms in Swift for iOS and OS X development. This project includes algorithms focused on Bayes theorem, neural networks, SVMs, Matrices, etc..</li>
</ul>
</div>
</div>
<span id="document-papers"></span><div class="section" id="papers-1">
<span id="papers"></span><h2>Papers<a class="headerlink" href="#papers-1" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#machine-learning" id="toc-entry-1">Machine Learning</a></li>
<li><a class="reference internal" href="#deep-learning" id="toc-entry-2">Deep Learning</a><ul>
<li><a class="reference internal" href="#understanding" id="toc-entry-3">Understanding</a></li>
<li><a class="reference internal" href="#optimization-training-techniques" id="toc-entry-4">Optimization / Training Techniques</a></li>
<li><a class="reference internal" href="#unsupervised-generative-models" id="toc-entry-5">Unsupervised / Generative Models</a></li>
<li><a class="reference internal" href="#image-segmentation-object-detection" id="toc-entry-6">Image Segmentation / Object Detection</a></li>
<li><a class="reference internal" href="#image-video" id="toc-entry-7">Image / Video</a></li>
<li><a class="reference internal" href="#natural-language-processing" id="toc-entry-8">Natural Language Processing</a></li>
<li><a class="reference internal" href="#speech-other" id="toc-entry-9">Speech / Other</a></li>
<li><a class="reference internal" href="#reinforcement-learning" id="toc-entry-10">Reinforcement Learning</a></li>
<li><a class="reference internal" href="#new-papers" id="toc-entry-11">New papers</a></li>
<li><a class="reference internal" href="#classic-papers" id="toc-entry-12">Classic Papers</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="machine-learning">
<h3><a class="toc-backref" href="#toc-entry-1">Machine Learning</a><a class="headerlink" href="#machine-learning" title="Permalink to this headline">¶</a></h3>
<p>Be the first to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet">contribute!</a></p>
</div>
<div class="section" id="deep-learning">
<h3><a class="toc-backref" href="#toc-entry-2">Deep Learning</a><a class="headerlink" href="#deep-learning" title="Permalink to this headline">¶</a></h3>
<p>Forked from terryum’s <a class="reference external" href="https://github.com/terryum/awesome-deep-learning-papers">awesome deep learning papers</a>.</p>
<div class="section" id="understanding">
<h4><a class="toc-backref" href="#toc-entry-3">Understanding</a><a class="headerlink" href="#understanding" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Distilling the knowledge in a neural network (2015), G. Hinton et al. <a class="reference external" href="http://arxiv.org/1503.02531">[pdf]</a></li>
<li>Deep neural networks are easily fooled: High confidence predictions for unrecognizable images (2015), A. Nguyen et al. <a class="reference external" href="http://arxiv.org/1412.1897">[pdf]</a></li>
<li>How transferable are features in deep neural networks? (2014), J. Yosinski et al. <a class="reference external" href="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf">[pdf]</a></li>
<li>CNN features off-the-Shelf: An astounding baseline for recognition (2014), A. Razavian et al. <a class="reference external" href="http://www.cv-foundation.org//openaccess/content_cvpr_workshops_2014/W15/papers/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.pdf">[pdf]</a></li>
<li>Learning and transferring mid-Level image representations using convolutional neural networks (2014), M. Oquab et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf">[pdf]</a></li>
<li>Visualizing and understanding convolutional networks (2014), M. Zeiler and R. Fergus <a class="reference external" href="http://arxiv.org/1311.2901">[pdf]</a></li>
<li>Decaf: A deep convolutional activation feature for generic visual recognition (2014), J. Donahue et al. <a class="reference external" href="http://arxiv.org/1310.1531">[pdf]</a></li>
</ul>
</div>
<div class="section" id="optimization-training-techniques">
<h4><a class="toc-backref" href="#toc-entry-4">Optimization / Training Techniques</a><a class="headerlink" href="#optimization-training-techniques" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Batch normalization: Accelerating deep network training by reducing internal covariate shift (2015), S. Loffe and C. Szegedy <a class="reference external" href="http://arxiv.org/1502.03167">[pdf]</a></li>
<li>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification (2015), K. He et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf">[pdf]</a></li>
<li>Dropout: A simple way to prevent neural networks from overfitting (2014), N. Srivastava et al. <a class="reference external" href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">[pdf]</a></li>
<li>Adam: A method for stochastic optimization (2014), D. Kingma and J. Ba <a class="reference external" href="http://arxiv.org/1412.6980">[pdf]</a></li>
<li>Improving neural networks by preventing co-adaptation of feature detectors (2012), G. Hinton et al. <a class="reference external" href="http://arxiv.org/1207.0580.pdf">[pdf]</a></li>
<li>Random search for hyper-parameter optimization (2012) J. Bergstra and Y. Bengio <a class="reference external" href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a">[pdf]</a></li>
</ul>
</div>
<div class="section" id="unsupervised-generative-models">
<h4><a class="toc-backref" href="#toc-entry-5">Unsupervised / Generative Models</a><a class="headerlink" href="#unsupervised-generative-models" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Pixel recurrent neural networks (2016), A. Oord et al. <a class="reference external" href="http://arxiv.org/1601.06759v2.pdf">[pdf]</a></li>
<li>Improved techniques for training GANs (2016), T. Salimans et al. <a class="reference external" href="http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf">[pdf]</a></li>
<li>Unsupervised representation learning with deep convolutional generative adversarial networks (2015), A. Radford et al. <a class="reference external" href="https://arxiv.org/1511.06434v2">[pdf]</a></li>
<li>DRAW: A recurrent neural network for image generation (2015), K. Gregor et al. <a class="reference external" href="http://arxiv.org/1502.04623">[pdf]</a></li>
<li>Generative adversarial nets (2014), I. Goodfellow et al. <a class="reference external" href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">[pdf]</a></li>
<li>Auto-encoding variational Bayes (2013), D. Kingma and M. Welling <a class="reference external" href="http://arxiv.org/1312.6114">[pdf]</a></li>
<li>Building high-level features using large scale unsupervised learning (2013), Q. Le et al. <a class="reference external" href="http://arxiv.org/1112.6209">[pdf]</a></li>
</ul>
</div>
<div class="section" id="image-segmentation-object-detection">
<h4><a class="toc-backref" href="#toc-entry-6">Image Segmentation / Object Detection</a><a class="headerlink" href="#image-segmentation-object-detection" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>You only look once: Unified, real-time object detection (2016), J. Redmon et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf">[pdf]</a></li>
<li>Fully convolutional networks for semantic segmentation (2015), J. Long et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf">[pdf]</a></li>
<li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (2015), S. Ren et al. <a class="reference external" href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">[pdf]</a></li>
<li>Fast R-CNN (2015), R. Girshick <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf">[pdf]</a></li>
<li>Rich feature hierarchies for accurate object detection and semantic segmentation (2014), R. Girshick et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">[pdf]</a></li>
<li>Semantic image segmentation with deep convolutional nets and fully connected CRFs, L. Chen et al. <a class="reference external" href="https://arxiv.org/1412.7062">[pdf]</a></li>
<li>Learning hierarchical features for scene labeling (2013), C. Farabet et al. <a class="reference external" href="https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/farabet-pami-13.pdf">[pdf]</a></li>
</ul>
</div>
<div class="section" id="image-video">
<h4><a class="toc-backref" href="#toc-entry-7">Image / Video</a><a class="headerlink" href="#image-video" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Image Super-Resolution Using Deep Convolutional Networks (2016), C. Dong et al. <a class="reference external" href="https://arxiv.org/1501.00092v3.pdf">[pdf]</a></li>
<li>A neural algorithm of artistic style (2015), L. Gatys et al. <a class="reference external" href="https://arxiv.org/1508.06576">[pdf]</a></li>
<li>Deep visual-semantic alignments for generating image descriptions (2015), A. Karpathy and L. Fei-Fei <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf">[pdf]</a></li>
<li>Show, attend and tell: Neural image caption generation with visual attention (2015), K. Xu et al. <a class="reference external" href="http://arxiv.org/1502.03044">[pdf]</a></li>
<li>Show and tell: A neural image caption generator (2015), O. Vinyals et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf">[pdf]</a></li>
<li>Long-term recurrent convolutional networks for visual recognition and description (2015), J. Donahue et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf">[pdf]</a></li>
<li>VQA: Visual question answering (2015), S. Antol et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf">[pdf]</a></li>
<li>DeepFace: Closing the gap to human-level performance in face verification (2014), Y. Taigman et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf">[pdf]</a>:</li>
<li>Large-scale video classification with convolutional neural networks (2014), A. Karpathy et al. <a class="reference external" href="http://vision.stanford.edu/karpathy14.pdf">[pdf]</a></li>
<li>DeepPose: Human pose estimation via deep neural networks (2014), A. Toshev and C. Szegedy <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf">[pdf]</a></li>
<li>Two-stream convolutional networks for action recognition in videos (2014), K. Simonyan et al. <a class="reference external" href="http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf">[pdf]</a></li>
<li>3D convolutional neural networks for human action recognition (2013), S. Ji et al. <a class="reference external" href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf">[pdf]</a></li>
</ul>
</div>
<div class="section" id="natural-language-processing">
<h4><a class="toc-backref" href="#toc-entry-8">Natural Language Processing</a><a class="headerlink" href="#natural-language-processing" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Neural Architectures for Named Entity Recognition (2016), G. Lample et al. <a class="reference external" href="http://aclweb.org/anthology/N/N16/N16-1030.pdf">[pdf]</a></li>
<li>Exploring the limits of language modeling (2016), R. Jozefowicz et al. <a class="reference external" href="http://arxiv.org/1602.02410">[pdf]</a></li>
<li>Teaching machines to read and comprehend (2015), K. Hermann et al. <a class="reference external" href="http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf">[pdf]</a></li>
<li>Effective approaches to attention-based neural machine translation (2015), M. Luong et al. <a class="reference external" href="https://arxiv.org/1508.04025">[pdf]</a></li>
<li>Conditional random fields as recurrent neural networks (2015), S. Zheng and S. Jayasumana. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf">[pdf]</a></li>
<li>Memory networks (2014), J. Weston et al. <a class="reference external" href="https://arxiv.org/1410.3916">[pdf]</a></li>
<li>Neural turing machines (2014), A. Graves et al. <a class="reference external" href="https://arxiv.org/1410.5401">[pdf]</a></li>
<li>Neural machine translation by jointly learning to align and translate (2014), D. Bahdanau et al. <a class="reference external" href="http://arxiv.org/1409.0473">[pdf]</a></li>
<li>Sequence to sequence learning with neural networks (2014), I. Sutskever et al. <a class="reference external" href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">[pdf]</a></li>
<li>Learning phrase representations using RNN encoder-decoder for statistical machine translation (2014), K. Cho et al. <a class="reference external" href="http://arxiv.org/1406.1078">[pdf]</a></li>
<li>A convolutional neural network for modeling sentences (2014), N. Kalchbrenner et al. <a class="reference external" href="http://arxiv.org/1404.2188v1">[pdf]</a></li>
<li>Convolutional neural networks for sentence classification (2014), Y. Kim <a class="reference external" href="http://arxiv.org/1408.5882">[pdf]</a></li>
<li>Glove: Global vectors for word representation (2014), J. Pennington et al. <a class="reference external" href="http://anthology.aclweb.org/D/D14/D14-1162.pdf">[pdf]</a></li>
<li>Distributed representations of sentences and documents (2014), Q. Le and T. Mikolov <a class="reference external" href="http://arxiv.org/1405.4053">[pdf]</a></li>
<li>Distributed representations of words and phrases and their compositionality (2013), T. Mikolov et al. <a class="reference external" href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">[pdf]</a></li>
<li>Efficient estimation of word representations in vector space (2013), T. Mikolov et al.  <a class="reference external" href="http://arxiv.org/1301.3781">[pdf]</a></li>
<li>Recursive deep models for semantic compositionality over a sentiment treebank (2013), R. Socher et al. <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&amp;rep=rep1&amp;type=pdf">[pdf]</a></li>
<li>Generating sequences with recurrent neural networks (2013), A. Graves. <a class="reference external" href="https://arxiv.org/1308.0850">[pdf]</a></li>
</ul>
</div>
<div class="section" id="speech-other">
<h4><a class="toc-backref" href="#toc-entry-9">Speech / Other</a><a class="headerlink" href="#speech-other" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>End-to-end attention-based large vocabulary speech recognition (2016), D. Bahdanau et al. <a class="reference external" href="https://arxiv.org/1508.04395">[pdf]</a></li>
<li>Deep speech 2: End-to-end speech recognition in English and Mandarin (2015), D. Amodei et al. <a class="reference external" href="https://arxiv.org/1512.02595">[pdf]</a></li>
<li>Speech recognition with deep recurrent neural networks (2013), A. Graves <a class="reference external" href="http://arxiv.org/1303.5778.pdf">[pdf]</a></li>
<li>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups (2012), G. Hinton et al. <a class="reference external" href="http://www.cs.toronto.edu/~asamir/papers/SPM_DNN_12.pdf">[pdf]</a></li>
<li>Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition (2012) G. Dahl et al. <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.337.7548&amp;rep=rep1&amp;type=pdf">[pdf]</a></li>
<li>Acoustic modeling using deep belief networks (2012), A. Mohamed et al. <a class="reference external" href="http://www.cs.toronto.edu/~asamir/papers/speechDBN_jrnl.pdf">[pdf]</a></li>
</ul>
</div>
<div class="section" id="reinforcement-learning">
<h4><a class="toc-backref" href="#toc-entry-10">Reinforcement Learning</a><a class="headerlink" href="#reinforcement-learning" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>End-to-end training of deep visuomotor policies (2016), S. Levine et al. <a class="reference external" href="http://www.jmlr.org/papers/volume17/15-522/source/15-522.pdf">[pdf]</a></li>
<li>Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection (2016), S. Levine et al. <a class="reference external" href="https://arxiv.org/1603.02199">[pdf]</a></li>
<li>Asynchronous methods for deep reinforcement learning (2016), V. Mnih et al. <a class="reference external" href="http://www.jmlr.org/proceedings/papers/v48/mniha16.pdf">[pdf]</a></li>
<li>Deep Reinforcement Learning with Double Q-Learning (2016), H. Hasselt et al. <a class="reference external" href="https://arxiv.org/1509.06461.pdf">[pdf]</a></li>
<li>Mastering the game of Go with deep neural networks and tree search (2016), D. Silver et al. <a class="reference external" href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">[pdf]</a></li>
<li>Continuous control with deep reinforcement learning (2015), T. Lillicrap et al. <a class="reference external" href="https://arxiv.org/1509.02971">[pdf]</a></li>
<li>Human-level control through deep reinforcement learning (2015), V. Mnih et al. <a class="reference external" href="http://www.davidqiu.com:8888/research/nature14236.pdf">[pdf]</a></li>
<li>Deep learning for detecting robotic grasps (2015), I. Lenz et al. <a class="reference external" href="http://www.cs.cornell.edu/~asaxena/papers/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf">[pdf]</a></li>
<li>Playing atari with deep reinforcement learning (2013), V. Mnih et al. <a class="reference external" href="http://arxiv.org/1312.5602.pdf)">[pdf]</a></li>
</ul>
</div>
<div class="section" id="new-papers">
<h4><a class="toc-backref" href="#toc-entry-11">New papers</a><a class="headerlink" href="#new-papers" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Deep Photo Style Transfer (2017), F. Luan et al. <a class="reference external" href="http://arxiv.org/1703.07511v1.pdf">[pdf]</a></li>
<li>Evolution Strategies as a Scalable Alternative to Reinforcement Learning (2017), T. Salimans et al. <a class="reference external" href="http://arxiv.org/1703.03864v1.pdf">[pdf]</a></li>
<li>Deformable Convolutional Networks (2017), J. Dai et al. <a class="reference external" href="http://arxiv.org/1703.06211v2.pdf">[pdf]</a></li>
<li>Mask R-CNN (2017), K. He et al. <a class="reference external" href="https://128.84.21.199/1703.06870">[pdf]</a></li>
<li>Learning to discover cross-domain relations with generative adversarial networks (2017), T. Kim et al. <a class="reference external" href="http://arxiv.org/1703.05192v1.pdf">[pdf]</a></li>
<li>Deep voice: Real-time neural text-to-speech (2017), S. Arik et al., <a class="reference external" href="http://arxiv.org/1702.07825v2.pdf">[pdf]</a></li>
<li>PixelNet: Representation of the pixels, by the pixels, and for the pixels (2017), A. Bansal et al. <a class="reference external" href="http://arxiv.org/1702.06506v1.pdf">[pdf]</a></li>
<li>Batch renormalization: Towards reducing minibatch dependence in batch-normalized models (2017), S. Ioffe. <a class="reference external" href="https://arxiv.org/abs/1702.03275">[pdf]</a></li>
<li>Wasserstein GAN (2017), M. Arjovsky et al. <a class="reference external" href="https://arxiv.org/1701.07875v1">[pdf]</a></li>
<li>Understanding deep learning requires rethinking generalization (2017), C. Zhang et al. <a class="reference external" href="https://arxiv.org/1611.03530">[pdf]</a></li>
<li>Least squares generative adversarial networks (2016), X. Mao et al. <a class="reference external" href="https://arxiv.org/abs/1611.04076v2">[pdf]</a></li>
</ul>
</div>
<div class="section" id="classic-papers">
<h4><a class="toc-backref" href="#toc-entry-12">Classic Papers</a><a class="headerlink" href="#classic-papers" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>An analysis of single-layer networks in unsupervised feature learning (2011), A. Coates et al. <a class="reference external" href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf">[pdf]</a></li>
<li>Deep sparse rectifier neural networks (2011), X. Glorot et al. <a class="reference external" href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf">[pdf]</a></li>
<li>Natural language processing (almost) from scratch (2011), R. Collobert et al. <a class="reference external" href="http://arxiv.org/1103.0398">[pdf]</a></li>
<li>Recurrent neural network based language model (2010), T. Mikolov et al. <a class="reference external" href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf">[pdf]</a></li>
<li>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al. <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&amp;rep=rep1&amp;type=pdf">[pdf]</a></li>
<li>Learning mid-level features for recognition (2010), Y. Boureau <a class="reference external" href="http://ece.duke.edu/~lcarin/boureau-cvpr-10.pdf">[pdf]</a></li>
<li>A practical guide to training restricted boltzmann machines (2010), G. Hinton <a class="reference external" href="http://www.csri.utoronto.ca/~hinton/absps/guideTR.pdf">[pdf]</a></li>
<li>Understanding the difficulty of training deep feedforward neural networks (2010), X. Glorot and Y. Bengio <a class="reference external" href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf">[pdf]</a></li>
<li>Why does unsupervised pre-training help deep learning (2010), D. Erhan et al. <a class="reference external" href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf">[pdf]</a></li>
<li>Learning deep architectures for AI (2009), Y. Bengio. <a class="reference external" href="http://sanghv.com/download/soft/machine%20learning,%20artificial%20intelligence,%20mathematics%20ebooks/ML/learning%20deep%20architectures%20for%20AI%20(2009).pdf">[pdf]</a></li>
<li>Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations (2009), H. Lee et al. <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&amp;rep=rep1&amp;type=pdf">[pdf]</a></li>
<li>Greedy layer-wise training of deep networks (2007), Y. Bengio et al. <a class="reference external" href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_739.pdf">[pdf]</a></li>
<li>A fast learning algorithm for deep belief nets (2006), G. Hinton et al. <a class="reference external" href="http://nuyoo.utm.mx/~jjf/rna/A8%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets.pdf">[pdf]</a></li>
<li>Gradient-based learning applied to document recognition (1998), Y. LeCun et al. <a class="reference external" href="http://yann.lecun.com/exdb/publis/lecun-01a.pdf">[pdf]</a></li>
<li>Long short-term memory (1997), S. Hochreiter and J. Schmidhuber. <a class="reference external" href="http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1997.9.8.1735">[pdf]</a></li>
</ul>
</div>
</div>
</div>
<span id="document-other_content"></span><div class="section" id="other-content">
<span id="content"></span><h2>Other Content<a class="headerlink" href="#other-content" title="Permalink to this headline">¶</a></h2>
<p>Books, blogs, courses and more forked from josephmisiti’s <a class="reference external" href="https://github.com/josephmisiti/awesome-machine-learning">awesome machine learning</a></p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#blogs" id="toc-entry-1">Blogs</a><ul>
<li><a class="reference internal" href="#data-science" id="toc-entry-2">Data Science</a></li>
<li><a class="reference internal" href="#machine-learning" id="toc-entry-3">Machine learning</a></li>
<li><a class="reference internal" href="#math" id="toc-entry-4">Math</a></li>
</ul>
</li>
<li><a class="reference internal" href="#books" id="toc-entry-5">Books</a><ul>
<li><a class="reference internal" href="#machine-learning-1" id="toc-entry-6">Machine learning</a></li>
<li><a class="reference internal" href="#deep-learning" id="toc-entry-7">Deep learning</a></li>
<li><a class="reference internal" href="#probability-statistics" id="toc-entry-8">Probability &amp; Statistics</a></li>
<li><a class="reference internal" href="#linear-algebra" id="toc-entry-9">Linear Algebra</a></li>
</ul>
</li>
<li><a class="reference internal" href="#courses" id="toc-entry-10">Courses</a></li>
<li><a class="reference internal" href="#podcasts" id="toc-entry-11">Podcasts</a></li>
<li><a class="reference internal" href="#tutorials" id="toc-entry-12">Tutorials</a></li>
</ul>
</div>
<div class="section" id="blogs">
<h3><a class="toc-backref" href="#toc-entry-1">Blogs</a><a class="headerlink" href="#blogs" title="Permalink to this headline">¶</a></h3>
<div class="section" id="data-science">
<h4><a class="toc-backref" href="#toc-entry-2">Data Science</a><a class="headerlink" href="#data-science" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><a class="reference external" href="https://jeremykun.com/">https://jeremykun.com/</a></li>
<li><a class="reference external" href="http://iamtrask.github.io/">http://iamtrask.github.io/</a></li>
<li><a class="reference external" href="http://blog.explainmydata.com/">http://blog.explainmydata.com/</a></li>
<li><a class="reference external" href="http://andrewgelman.com/">http://andrewgelman.com/</a></li>
<li><a class="reference external" href="http://simplystatistics.org/">http://simplystatistics.org/</a></li>
<li><a class="reference external" href="http://www.evanmiller.org/">http://www.evanmiller.org/</a></li>
<li><a class="reference external" href="http://jakevdp.github.io/">http://jakevdp.github.io/</a></li>
<li><a class="reference external" href="http://blog.yhat.com/">http://blog.yhat.com/</a></li>
<li><a class="reference external" href="http://wesmckinney.com">http://wesmckinney.com</a></li>
<li><a class="reference external" href="http://www.overkillanalytics.net/">http://www.overkillanalytics.net/</a></li>
<li><a class="reference external" href="http://newton.cx/~peter/">http://newton.cx/~peter/</a></li>
<li><a class="reference external" href="http://mbakker7.github.io/exploratory_computing_with_python/">http://mbakker7.github.io/exploratory_computing_with_python/</a></li>
<li><a class="reference external" href="https://sebastianraschka.com/blog/index.html">https://sebastianraschka.com/blog/index.html</a></li>
<li><a class="reference external" href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/">http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/</a></li>
<li><a class="reference external" href="http://colah.github.io/">http://colah.github.io/</a></li>
<li><a class="reference external" href="http://www.thomasdimson.com/">http://www.thomasdimson.com/</a></li>
<li><a class="reference external" href="http://blog.smellthedata.com/">http://blog.smellthedata.com/</a></li>
<li><a class="reference external" href="https://sebastianraschka.com/">https://sebastianraschka.com/</a></li>
<li><a class="reference external" href="http://dogdogfish.com/">http://dogdogfish.com/</a></li>
<li><a class="reference external" href="http://www.johnmyleswhite.com/">http://www.johnmyleswhite.com/</a></li>
<li><a class="reference external" href="http://drewconway.com/zia/">http://drewconway.com/zia/</a></li>
<li><a class="reference external" href="http://bugra.github.io/">http://bugra.github.io/</a></li>
<li><a class="reference external" href="http://opendata.cern.ch/">http://opendata.cern.ch/</a></li>
<li><a class="reference external" href="https://alexanderetz.com/">https://alexanderetz.com/</a></li>
<li><a class="reference external" href="http://www.sumsar.net/">http://www.sumsar.net/</a></li>
<li><a class="reference external" href="https://www.countbayesie.com">https://www.countbayesie.com</a></li>
<li><a class="reference external" href="http://blog.kaggle.com/">http://blog.kaggle.com/</a></li>
<li><a class="reference external" href="http://www.danvk.org/">http://www.danvk.org/</a></li>
<li><a class="reference external" href="http://hunch.net/">http://hunch.net/</a></li>
<li><a class="reference external" href="http://www.randalolson.com/blog/">http://www.randalolson.com/blog/</a></li>
<li><a class="reference external" href="https://www.johndcook.com/blog/r_language_for_programmers/">https://www.johndcook.com/blog/r_language_for_programmers/</a></li>
<li><a class="reference external" href="http://www.dataschool.io/">http://www.dataschool.io/</a></li>
</ul>
</div>
<div class="section" id="machine-learning">
<h4><a class="toc-backref" href="#toc-entry-3">Machine learning</a><a class="headerlink" href="#machine-learning" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><a class="reference external" href="https://www.openai.com/">OpenAI</a></li>
<li><a class="reference external" href="http://distill.pub/">Distill</a></li>
<li><a class="reference external" href="http://karpathy.github.io/">Andrej Karpathy Blog</a></li>
<li><a class="reference external" href="http://colah.github.io/">Colah’s Blog</a></li>
<li><a class="reference external" href="http://www.wildml.com/">WildML</a></li>
<li><a class="reference external" href="http://www.fastml.com/">FastML</a></li>
<li><a class="reference external" href="https://blog.acolyer.org">TheMorningPaper</a></li>
</ul>
</div>
<div class="section" id="math">
<h4><a class="toc-backref" href="#toc-entry-4">Math</a><a class="headerlink" href="#math" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><a class="reference external" href="http://www.sumsar.net/">http://www.sumsar.net/</a></li>
<li><a class="reference external" href="http://allendowney.blogspot.ca/">http://allendowney.blogspot.ca/</a></li>
<li><a class="reference external" href="https://healthyalgorithms.com/">https://healthyalgorithms.com/</a></li>
<li><a class="reference external" href="https://petewarden.com/">https://petewarden.com/</a></li>
<li><a class="reference external" href="http://mrtz.org/blog/">http://mrtz.org/blog/</a></li>
</ul>
</div>
</div>
<div class="section" id="books">
<h3><a class="toc-backref" href="#toc-entry-5">Books</a><a class="headerlink" href="#books" title="Permalink to this headline">¶</a></h3>
<div class="section" id="machine-learning-1">
<h4><a class="toc-backref" href="#toc-entry-6">Machine learning</a><a class="headerlink" href="#machine-learning-1" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><a class="reference external" href="https://www.manning.com/books/real-world-machine-learning">Real World Machine Learning</a> [Free Chapters]</li>
<li><a class="reference external" href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction To Statistical Learning</a> - Book + R Code</li>
<li><a class="reference external" href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">Elements of Statistical Learning</a> - Book</li>
<li><a class="reference external" href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/">Probabilistic Programming &amp; Bayesian Methods for Hackers</a> - Book + IPython Notebooks</li>
<li><a class="reference external" href="http://greenteapress.com/wp/think-bayes/">Think Bayes</a> - Book + Python Code</li>
<li><a class="reference external" href="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html">Information Theory, Inference, and Learning Algorithms</a></li>
<li><a class="reference external" href="http://www.gaussianprocess.org/gpml/chapters/">Gaussian Processes for Machine Learning</a></li>
<li><a class="reference external" href="http://lintool.github.io/MapReduceAlgorithms/">Data Intensive Text Processing w/ MapReduce</a></li>
<li><a class="reference external" href="http://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html">Reinforcement Learning: - An Introduction</a></li>
<li><a class="reference external" href="http://infolab.stanford.edu/~ullman/mmds/book.pdf">Mining Massive Datasets</a></li>
<li><a class="reference external" href="https://www.ics.uci.edu/~welling/teaching/273ASpring10/IntroMLBook.pdf">A First Encounter with Machine Learning</a></li>
<li><a class="reference external" href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf">Pattern Recognition and Machine Learning</a></li>
<li><a class="reference external" href="http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/090310.pdf">Machine Learning &amp; Bayesian Reasoning</a></li>
<li><a class="reference external" href="http://alex.smola.org/drafts/thebook.pdf">Introduction to Machine Learning</a> - Alex Smola and S.V.N. Vishwanathan</li>
<li><a class="reference external" href="http://www.szit.bme.hu/~gyorfi/pbook.pdf">A Probabilistic Theory of Pattern Recognition</a></li>
<li><a class="reference external" href="http://nlp.stanford.edu/IR-book/pdf/irbookprint.pdf">Introduction to Information Retrieval</a></li>
<li><a class="reference external" href="https://www.otexts.org/fpp/">Forecasting: principles and practice</a></li>
<li><a class="reference external" href="http://www.markwatson.com/opencontent_data/JavaAI3rd.pdf">Practical Artificial Intelligence Programming in Java</a></li>
<li><a class="reference external" href="https://arxiv.org/pdf/0904.3664v1.pdf">Introduction to Machine Learning</a> - Amnon Shashua</li>
<li><a class="reference external" href="http://www.intechopen.com/books/reinforcement_learning">Reinforcement Learning</a></li>
<li><a class="reference external" href="http://www.intechopen.com/books/machine_learning">Machine Learning</a></li>
<li><a class="reference external" href="http://ai.stanford.edu/~nilsson/QAI/qai.pdf">A Quest for AI</a></li>
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.177.857&amp;rep=rep1&amp;type=pdf">Introduction to Applied Bayesian Statistics and Estimation for Social Scientists</a> - Scott M. Lynch</li>
<li><a class="reference external" href="https://users.soe.ucsc.edu/~draper/draper-BMIP-dec2005.pdf">Bayesian Modeling, Inference and Prediction</a></li>
<li><a class="reference external" href="http://ciml.info/">A Course in Machine Learning</a></li>
<li><a class="reference external" href="http://www1.maths.leeds.ac.uk/~charles/statlog/">Machine Learning, Neural and Statistical Classification</a></li>
<li><a class="reference external" href="http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage">Bayesian Reasoning and Machine Learning</a> Book+MatlabToolBox</li>
<li><a class="reference external" href="https://leanpub.com/rprogramming">R Programming for Data Science</a></li>
<li><a class="reference external" href="http://cs.du.edu/~mitchell/mario_books/Data_Mining:_Practical_Machine_Learning_Tools_and_Techniques_-_2e_-_Witten_&amp;_Frank.pdf">Data Mining - Practical Machine Learning Tools and Techniques</a> Book</li>
</ul>
</div>
<div class="section" id="deep-learning">
<h4><a class="toc-backref" href="#toc-entry-7">Deep learning</a><a class="headerlink" href="#deep-learning" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><a class="reference external" href="http://www.deeplearningbook.org/">Deep Learning - An MIT Press book</a></li>
<li><a class="reference external" href="http://www.cs.columbia.edu/~mcollins/notes-spring2013.html">Coursera Course Book on NLP</a></li>
<li><a class="reference external" href="http://www.nltk.org/book/">NLTK</a></li>
<li><a class="reference external" href="http://victoria.lviv.ua/html/fl5/NaturalLanguageProcessingWithPython.pdf">NLP w/ Python</a></li>
<li><a class="reference external" href="http://nlp.stanford.edu/fsnlp/promo/">Foundations of Statistical Natural Language Processing</a></li>
<li><a class="reference external" href="http://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf">An Introduction to Information Retrieval</a></li>
<li><a class="reference external" href="http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-2col-dkrieselcom.pdf">A Brief Introduction to Neural Networks</a></li>
<li><a class="reference external" href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a></li>
</ul>
</div>
<div class="section" id="probability-statistics">
<h4><a class="toc-backref" href="#toc-entry-8">Probability &amp; Statistics</a><a class="headerlink" href="#probability-statistics" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><a class="reference external" href="http://www.greenteapress.com/thinkstats/">Think Stats</a> - Book + Python Code</li>
<li><a class="reference external" href="http://heather.cs.ucdavis.edu/probstatbook">From Algorithms to Z-Scores</a> - Book</li>
<li><a class="reference external" href="http://heather.cs.ucdavis.edu/~matloff/132/NSPpart.pdf)-Book(NotFinished">The Art of R Programming</a></li>
<li><a class="reference external" href="http://people.math.umass.edu/~lavine/Book/book.pdf">Introduction to statistical thought</a></li>
<li><a class="reference external" href="http://www.math.uiuc.edu/~r-ash/BPT/BPT.pdf">Basic Probability Theory</a></li>
<li><a class="reference external" href="https://math.dartmouth.edu/~prob/prob/prob.pdf">Introduction to probability</a> - By Dartmouth College</li>
<li><a class="reference external" href="http://uncertainty.stat.cmu.edu/wp-content/uploads/2011/05/principles-of-uncertainty.pdf">Principle of Uncertainty</a></li>
<li><a class="reference external" href="http://statistics.zone/">Probability &amp; Statistics Cookbook</a></li>
<li><a class="reference external" href="http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf">Advanced Data Analysis From An Elementary Point of View</a></li>
<li><a class="reference external" href="http://athenasc.com/probbook.html">Introduction to Probability</a> -  Book and course by MIT</li>
<li><a class="reference external" href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">The Elements of Statistical Learning: Data Mining, Inference, and Prediction.</a> -Book</li>
<li><a class="reference external" href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning with Applications in R</a> - Book</li>
<li><a class="reference external" href="http://health.adelaide.edu.au/psychology/ccs/teaching/lsr/">Learning Statistics Using R</a></li>
<li><a class="reference external" href="https://cran.r-project.org/web/packages/IPSUR/vignettes/IPSUR.pdf">Introduction to Probability and Statistics Using R</a> - Book</li>
<li><a class="reference external" href="http://adv-r.had.co.nz">Advanced R Programming</a> - Book</li>
<li><a class="reference external" href="http://cran.r-project.org/doc/contrib/Faraway-PRA.pdf">Practical Regression and Anova using R</a> - Book</li>
<li><a class="reference external" href="http://www.columbia.edu/~cjd11/charles_dimaggio/DIRE/resources/R/practicalsBookNoAns.pdf">R practicals</a> - Book</li>
<li><a class="reference external" href="http://www.burns-stat.com/pages/Tutor/R_inferno.pdf">The R Inferno</a> - Book</li>
</ul>
</div>
<div class="section" id="linear-algebra">
<h4><a class="toc-backref" href="#toc-entry-9">Linear Algebra</a><a class="headerlink" href="#linear-algebra" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><a class="reference external" href="http://www.math.brown.edu/~treil/papers/LADW/book.pdf">Linear Algebra Done Wrong</a></li>
<li><a class="reference external" href="https://math.byu.edu/~klkuttle/Linearalgebra.pdf">Linear Algebra, Theory, and Applications</a></li>
<li><a class="reference external" href="http://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a></li>
<li><a class="reference external" href="http://www.seas.ucla.edu/~vandenbe/103/reader.pdf">Applied Numerical Computing</a></li>
<li><a class="reference external" href="http://egrcc.github.io/docs/math/applied-numerical-linear-algebra.pdf">Applied Numerical Linear Algebra</a></li>
</ul>
</div>
</div>
<div class="section" id="courses">
<h3><a class="toc-backref" href="#toc-entry-10">Courses</a><a class="headerlink" href="#courses" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://cs231n.stanford.edu/">CS231n, Convolutional Neural Networks for Visual Recognition, Stanford University</a></li>
<li><a class="reference external" href="http://cs224d.stanford.edu/">CS224d, Deep Learning for Natural Language Processing, Stanford University</a></li>
<li><a class="reference external" href="https://github.com/oxford-cs-deepnlp-2017/lectures">Oxford Deep NLP 2017, Deep Learning for Natural Language Processing, University of Oxford</a></li>
<li><a class="reference external" href="https://www.edx.org/course/artificial-intelligence-ai-columbiax-csmm-101x">Artificial Intelligence (Columbia University)</a> - free</li>
<li><a class="reference external" href="https://www.edx.org/course/machine-learning-columbiax-csmm-102x">Machine Learning (Columbia University)</a> - free</li>
<li><a class="reference external" href="https://www.coursera.org/learn/machine-learning">Machine Learning (Stanford University)</a> - free</li>
<li><a class="reference external" href="https://www.coursera.org/learn/neural-networks">Neural Networks for Machine Learning (University of Toronto)</a> - free</li>
<li><a class="reference external" href="https://www.coursera.org/specializations/machine-learning">Machine Learning Specialization (University of Washington)</a> - Courses: Machine Learning Foundations: A Case Study Approach, Machine Learning: Regression, Machine Learning: Classification, Machine Learning: Clustering &amp; Retrieval, Machine Learning: Recommender Systems &amp; Dimensionality Reduction,Machine Learning Capstone: An Intelligent Application with Deep Learning; free</li>
<li><a class="reference external" href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/">Machine Learning Course (2014-15 session) (by Nando de Freitas, University of Oxford)</a> - Lecture slides and video recordings.</li>
<li><a class="reference external" href="http://www.work.caltech.edu/telecourse.html">Learning from Data (by Yaser S. Abu-Mostafa, Caltech)</a> - Lecture videos available</li>
</ul>
</div>
<div class="section" id="podcasts">
<h3><a class="toc-backref" href="#toc-entry-11">Podcasts</a><a class="headerlink" href="#podcasts" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://radar.oreilly.com/tag/oreilly-data-show-podcast">The O’Reilly Data Show</a></li>
<li><a class="reference external" href="http://partiallyderivative.com/">Partially Derivative</a></li>
<li><a class="reference external" href="http://www.thetalkingmachines.com/">The Talking Machines</a></li>
<li><a class="reference external" href="https://dataskeptic.com/">The Data Skeptic</a></li>
<li><a class="reference external" href="http://benjaffe.github.io/linear-digressions-site/">Linear Digressions</a></li>
<li><a class="reference external" href="http://datastori.es/">Data Stories</a></li>
<li><a class="reference external" href="http://www.learningmachines101.com/">Learning Machines 101</a></li>
<li><a class="reference external" href="http://simplystatistics.org/2015/09/17/not-so-standard-deviations-the-podcast/">Not So Standard Deviations</a></li>
<li><a class="reference external" href="https://twimlai.com/shows/">TWIMLAI</a></li>
</ul>
<p>-<a href="#system-message-1"><span class="problematic" id="problematic-1">_`</span></a>Machine Learning Guide &lt;<a class="reference external" href="http://ocdevel.com/mlg">http://ocdevel.com/mlg</a>&gt;`_</p>
</div>
<div class="section" id="tutorials">
<h3><a class="toc-backref" href="#toc-entry-12">Tutorials</a><a class="headerlink" href="#tutorials" title="Permalink to this headline">¶</a></h3>
<p>Be the first to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet">contribute!</a></p>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-contribute"></span><div class="section" id="contribute-1">
<span id="contribute"></span><h2>Contribute<a class="headerlink" href="#contribute-1" title="Permalink to this headline">¶</a></h2>
<p>Become a contributor! Check out our <a class="reference external" href="http://github.com/bfortuner/ml-cheatsheet/">github</a> for more information.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017
      
        <span class="commit">
          Revision <code>ad889a82</code>.
        </span>
      

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: latest
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      <dl>
        <dt>Versions</dt>
        
          <dd><a href="/en/latest/">latest</a></dd>
        
      </dl>
      <dl>
        <dt>Downloads</dt>
        
          <dd><a href="//ml-cheatsheet.readthedocs.io/_/downloads/en/latest/pdf/">pdf</a></dd>
        
          <dd><a href="//ml-cheatsheet.readthedocs.io/_/downloads/en/latest/htmlzip/">html</a></dd>
        
          <dd><a href="//ml-cheatsheet.readthedocs.io/_/downloads/en/latest/epub/">epub</a></dd>
        
      </dl>
      <dl>
        <dt>On Read the Docs</dt>
          <dd>
            <a href="//readthedocs.org/projects/ml-cheatsheet/?fromdocs=ml-cheatsheet">Project Home</a>
          </dd>
          <dd>
            <a href="//readthedocs.org/builds/ml-cheatsheet/?fromdocs=ml-cheatsheet">Builds</a>
          </dd>
      </dl>
      <hr/>
      Free document hosting provided by <a href="http://www.readthedocs.org">Read the Docs</a>.

    </div>
  </div>



  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
   

</body>
</html>